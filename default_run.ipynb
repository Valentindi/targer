{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "I0204 19:56:59.004187 4668726720 main.py:127] =========================================\n",
      "2020-02-04 19:56:59,004 [MainThread  ] [INFO ]  =========================================\n",
      "I0204 19:56:59.004513 4668726720 main.py:128] ==============Begin logging==============\n",
      "2020-02-04 19:56:59,004 [MainThread  ] [INFO ]  ==============Begin logging==============\n",
      "I0204 19:56:59.004727 4668726720 main.py:129] =========================================\n",
      "2020-02-04 19:56:59,004 [MainThread  ] [INFO ]  =========================================\n",
      "I0204 19:56:59.004853 4668726720 main.py:137] use cpu\n",
      "2020-02-04 19:56:59,004 [MainThread  ] [INFO ]  use cpu\n",
      "I0204 19:56:59.004960 4668726720 main.py:140] load dataset\n",
      "2020-02-04 19:56:59,004 [MainThread  ] [INFO ]  load dataset\n",
      "args train = data/NER/CoNNL_2003_shared_task/train.txt\n",
      "I0204 19:56:59.355732 4668726720 data_io_connl_ner_2003.py:51] Loading from data/NER/CoNNL_2003_shared_task/train.txt: 14041 samples, 203621 words.\n",
      "2020-02-04 19:56:59,355 [MainThread  ] [INFO ]  Loading from data/NER/CoNNL_2003_shared_task/train.txt: 14041 samples, 203621 words.\n",
      "I0204 19:56:59.456635 4668726720 data_io_connl_ner_2003.py:51] Loading from data/NER/CoNNL_2003_shared_task/dev.txt: 3250 samples, 51362 words.\n",
      "2020-02-04 19:56:59,456 [MainThread  ] [INFO ]  Loading from data/NER/CoNNL_2003_shared_task/dev.txt: 3250 samples, 51362 words.\n",
      "I0204 19:56:59.548123 4668726720 data_io_connl_ner_2003.py:51] Loading from data/NER/CoNNL_2003_shared_task/test.txt: 3453 samples, 46435 words.\n",
      "2020-02-04 19:56:59,548 [MainThread  ] [INFO ]  Loading from data/NER/CoNNL_2003_shared_task/test.txt: 3453 samples, 46435 words.\n",
      "I0204 19:56:59.549488 4668726720 main.py:142] create DatasetsBank\n",
      "2020-02-04 19:56:59,549 [MainThread  ] [INFO ]  create DatasetsBank\n",
      "I0204 19:57:25.812157 4668726720 datasets_bank.py:21] DatasetsBank: len(unique_words_list) = 35071 unique words.\n",
      "2020-02-04 19:57:25,812 [MainThread  ] [INFO ]  DatasetsBank: len(unique_words_list) = 35071 unique words.\n",
      "I0204 19:57:36.682898 4668726720 datasets_bank.py:21] DatasetsBank: len(unique_words_list) = 40355 unique words.\n",
      "2020-02-04 19:57:36,682 [MainThread  ] [INFO ]  DatasetsBank: len(unique_words_list) = 40355 unique words.\n",
      "I0204 19:57:50.716990 4668726720 datasets_bank.py:21] DatasetsBank: len(unique_words_list) = 45881 unique words.\n",
      "2020-02-04 19:57:50,716 [MainThread  ] [INFO ]  DatasetsBank: len(unique_words_list) = 45881 unique words.\n",
      "I0204 19:57:50.717188 4668726720 main.py:149] create Word Sequence Indexer\n",
      "2020-02-04 19:57:50,717 [MainThread  ] [INFO ]  create Word Sequence Indexer\n",
      "create seq indexer BERT\n",
      "Traceback (most recent call last):\n",
      "  File \"main.py\", line 159, in <module>\n",
      "    word_seq_indexer = SeqIndexerBert(gpu=args.gpu, check_for_lowercase=args.check_for_lowercase, path_to_pretrained = args.path_to_bert, model_frozen = args.bert_frozen)\n",
      "  File \"/Users/valentindittmar/git/ma/targer/src/seq_indexers/seq_indexer_bert.py\", line 26, in __init__\n",
      "    self.tokenizer = tokenizer_custom_bert.FullTokenizer(path_to_pretrained + '/vocab.txt')\n",
      "  File \"/Users/valentindittmar/git/ma/targer/src/tokenizers/tokenizer_custom_bert.py\", line 244, in __init__\n",
      "    self.vocab = load_vocab(vocab_file)\n",
      "  File \"/Users/valentindittmar/git/ma/targer/src/tokenizers/tokenizer_custom_bert.py\", line 134, in load_vocab\n",
      "    with tf.gfile.GFile(vocab_file, \"r\") as reader:\n",
      "AttributeError: module 'tensorflow' has no attribute 'gfile'\n"
     ]
    }
   ],
   "source": [
    "#! python3 main.py --train \"/home/vika/targer/data/NER/Indian_dataset/train.csv\" --dev \"/home/vika/targer/data/NER/Indian_dataset/dev.csv\" --data-io connl-ner-2003 --evaluator f1-alpha-match-10 --model 'BiRNN' --opt adam --lr 0.001 --save-best yes --patience 20 --rnn-hidden-dim 200 --gpu 0 --test \"/home/vika/targer/data/NER/Indian_dataset/test.csv\" --elmo False --lr 0.001 --evaluator f1-macro --bert True \n",
    "! python3 main.py --train \"data/NER/CoNNL_2003_shared_task/train.txt\" --dev \"data/NER/CoNNL_2003_shared_task/dev.txt\" --data-io connl-ner-2003 --evaluator f1-alpha-match-10 --model 'BiRNN' --opt adam --lr 0.001 --save-best yes --patience 20 --rnn-hidden-dim 200 --gpu -1 --test \"data/NER/CoNNL_2003_shared_task/test.txt\" --elmo False --lr 0.001 --evaluator f1-macro --bert True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "can't assign to operator (<ipython-input-2-1e958a7a1ebd>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-1e958a7a1ebd>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    B-OBJ = f1 = 72.90, precision = 77.29, recall = 68.98\u001b[0m\n\u001b[0m                                                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m can't assign to operator\n"
     ]
    }
   ],
   "source": [
    "B-OBJ = f1 = 72.90, precision = 77.29, recall = 68.98\n",
    "B-PREDFULL = f1 = 61.46, precision = 54.22, recall = 70.93\n",
    "I-OBJ = f1 = 0.00, precision = 0.00, recall = 0.00\n",
    "I-PREDFULL = f1 = 67.58, precision = 64.35, recall = 71.15\n",
    "O = f1 = 96.41, precision = 95.67, recall = 97.16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Oct 15 01:03:23 2019       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 390.87                 Driver Version: 390.87                    |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce GTX 108...  Off  | 00000000:05:00.0 Off |                  N/A |\n",
      "| 35%   54C    P8    19W / 250W |    773MiB / 11170MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  GeForce GTX 108...  Off  | 00000000:06:00.0 Off |                  N/A |\n",
      "| 51%   73C    P2    85W / 250W |   4198MiB / 11178MiB |     28%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  GeForce GTX 108...  Off  | 00000000:09:00.0 Off |                  N/A |\n",
      "| 28%   47C    P8    16W / 250W |   2252MiB / 11178MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  GeForce GTX 108...  Off  | 00000000:0A:00.0 Off |                  N/A |\n",
      "| 27%   46C    P8    18W / 250W |    777MiB / 11178MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.load('answer.pth')\n",
    "b = torch.load('index.pth')\n",
    "c = torch.load('self_tensor1.pth')\n",
    "d = torch.load('token_tensor.pth')\n",
    "f = torch.load('encoded_layers.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.3613, -3.6562,  1.3574,  5.7123, -6.4745],\n",
       "        [ 1.3811, -3.5984,  1.4607,  6.8327, -7.5562],\n",
       "        [ 1.3470, -3.8593,  1.7187,  7.2637, -7.1724],\n",
       "        [ 1.2666, -3.1740,  1.2468,  6.8762, -7.5805],\n",
       "        [ 1.3244, -3.1887,  1.4618,  7.2793, -4.2320],\n",
       "        [ 1.4404, -3.5569,  1.4535,  7.7213, -7.4975],\n",
       "        [ 1.0332, -3.8156,  1.4881,  7.7240, -6.5270],\n",
       "        [ 1.3814, -2.9273,  1.4611,  7.4868, -6.3432],\n",
       "        [ 1.3607, -3.7643,  1.7103,  6.9592, -7.6278],\n",
       "        [ 0.6985, -3.8065,  1.3775,  5.7472, -7.6190],\n",
       "        [ 0.6302, -3.0828,  0.6989,  7.7069, -7.6807],\n",
       "        [ 0.7597, -3.7258,  0.9455,  7.6786, -7.1789],\n",
       "        [ 1.2883, -3.6161,  0.7259,  5.3001, -5.9318],\n",
       "        [ 0.6147, -3.6421,  1.4251,  6.2506, -7.3509],\n",
       "        [ 1.2497, -2.6943,  1.6016,  7.1442, -7.6118],\n",
       "        [ 1.1236, -3.7036,  1.5411,  7.0981, -7.6096],\n",
       "        [ 1.2301, -3.7275,  1.6789,  7.7935, -7.1704],\n",
       "        [ 1.2993, -3.2318,  1.4254,  6.7568, -7.2071],\n",
       "        [ 1.4457, -3.7872,  1.0614,  7.3247, -7.3499],\n",
       "        [ 0.1642, -3.7958,  1.5194,  5.9739, -6.7925],\n",
       "        [ 1.2940, -2.4832,  0.8999,  6.1876, -7.2753],\n",
       "        [ 1.3678, -2.9297,  1.1174,  7.8332, -7.7405],\n",
       "        [ 1.4128, -3.6737,  1.2945,  7.7787, -7.0268],\n",
       "        [ 1.3267, -3.7246,  1.0423,  7.4425, -6.8988],\n",
       "        [ 1.4694, -3.0134,  1.6233,  6.2878, -7.3289],\n",
       "        [ 1.4173, -3.9035,  1.6731,  3.5139, -7.6586],\n",
       "        [ 0.6341, -3.7488,  1.7582,  7.6026, -7.5752],\n",
       "        [ 1.0306, -3.7648,  1.5617,  7.9723, -5.7399],\n",
       "        [ 1.2359, -3.7764,  1.1162,  6.2315, -7.6824],\n",
       "        [ 0.5789, -3.0790,  1.2522,  7.0489, -6.9271],\n",
       "        [ 1.4525, -3.1791,  1.0223,  7.5639, -7.7040],\n",
       "        [ 1.1472, -3.6928,  1.4694,  7.7524, -5.6724],\n",
       "        [ 0.7386, -3.7838,  1.8370,  7.6712, -7.4108],\n",
       "        [ 0.7579, -3.7986,  1.4424,  7.6573, -7.4848],\n",
       "        [ 0.6869, -3.7816,  0.9460,  7.6862, -6.4637],\n",
       "        [ 0.6826, -3.8776,  1.6169,  6.9863, -7.2920],\n",
       "        [ 1.3037, -3.3347,  1.4570,  6.5263, -7.6096],\n",
       "        [ 1.2339, -3.8369,  1.5368,  6.7081, -6.2322],\n",
       "        [ 1.2822, -3.0550,  1.4977,  4.7067, -7.4864],\n",
       "        [ 1.4595, -3.0371,  1.6127,  6.9259, -5.9903],\n",
       "        [ 1.0545, -3.7181,  1.6507,  6.4583, -6.9786],\n",
       "        [ 1.0288, -3.2877,  1.6544,  5.1386, -7.4253],\n",
       "        [ 1.1833, -3.6563,  0.7355,  7.4577, -6.4076],\n",
       "        [ 1.3208, -2.8604,  1.5438,  7.5106, -6.8137],\n",
       "        [ 1.1425, -3.6329,  0.9223,  6.9618, -7.5776],\n",
       "        [ 1.3349, -2.5196,  0.7085,  7.6040, -4.5122],\n",
       "        [ 1.4719, -3.7945,  1.3103,  7.3323, -7.5794],\n",
       "        [ 1.1937, -3.1988,  1.4691,  7.6827, -7.6748],\n",
       "        [ 1.4341, -3.6144,  1.7574,  6.6370, -7.4999],\n",
       "        [ 1.2304, -3.5933,  0.5635,  7.1400, -6.8291],\n",
       "        [ 1.2429, -2.8142,  1.6366,  5.9786, -5.5539],\n",
       "        [ 0.7087, -3.6518,  1.5588,  7.6305, -7.7866],\n",
       "        [ 1.3591, -3.1859,  1.5476,  7.4503, -7.5586],\n",
       "        [ 1.3638, -3.8608,  1.3887,  7.7105, -7.0289],\n",
       "        [ 0.7966, -3.5834,  1.1699,  7.0906, -5.2908]],\n",
       "       device='cuda:0', grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0][0,:,:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_embeddings = []\n",
    "for batch_i in range(d[0].shape[0]): #batch_size\n",
    "    token_embeddings = []\n",
    "    for token_i in range(d[0].shape[1]):  #number of token in batch element\n",
    "        hidden_layers = [] \n",
    "        for layer_i in range(len(f[0])):\n",
    "            vec = f[0][layer_i][batch_i][token_i]\n",
    "            hidden_layers.append(vec)\n",
    "        token_embeddings.append(hidden_layers)\n",
    "    summed_last_4_layers = [torch.sum(torch.stack(layer)[-4:], 0) for layer in token_embeddings]\n",
    "    summed_last_4_layers = torch.stack(summed_last_4_layers)\n",
    "    batch_embeddings.append(summed_last_4_layers)\n",
    "\n",
    "answer = torch.stack(batch_embeddings) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.9893,  0.2826,  1.2681,  1.2064, -0.6610],\n",
       "        [ 0.4920,  0.4209,  1.1853,  1.1908, -0.6767],\n",
       "        [ 0.8199,  0.4248,  1.1671,  1.3297, -0.9326],\n",
       "        [ 0.6627,  0.5955,  1.4539,  1.2195, -0.5065],\n",
       "        [ 0.8780,  0.4360,  1.0800,  1.1473, -0.6266],\n",
       "        [ 0.8378,  0.4508,  1.2516,  1.3484, -0.8536],\n",
       "        [ 0.8358,  0.4692,  1.1450,  1.1634, -0.7156],\n",
       "        [ 0.7691,  0.1528,  0.9802,  1.1706, -0.6026],\n",
       "        [ 0.8942,  0.4878,  1.1039,  1.0773, -0.4014],\n",
       "        [ 0.8619,  0.2270,  0.7334,  1.0017, -0.6570],\n",
       "        [ 0.8472,  0.6062,  1.1520,  1.1398, -0.7520],\n",
       "        [ 0.8631,  0.4388,  1.1885,  1.1244, -0.4540],\n",
       "        [ 0.8302,  0.2417,  1.0440,  1.1638, -0.5801],\n",
       "        [ 0.8355,  0.3575,  0.5310,  1.1016, -0.6472],\n",
       "        [ 0.8825,  0.4304,  1.0542,  1.4129, -0.5985],\n",
       "        [ 0.8327,  0.4202,  0.7623,  1.2089, -0.6835],\n",
       "        [ 0.5010,  0.3375,  1.1762,  1.3705, -0.7708],\n",
       "        [ 0.5229,  0.2402,  0.5606,  1.3117, -0.6126],\n",
       "        [ 0.3261,  0.5190,  1.4746,  1.5802, -0.9162],\n",
       "        [ 0.6279,  0.6475,  1.4346,  1.1845, -0.7804],\n",
       "        [ 0.8625,  0.6020,  1.2012,  0.9832, -0.4767],\n",
       "        [ 0.9201,  0.4973,  1.1847,  1.2178, -0.6750],\n",
       "        [ 0.8199,  0.4691,  1.1762,  1.3352, -0.8467],\n",
       "        [ 0.8481,  0.4769,  1.1664,  1.2014, -0.6567],\n",
       "        [ 0.8955,  0.2363,  0.5907,  0.9841, -0.5415],\n",
       "        [ 0.8324,  0.5121,  1.4651,  1.5435, -0.5712],\n",
       "        [ 0.8153,  0.4824,  1.0580,  1.0742, -0.6613],\n",
       "        [ 0.7735,  0.4442,  1.0734,  1.1477, -0.5032],\n",
       "        [ 0.7196,  0.4053,  1.1945,  1.1278, -0.6685],\n",
       "        [ 0.7683,  0.4303,  0.7860,  1.1973, -0.4368],\n",
       "        [ 0.9397,  0.4871,  1.2202,  0.4497, -0.6903],\n",
       "        [ 0.8003,  0.4281,  1.1286,  1.0764, -0.4500],\n",
       "        [ 0.3760,  0.5902,  1.2131,  1.0877, -0.6772],\n",
       "        [ 0.8919,  0.2616,  1.4818,  0.3577, -0.6906],\n",
       "        [ 0.8404,  0.1829,  1.1541,  1.1781, -0.7276],\n",
       "        [ 0.8182,  0.4506,  1.1428,  1.2073, -0.7235],\n",
       "        [ 0.7992,  0.4273,  1.3236,  1.5331, -0.5604],\n",
       "        [ 0.8307,  0.4665,  0.9022,  1.1926, -0.7072],\n",
       "        [ 0.8489,  0.5785,  1.2172,  1.1863, -0.6843],\n",
       "        [ 0.8784,  0.5300,  0.9135,  1.1950, -0.5535],\n",
       "        [ 0.8571,  0.3913,  1.2016,  1.1582, -0.7119],\n",
       "        [ 0.8229,  0.2452,  1.1590,  1.0661, -0.4848],\n",
       "        [ 0.9162,  0.2439,  1.1568,  1.2002, -0.7773],\n",
       "        [ 0.8571,  0.4202,  1.2259,  1.1826, -0.6037],\n",
       "        [ 0.8626,  0.4798,  1.2405,  1.1037, -0.6785],\n",
       "        [ 0.8273,  0.4696,  0.9978,  0.9628, -0.6906],\n",
       "        [ 0.4528,  0.3746,  0.8458,  1.0841, -0.6206],\n",
       "        [ 0.2784,  0.4435,  1.5094,  1.5392, -0.8048],\n",
       "        [ 0.5037,  0.4761,  1.2314,  1.1613, -0.6809],\n",
       "        [ 0.6989,  0.3971,  1.1557,  1.0766, -0.7321],\n",
       "        [ 0.8517,  0.5018,  1.1317,  1.1681, -0.4595],\n",
       "        [ 0.8205,  0.2426,  0.8359,  1.1794, -0.6745],\n",
       "        [ 0.7935,  0.1364,  1.4021,  1.2245, -0.6194],\n",
       "        [ 0.8195,  0.5095,  0.7418,  1.1997, -0.4164],\n",
       "        [ 0.2914,  0.4264,  1.0864,  0.4416, -0.7385]],\n",
       "       device='cuda:0', grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer[0,:,:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0648, -0.1361, -0.0713, -0.0623, -0.1207, -0.1038, -0.1765, -0.0665,\n",
       "        -0.0741, -0.1469,  0.0411, -0.0504, -0.1846, -0.0663, -0.1573, -0.1244,\n",
       "        -0.1101, -0.1713, -0.0625, -0.1255, -0.1611, -0.1153, -0.1704, -0.1119,\n",
       "        -0.0439, -0.0675, -0.1214, -0.1203, -0.1019, -0.1244, -0.1384, -0.0495,\n",
       "        -0.0975, -0.1016, -0.0716, -0.0783, -0.0182, -0.1012, -0.0937, -0.1249,\n",
       "        -0.0844, -0.0722, -0.1134, -0.1770, -0.1207, -0.1154, -0.0988, -0.1692,\n",
       "         0.0002, -0.1079, -0.0402, -0.1104, -0.0891, -0.0841, -0.0974],\n",
       "       device='cuda:0', grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f[0][10][0,:, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2310, -1.4246, -0.5752,  0.7972, -2.9637],\n",
       "        [ 0.1142, -1.9174, -0.7985,  0.4218, -3.1890],\n",
       "        [-0.0349, -1.4001, -0.8697,  1.3980, -4.1933],\n",
       "        [ 1.0246, -1.9242, -0.6518,  0.9145, -3.3030],\n",
       "        [-0.2269, -1.2031, -0.6052,  0.9763, -4.3236],\n",
       "        [ 0.7270, -1.7006, -0.6023,  0.7419, -3.6929],\n",
       "        [ 0.7520, -1.1643, -0.6135,  0.6496, -4.2519],\n",
       "        [ 0.2550, -1.6039, -0.3588,  1.0800, -4.3409],\n",
       "        [ 0.6947, -1.3939, -0.8647,  0.3908, -2.8573],\n",
       "        [ 0.7784, -1.6870, -0.5853,  1.0189, -2.8029],\n",
       "        [ 0.2553, -1.2303, -0.9118,  0.7660, -3.6057],\n",
       "        [-0.0664, -0.1857, -1.0239,  1.0297, -4.3019],\n",
       "        [ 0.0318, -1.7155, -0.5830,  1.0159, -3.3180],\n",
       "        [ 1.2713, -1.5704, -0.8206,  0.5319, -3.4467],\n",
       "        [ 0.4250, -1.2194, -0.4668,  0.8713, -4.2420],\n",
       "        [ 0.7625, -1.6525, -0.2535,  0.7632, -4.2304],\n",
       "        [ 0.1344, -0.8862, -0.5777,  0.9692, -3.8707],\n",
       "        [ 0.6070, -1.4817, -0.6725,  0.7431, -4.0146],\n",
       "        [ 0.3641, -1.7730,  0.1540,  0.8793, -3.0920],\n",
       "        [-0.3696, -1.7412, -0.8550,  0.8567, -4.1590],\n",
       "        [ 0.5718, -1.5867, -0.9027,  0.2680, -4.3277],\n",
       "        [ 0.8005, -1.6539, -0.3953,  1.2190, -4.3961],\n",
       "        [ 0.5013, -1.4836, -0.1864,  0.8220, -3.6217],\n",
       "        [ 1.1672, -1.1731, -0.4970,  0.6954, -4.1610],\n",
       "        [ 0.5822, -1.7683, -0.4506,  0.5928, -3.4043],\n",
       "        [ 0.2777, -1.5455, -0.4996,  0.5217, -4.4570],\n",
       "        [ 0.6868, -1.7071, -0.8946,  0.8673, -3.5143],\n",
       "        [ 0.8198, -1.3617, -0.8182,  0.6301, -4.0331],\n",
       "        [ 0.0467, -0.6535, -0.7650,  0.8448, -3.7458],\n",
       "        [ 0.0165, -0.4177, -0.8413,  0.6806, -4.3450],\n",
       "        [ 0.4255, -1.4385, -0.9519,  0.9625, -3.5137],\n",
       "        [ 0.3887, -1.2615, -0.8193,  0.2415, -3.7856],\n",
       "        [-0.3133, -1.5136, -0.3547,  1.1303, -2.5977],\n",
       "        [ 0.6180, -1.5657, -0.8874,  0.8668, -3.7304],\n",
       "        [ 0.4826, -0.6751, -0.6942,  0.0043, -3.3763],\n",
       "        [ 0.1539, -1.5359, -0.8803,  0.6440, -2.2088],\n",
       "        [ 0.3063, -1.3290, -0.6183,  0.6313, -3.8551],\n",
       "        [ 0.3928, -1.6089, -0.8897,  0.3393, -4.0683],\n",
       "        [ 0.8980, -1.5715, -0.6434,  0.9248, -4.2299],\n",
       "        [ 0.1388, -1.5038, -0.8056,  1.0345, -4.5038],\n",
       "        [ 0.9339, -1.4926, -0.3588,  1.0935, -3.7367],\n",
       "        [ 0.2728, -1.4645, -0.9202,  0.9282, -3.5692],\n",
       "        [ 0.6380, -1.7855, -0.7479,  0.8873, -3.1464],\n",
       "        [ 0.9215, -1.3539, -0.8001,  0.5738, -4.0076],\n",
       "        [ 0.4541, -1.5033, -0.4076,  1.1835, -2.6305]],\n",
       "       device='cuda:0', grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0][0, :, :5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 60])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "args train = /home/vika/targer/data/NER/Indian_dataset/train.csv\n",
      "Loading from /home/vika/targer/data/NER/Indian_dataset/train.csv: 23999 samples, 448609 words.\n",
      "Loading from /home/vika/targer/data/NER/Indian_dataset/dev.csv: 1803 samples, 33827 words.\n",
      "Loading from /home/vika/targer/data/NER/Indian_dataset/test.csv: 1093 samples, 18921 words.\n",
      "DatasetsBank: len(unique_words_list) = 16615 unique words.\n",
      "DatasetsBank: len(unique_words_list) = 17208 unique words.\n",
      "DatasetsBank: len(unique_words_list) = 17829 unique words.\n",
      "qu True\n",
      "True\n",
      "False\n",
      "2\n",
      "create seq indexer elmo!\n",
      "\n",
      "load_vocabulary_from_tag_sequences:\n",
      " -- class_num = 5\n",
      " -- {'<pad>': 0, 'NONE': 1, 'PROD1': 2, 'PRED': 3, 'ASP': 4, 'PROD2': 5}\n",
      "LayerContextWordEmbeddings init\n",
      "\n",
      "Start training...\n",
      "\n",
      "\n",
      "\n",
      "++ predicting, batch 2399/2399 (100.00%).\n",
      "F1 scores\n",
      "------------------------\n",
      "            ASP = 2.12\n",
      "           NONE = 1.40\n",
      "           PRED = 12.96\n",
      "          PROD1 = 4.80\n",
      "          PROD2 = 7.91\n",
      "------------------------\n",
      "Macro-F1 = 5.839\n",
      "\n",
      "\n",
      "++ predicting, batch 180/180 (100.00%).\n",
      "F1 scores\n",
      "------------------------\n",
      "            ASP = 2.29\n",
      "           NONE = 1.32\n",
      "           PRED = 12.71\n",
      "          PROD1 = 4.88\n",
      "          PROD2 = 7.67\n",
      "------------------------\n",
      "Macro-F1 = 5.775\n",
      "\n",
      "\n",
      "++ predicting, batch 109/109 (100.00%).\n",
      "F1 scores\n",
      "------------------------\n",
      "            ASP = 8.06\n",
      "           NONE = 1.82\n",
      "           PRED = 13.22\n",
      "          PROD1 = 6.03\n",
      "          PROD2 = 13.77\n",
      "------------------------\n",
      "Macro-F1 = 8.580\n",
      "\n",
      "== eval epoch 0/100 \"f1-macro\" train / dev / test | 5.84 / 5.77 / 8.58.\n",
      "## [BEST epoch], 394 seconds.\n",
      "\n",
      "-- train epoch 1/100, batch 2399/2399 (100.00%), loss = 20.71.\n",
      "\n",
      "++ predicting, batch 2399/2399 (100.00%).\n",
      "F1 scores\n",
      "------------------------\n",
      "            ASP = 60.37\n",
      "           NONE = 96.07\n",
      "           PRED = 80.17\n",
      "          PROD1 = 78.27\n",
      "          PROD2 = 77.01\n",
      "------------------------\n",
      "Macro-F1 = 78.380\n",
      "\n",
      "\n",
      "++ predicting, batch 180/180 (100.00%).\n",
      "F1 scores\n",
      "------------------------\n",
      "            ASP = 60.55\n",
      "           NONE = 95.97\n",
      "           PRED = 80.81\n",
      "          PROD1 = 76.48\n",
      "          PROD2 = 73.27\n",
      "------------------------\n",
      "Macro-F1 = 77.415\n",
      "\n",
      "\n",
      "++ predicting, batch 109/109 (100.00%).\n",
      "F1 scores\n",
      "------------------------\n",
      "            ASP = 29.28\n",
      "           NONE = 83.71\n",
      "           PRED = 48.00\n",
      "          PROD1 = 41.96\n",
      "          PROD2 = 29.36\n",
      "------------------------\n",
      "Macro-F1 = 46.463\n",
      "\n",
      "== eval epoch 1/100 \"f1-macro\" train / dev / test | 78.38 / 77.42 / 46.46.\n",
      "## [BEST epoch], 779 seconds.\n",
      "\n",
      "-- train epoch 2/100, batch 2399/2399 (100.00%), loss = 13.91.\n",
      "\n",
      "++ predicting, batch 2399/2399 (100.00%).\n",
      "F1 scores\n",
      "------------------------\n",
      "            ASP = 67.28\n",
      "           NONE = 96.36\n",
      "           PRED = 82.33\n",
      "          PROD1 = 82.53\n",
      "          PROD2 = 78.60\n",
      "------------------------\n",
      "Macro-F1 = 81.421\n",
      "\n",
      "\n",
      "++ predicting, batch 180/180 (100.00%).\n",
      "F1 scores\n",
      "------------------------\n",
      "            ASP = 65.55\n",
      "           NONE = 96.13\n",
      "           PRED = 82.39\n",
      "          PROD1 = 80.14\n",
      "          PROD2 = 76.37\n",
      "------------------------\n",
      "Macro-F1 = 80.115\n",
      "\n",
      "\n",
      "++ predicting, batch 109/109 (100.00%).\n",
      "F1 scores\n",
      "------------------------\n",
      "            ASP = 35.62\n",
      "           NONE = 84.29\n",
      "           PRED = 56.50\n",
      "          PROD1 = 47.57\n",
      "          PROD2 = 49.60\n",
      "------------------------\n",
      "Macro-F1 = 54.717\n",
      "\n",
      "== eval epoch 2/100 \"f1-macro\" train / dev / test | 81.42 / 80.12 / 54.72.\n",
      "## [BEST epoch], 750 seconds.\n",
      "\n",
      "-- train epoch 3/100, batch 2399/2399 (100.00%), loss = 12.95.\n",
      "\n",
      "++ predicting, batch 2399/2399 (100.00%).\n",
      "F1 scores\n",
      "------------------------\n",
      "            ASP = 70.27\n",
      "           NONE = 97.10\n",
      "           PRED = 84.61\n",
      "          PROD1 = 85.79\n",
      "          PROD2 = 84.52\n",
      "------------------------\n",
      "Macro-F1 = 84.460\n",
      "\n",
      "\n",
      "++ predicting, batch 180/180 (100.00%).\n",
      "F1 scores\n",
      "------------------------\n",
      "            ASP = 70.46\n",
      "           NONE = 96.88\n",
      "           PRED = 84.31\n",
      "          PROD1 = 83.20\n",
      "          PROD2 = 79.68\n",
      "------------------------\n",
      "Macro-F1 = 82.905\n",
      "\n",
      "\n",
      "++ predicting, batch 109/109 (100.00%).\n",
      "F1 scores\n",
      "------------------------\n",
      "            ASP = 39.58\n",
      "           NONE = 84.46\n",
      "           PRED = 58.58\n",
      "          PROD1 = 46.97\n",
      "          PROD2 = 41.34\n",
      "------------------------\n",
      "Macro-F1 = 54.186\n",
      "\n",
      "== eval epoch 3/100 \"f1-macro\" train / dev / test | 84.46 / 82.91 / 54.19.\n",
      "## [BEST epoch], 730 seconds.\n",
      "\n",
      "-- train epoch 4/100, batch 2399/2399 (100.00%), loss = 11.83.\n",
      "\n",
      "++ predicting, batch 2399/2399 (100.00%).\n",
      "F1 scores\n",
      "------------------------\n",
      "            ASP = 69.65\n",
      "           NONE = 97.29\n",
      "           PRED = 85.24\n",
      "          PROD1 = 87.71\n",
      "          PROD2 = 85.27\n",
      "------------------------\n",
      "Macro-F1 = 85.033\n",
      "\n",
      "\n",
      "++ predicting, batch 180/180 (100.00%).\n",
      "F1 scores\n",
      "------------------------\n",
      "            ASP = 67.14\n",
      "           NONE = 96.96\n",
      "           PRED = 84.39\n",
      "          PROD1 = 85.18\n",
      "          PROD2 = 80.69\n",
      "------------------------\n",
      "Macro-F1 = 82.871\n",
      "\n",
      "\n",
      "++ predicting, batch 109/109 (100.00%).\n",
      "F1 scores\n",
      "------------------------\n",
      "            ASP = 32.57\n",
      "           NONE = 84.52\n",
      "           PRED = 62.71\n",
      "          PROD1 = 43.83\n",
      "          PROD2 = 38.96\n",
      "------------------------\n",
      "Macro-F1 = 52.519\n",
      "\n",
      "== eval epoch 4/100 \"f1-macro\" train / dev / test | 85.03 / 82.87 / 52.52.\n",
      "## [no improvement micro-f1 on DEV during the last 1 epochs (best_f1_dev=82.91), 833 seconds].\n",
      "\n",
      "-- train epoch 5/100, batch 2399/2399 (100.00%), loss = 11.55.\n",
      "\n",
      "++ predicting, batch 2399/2399 (100.00%).\n",
      "F1 scores\n",
      "------------------------\n",
      "            ASP = 70.28\n",
      "           NONE = 97.54\n",
      "           PRED = 86.14\n",
      "          PROD1 = 88.85\n",
      "          PROD2 = 88.32\n",
      "------------------------\n",
      "Macro-F1 = 86.226\n",
      "\n",
      "\n",
      "++ predicting, batch 180/180 (100.00%).\n",
      "F1 scores\n",
      "------------------------\n",
      "            ASP = 67.24\n",
      "           NONE = 97.15\n",
      "           PRED = 84.96\n",
      "          PROD1 = 86.78\n",
      "          PROD2 = 84.30\n",
      "------------------------\n",
      "Macro-F1 = 84.087\n",
      "\n",
      "\n",
      "++ predicting, batch 109/109 (100.00%).\n",
      "F1 scores\n",
      "------------------------\n",
      "            ASP = 37.91\n",
      "           NONE = 85.61\n",
      "           PRED = 64.87\n",
      "          PROD1 = 51.13\n",
      "          PROD2 = 53.39\n",
      "------------------------\n",
      "Macro-F1 = 58.583\n",
      "\n",
      "== eval epoch 5/100 \"f1-macro\" train / dev / test | 86.23 / 84.09 / 58.58.\n",
      "## [BEST epoch], 782 seconds.\n",
      "\n",
      "-- train epoch 6/100, batch 2399/2399 (100.00%), loss = 10.88.\n",
      "\n",
      "++ predicting, batch 2399/2399 (100.00%).\n",
      "F1 scores\n",
      "------------------------\n",
      "            ASP = 72.75\n",
      "           NONE = 97.08\n",
      "           PRED = 82.97\n",
      "          PROD1 = 88.88\n",
      "          PROD2 = 87.46\n",
      "------------------------\n",
      "Macro-F1 = 85.827\n",
      "\n",
      "\n",
      "++ predicting, batch 180/180 (100.00%).\n",
      "F1 scores\n",
      "------------------------\n",
      "            ASP = 69.50\n",
      "           NONE = 96.42\n",
      "           PRED = 80.68\n",
      "          PROD1 = 85.34\n",
      "          PROD2 = 82.55\n",
      "------------------------\n",
      "Macro-F1 = 82.900\n",
      "\n",
      "\n",
      "++ predicting, batch 109/109 (100.00%).\n",
      "F1 scores\n",
      "------------------------\n",
      "            ASP = 44.83\n",
      "           NONE = 84.16\n",
      "           PRED = 64.50\n",
      "          PROD1 = 54.89\n",
      "          PROD2 = 55.88\n",
      "------------------------\n",
      "Macro-F1 = 60.853\n",
      "\n",
      "== eval epoch 6/100 \"f1-macro\" train / dev / test | 85.83 / 82.90 / 60.85.\n",
      "## [no improvement micro-f1 on DEV during the last 1 epochs (best_f1_dev=84.09), 761 seconds].\n",
      "\n",
      "-- train epoch 7/100, batch 2399/2399 (100.00%), loss = 10.09.\n",
      "\n",
      "++ predicting, batch 2399/2399 (100.00%).\n",
      "F1 scores\n",
      "------------------------\n",
      "            ASP = 74.39\n",
      "           NONE = 97.72\n",
      "           PRED = 86.76\n",
      "          PROD1 = 90.30\n",
      "          PROD2 = 88.73\n",
      "------------------------\n",
      "Macro-F1 = 87.579\n",
      "\n",
      "\n",
      "++ predicting, batch 180/180 (100.00%).\n",
      "F1 scores\n",
      "------------------------\n",
      "            ASP = 70.63\n",
      "           NONE = 97.19\n",
      "           PRED = 85.39\n",
      "          PROD1 = 87.02\n",
      "          PROD2 = 83.04\n",
      "------------------------\n",
      "Macro-F1 = 84.655\n",
      "\n",
      "\n",
      "++ predicting, batch 109/109 (100.00%).\n",
      "F1 scores\n",
      "------------------------\n",
      "            ASP = 33.21\n",
      "           NONE = 85.70\n",
      "           PRED = 64.31\n",
      "          PROD1 = 53.93\n",
      "          PROD2 = 44.19\n",
      "------------------------\n",
      "Macro-F1 = 56.267\n",
      "\n",
      "== eval epoch 7/100 \"f1-macro\" train / dev / test | 87.58 / 84.65 / 56.27.\n",
      "## [BEST epoch], 761 seconds.\n",
      "\n",
      "-- train epoch 8/100, batch 2399/2399 (100.00%), loss = 10.05.\n",
      "\n",
      "++ predicting, batch 2399/2399 (100.00%).\n",
      "F1 scores\n",
      "------------------------\n",
      "            ASP = 75.86\n",
      "           NONE = 97.90\n",
      "           PRED = 87.73\n",
      "          PROD1 = 90.96\n",
      "          PROD2 = 90.21\n",
      "------------------------\n",
      "Macro-F1 = 88.531\n",
      "\n",
      "\n",
      "++ predicting, batch 180/180 (100.00%).\n",
      "F1 scores\n",
      "------------------------\n",
      "            ASP = 70.33\n",
      "           NONE = 97.24\n",
      "           PRED = 85.36\n",
      "          PROD1 = 86.96\n",
      "          PROD2 = 84.97\n",
      "------------------------\n",
      "Macro-F1 = 84.973\n",
      "\n",
      "\n",
      "++ predicting, batch 109/109 (100.00%).\n",
      "F1 scores\n",
      "------------------------\n",
      "            ASP = 42.65\n",
      "           NONE = 85.15\n",
      "           PRED = 67.25\n",
      "          PROD1 = 49.68\n",
      "          PROD2 = 41.56\n",
      "------------------------\n",
      "Macro-F1 = 57.257\n",
      "\n",
      "== eval epoch 8/100 \"f1-macro\" train / dev / test | 88.53 / 84.97 / 57.26.\n",
      "## [BEST epoch], 722 seconds.\n",
      "\n",
      "-- train epoch 9/100, batch 2399/2399 (100.00%), loss = 8.76.\n",
      "\n",
      "++ predicting, batch 2399/2399 (100.00%).\n",
      "F1 scores\n",
      "------------------------\n",
      "            ASP = 76.19\n",
      "           NONE = 98.04\n",
      "           PRED = 88.41\n",
      "          PROD1 = 91.87\n",
      "          PROD2 = 91.15\n",
      "------------------------\n",
      "Macro-F1 = 89.131\n",
      "\n",
      "\n",
      "++ predicting, batch 180/180 (100.00%).\n",
      "F1 scores\n",
      "------------------------\n",
      "            ASP = 71.47\n",
      "           NONE = 97.44\n",
      "           PRED = 86.12\n",
      "          PROD1 = 88.37\n",
      "          PROD2 = 85.98\n",
      "------------------------\n",
      "Macro-F1 = 85.876\n",
      "\n",
      "\n",
      "++ predicting, batch 109/109 (100.00%).\n",
      "F1 scores\n",
      "------------------------\n",
      "            ASP = 44.87\n",
      "           NONE = 86.26\n",
      "           PRED = 68.09\n",
      "          PROD1 = 53.58\n",
      "          PROD2 = 59.68\n",
      "------------------------\n",
      "Macro-F1 = 62.494\n",
      "\n",
      "== eval epoch 9/100 \"f1-macro\" train / dev / test | 89.13 / 85.88 / 62.49.\n",
      "## [BEST epoch], 598 seconds.\n",
      "\n",
      "-- train epoch 10/100, batch 2399/2399 (100.00%), loss = 8.45.\n",
      "\n",
      "++ predicting, batch 2399/2399 (100.00%).\n",
      "F1 scores\n",
      "------------------------\n",
      "            ASP = 77.10\n",
      "           NONE = 97.98\n",
      "           PRED = 88.38\n",
      "          PROD1 = 90.88\n",
      "          PROD2 = 90.21\n",
      "------------------------\n",
      "Macro-F1 = 88.909\n",
      "\n",
      "\n",
      "++ predicting, batch 180/180 (100.00%).\n",
      "F1 scores\n",
      "------------------------\n",
      "            ASP = 71.54\n",
      "           NONE = 97.31\n",
      "           PRED = 86.42\n",
      "          PROD1 = 86.32\n",
      "          PROD2 = 84.51\n",
      "------------------------\n",
      "Macro-F1 = 85.220\n",
      "\n",
      "\n",
      "++ predicting, batch 109/109 (100.00%).\n",
      "F1 scores\n",
      "------------------------\n",
      "            ASP = 40.99\n",
      "           NONE = 84.81\n",
      "           PRED = 71.04\n",
      "          PROD1 = 53.27\n",
      "          PROD2 = 49.66\n",
      "------------------------\n",
      "Macro-F1 = 59.955\n",
      "\n",
      "== eval epoch 10/100 \"f1-macro\" train / dev / test | 88.91 / 85.22 / 59.95.\n",
      "## [no improvement micro-f1 on DEV during the last 1 epochs (best_f1_dev=85.88), 597 seconds].\n",
      "\n",
      "-- train epoch 11/100, batch 2399/2399 (100.00%), loss = 8.65.\n",
      "\n",
      "++ predicting, batch 2399/2399 (100.00%).\n",
      "F1 scores\n",
      "------------------------\n",
      "            ASP = 76.85\n",
      "           NONE = 98.13\n",
      "           PRED = 87.99\n",
      "          PROD1 = 92.77\n",
      "          PROD2 = 91.88\n",
      "------------------------\n",
      "Macro-F1 = 89.524\n",
      "\n",
      "\n",
      "++ predicting, batch 180/180 (100.00%).\n",
      "F1 scores\n",
      "------------------------\n",
      "            ASP = 71.52\n",
      "           NONE = 97.51\n",
      "           PRED = 86.33\n",
      "          PROD1 = 88.13\n",
      "          PROD2 = 86.17\n",
      "------------------------\n",
      "Macro-F1 = 85.932\n",
      "\n",
      "\n",
      "++ predicting, batch 109/109 (100.00%).\n",
      "F1 scores\n",
      "------------------------\n",
      "            ASP = 33.87\n",
      "           NONE = 86.00\n",
      "           PRED = 68.47\n",
      "          PROD1 = 53.89\n",
      "          PROD2 = 55.61\n",
      "------------------------\n",
      "Macro-F1 = 59.566\n",
      "\n",
      "== eval epoch 11/100 \"f1-macro\" train / dev / test | 89.52 / 85.93 / 59.57.\n",
      "## [BEST epoch], 594 seconds.\n",
      "\n",
      "-- train epoch 12/100, batch 2399/2399 (100.00%), loss = 8.68.\n",
      "\n",
      "++ predicting, batch 2399/2399 (100.00%).\n",
      "F1 scores\n",
      "------------------------\n",
      "            ASP = 78.95\n",
      "           NONE = 98.24\n",
      "           PRED = 88.72\n",
      "          PROD1 = 93.57\n",
      "          PROD2 = 92.05\n",
      "------------------------\n",
      "Macro-F1 = 90.307\n",
      "\n",
      "\n",
      "++ predicting, batch 180/180 (100.00%).\n",
      "F1 scores\n",
      "------------------------\n",
      "            ASP = 73.87\n",
      "           NONE = 97.58\n",
      "           PRED = 86.72\n",
      "          PROD1 = 88.43\n",
      "          PROD2 = 86.06\n",
      "------------------------\n",
      "Macro-F1 = 86.530\n",
      "\n",
      "\n",
      "++ predicting, batch 109/109 (100.00%).\n",
      "F1 scores\n",
      "------------------------\n",
      "            ASP = 41.16\n",
      "           NONE = 86.58\n",
      "           PRED = 70.32\n",
      "          PROD1 = 55.54\n",
      "          PROD2 = 59.55\n",
      "------------------------\n",
      "Macro-F1 = 62.629\n",
      "\n",
      "== eval epoch 12/100 \"f1-macro\" train / dev / test | 90.31 / 86.53 / 62.63.\n",
      "## [BEST epoch], 599 seconds.\n",
      "\n",
      "-- train epoch 13/100, batch 2399/2399 (100.00%), loss = 7.86.\n",
      "\n",
      "++ predicting, batch 2399/2399 (100.00%).\n",
      "F1 scores\n",
      "------------------------\n",
      "            ASP = 78.90\n",
      "           NONE = 98.31\n",
      "           PRED = 89.19\n",
      "          PROD1 = 93.95\n",
      "          PROD2 = 93.05\n",
      "------------------------\n",
      "Macro-F1 = 90.682\n",
      "\n",
      "\n",
      "++ predicting, batch 180/180 (100.00%).\n",
      "F1 scores\n",
      "------------------------\n",
      "            ASP = 73.36\n",
      "           NONE = 97.59\n",
      "           PRED = 86.37\n",
      "          PROD1 = 89.14\n",
      "          PROD2 = 87.04\n",
      "------------------------\n",
      "Macro-F1 = 86.701\n",
      "\n",
      "\n",
      "++ predicting, batch 109/109 (100.00%).\n",
      "F1 scores\n",
      "------------------------\n",
      "            ASP = 41.89\n",
      "           NONE = 86.40\n",
      "           PRED = 69.43\n",
      "          PROD1 = 51.06\n",
      "          PROD2 = 57.57\n",
      "------------------------\n",
      "Macro-F1 = 61.269\n",
      "\n",
      "== eval epoch 13/100 \"f1-macro\" train / dev / test | 90.68 / 86.70 / 61.27.\n",
      "## [BEST epoch], 599 seconds.\n",
      "\n",
      "-- train epoch 14/100, batch 2399/2399 (100.00%), loss = 7.66.\n",
      "\n",
      "++ predicting, batch 2399/2399 (100.00%).\n",
      "F1 scores\n",
      "------------------------\n",
      "            ASP = 79.87\n",
      "           NONE = 98.37\n",
      "           PRED = 89.40\n",
      "          PROD1 = 94.00\n",
      "          PROD2 = 93.54\n",
      "------------------------\n",
      "Macro-F1 = 91.037\n",
      "\n",
      "\n",
      "++ predicting, batch 180/180 (100.00%).\n",
      "F1 scores\n",
      "------------------------\n",
      "            ASP = 73.19\n",
      "           NONE = 97.57\n",
      "           PRED = 86.71\n",
      "          PROD1 = 88.69\n",
      "          PROD2 = 86.97\n",
      "------------------------\n",
      "Macro-F1 = 86.625\n",
      "\n",
      "\n",
      "++ predicting, batch 109/109 (100.00%).\n",
      "F1 scores\n",
      "------------------------\n",
      "            ASP = 41.57\n",
      "           NONE = 86.23\n",
      "           PRED = 69.67\n",
      "          PROD1 = 51.65\n",
      "          PROD2 = 58.05\n",
      "------------------------\n",
      "Macro-F1 = 61.433\n",
      "\n",
      "== eval epoch 14/100 \"f1-macro\" train / dev / test | 91.04 / 86.63 / 61.43.\n",
      "## [no improvement micro-f1 on DEV during the last 1 epochs (best_f1_dev=86.70), 596 seconds].\n",
      "\n",
      "-- train epoch 15/100, batch 2399/2399 (100.00%), loss = 7.69.\n",
      "\n",
      "++ predicting, batch 2399/2399 (100.00%).\n",
      "F1 scores\n",
      "------------------------\n",
      "            ASP = 80.70\n",
      "           NONE = 98.45\n",
      "           PRED = 89.92\n",
      "          PROD1 = 94.65\n",
      "          PROD2 = 93.42\n",
      "------------------------\n",
      "Macro-F1 = 91.428\n",
      "\n",
      "\n",
      "++ predicting, batch 180/180 (100.00%).\n",
      "F1 scores\n",
      "------------------------\n",
      "            ASP = 73.26\n",
      "           NONE = 97.57\n",
      "           PRED = 86.55\n",
      "          PROD1 = 89.25\n",
      "          PROD2 = 86.04\n",
      "------------------------\n",
      "Macro-F1 = 86.535\n",
      "\n",
      "\n",
      "++ predicting, batch 109/109 (100.00%).\n",
      "F1 scores\n",
      "------------------------\n",
      "            ASP = 42.85\n",
      "           NONE = 86.39\n",
      "           PRED = 71.32\n",
      "          PROD1 = 55.11\n",
      "          PROD2 = 57.16\n",
      "------------------------\n",
      "Macro-F1 = 62.566\n",
      "\n",
      "== eval epoch 15/100 \"f1-macro\" train / dev / test | 91.43 / 86.54 / 62.57.\n",
      "## [no improvement micro-f1 on DEV during the last 2 epochs (best_f1_dev=86.70), 597 seconds].\n",
      "\n",
      "-- train epoch 16/100, batch 2399/2399 (100.00%), loss = 7.70.\n",
      "\n",
      "++ predicting, batch 2399/2399 (100.00%).\n",
      "F1 scores\n",
      "------------------------\n",
      "            ASP = 80.80\n",
      "           NONE = 98.48\n",
      "           PRED = 89.87\n",
      "          PROD1 = 94.91\n",
      "          PROD2 = 94.35\n",
      "------------------------\n",
      "Macro-F1 = 91.679\n",
      "\n",
      "\n",
      "++ predicting, batch 180/180 (100.00%).\n",
      "F1 scores\n",
      "------------------------\n",
      "            ASP = 73.79\n",
      "           NONE = 97.64\n",
      "           PRED = 86.90\n",
      "          PROD1 = 89.47\n",
      "          PROD2 = 88.30\n",
      "------------------------\n",
      "Macro-F1 = 87.219\n",
      "\n",
      "\n",
      "++ predicting, batch 109/109 (100.00%).\n",
      "F1 scores\n",
      "------------------------\n",
      "            ASP = 49.79\n",
      "           NONE = 86.53\n",
      "           PRED = 72.02\n",
      "          PROD1 = 57.26\n",
      "          PROD2 = 58.42\n",
      "------------------------\n",
      "Macro-F1 = 64.803\n",
      "\n",
      "== eval epoch 16/100 \"f1-macro\" train / dev / test | 91.68 / 87.22 / 64.80.\n",
      "## [BEST epoch], 597 seconds.\n",
      "\n",
      "-- train epoch 17/100, batch 2399/2399 (100.00%), loss = 7.04.\n",
      "\n",
      "++ predicting, batch 2399/2399 (100.00%).\n",
      "F1 scores\n",
      "------------------------\n",
      "            ASP = 80.14\n",
      "           NONE = 98.54\n",
      "           PRED = 90.36\n",
      "          PROD1 = 95.14\n",
      "          PROD2 = 94.54\n",
      "------------------------\n",
      "Macro-F1 = 91.743\n",
      "\n",
      "\n",
      "++ predicting, batch 180/180 (100.00%).\n",
      "F1 scores\n",
      "------------------------\n",
      "            ASP = 72.42\n",
      "           NONE = 97.70\n",
      "           PRED = 86.77\n",
      "          PROD1 = 89.56\n",
      "          PROD2 = 88.96\n",
      "------------------------\n",
      "Macro-F1 = 87.083\n",
      "\n",
      "\n",
      "++ predicting, batch 109/109 (100.00%).\n",
      "F1 scores\n",
      "------------------------\n",
      "            ASP = 45.38\n",
      "           NONE = 86.75\n",
      "           PRED = 70.41\n",
      "          PROD1 = 58.71\n",
      "          PROD2 = 62.68\n",
      "------------------------\n",
      "Macro-F1 = 64.786\n",
      "\n",
      "== eval epoch 17/100 \"f1-macro\" train / dev / test | 91.74 / 87.08 / 64.79.\n",
      "## [no improvement micro-f1 on DEV during the last 1 epochs (best_f1_dev=87.22), 574 seconds].\n",
      "\n",
      "-- train epoch 18/100, batch 2399/2399 (100.00%), loss = 6.86.\n",
      "\n",
      "++ predicting, batch 2399/2399 (100.00%).\n",
      "F1 scores\n",
      "------------------------\n",
      "            ASP = 81.43\n",
      "           NONE = 98.61\n",
      "           PRED = 90.50\n",
      "          PROD1 = 95.43\n",
      "          PROD2 = 94.91\n",
      "------------------------\n",
      "Macro-F1 = 92.176\n",
      "\n",
      "\n",
      "++ predicting, batch 180/180 (100.00%).\n",
      "F1 scores\n",
      "------------------------\n",
      "            ASP = 73.52\n",
      "           NONE = 97.76\n",
      "           PRED = 87.11\n",
      "          PROD1 = 90.03\n",
      "          PROD2 = 88.95\n",
      "------------------------\n",
      "Macro-F1 = 87.472\n",
      "\n",
      "\n",
      "++ predicting, batch 109/109 (100.00%).\n",
      "F1 scores\n",
      "------------------------\n",
      "            ASP = 42.82\n",
      "           NONE = 86.65\n",
      "           PRED = 71.72\n",
      "          PROD1 = 55.16\n",
      "          PROD2 = 57.86\n",
      "------------------------\n",
      "Macro-F1 = 62.842\n",
      "\n",
      "== eval epoch 18/100 \"f1-macro\" train / dev / test | 92.18 / 87.47 / 62.84.\n",
      "## [BEST epoch], 565 seconds.\n",
      "\n",
      "-- train epoch 19/100, batch 2399/2399 (100.00%), loss = 6.62.\n",
      "\n",
      "++ predicting, batch 2399/2399 (100.00%).\n",
      "F1 scores\n",
      "------------------------\n",
      "            ASP = 82.08\n",
      "           NONE = 98.64\n",
      "           PRED = 90.43\n",
      "          PROD1 = 95.63\n",
      "          PROD2 = 95.37\n",
      "------------------------\n",
      "Macro-F1 = 92.429\n",
      "\n",
      "\n",
      "++ predicting, batch 180/180 (100.00%).\n",
      "F1 scores\n",
      "------------------------\n",
      "            ASP = 73.67\n",
      "           NONE = 97.71\n",
      "           PRED = 87.75\n",
      "          PROD1 = 89.39\n",
      "          PROD2 = 89.11\n",
      "------------------------\n",
      "Macro-F1 = 87.525\n",
      "\n",
      "\n",
      "++ predicting, batch 109/109 (100.00%).\n",
      "F1 scores\n",
      "------------------------\n",
      "            ASP = 38.51\n",
      "           NONE = 86.74\n",
      "           PRED = 71.01\n",
      "          PROD1 = 55.51\n",
      "          PROD2 = 60.68\n",
      "------------------------\n",
      "Macro-F1 = 62.491\n",
      "\n",
      "== eval epoch 19/100 \"f1-macro\" train / dev / test | 92.43 / 87.53 / 62.49.\n",
      "## [BEST epoch], 567 seconds.\n",
      "\n",
      "-- train epoch 20/100, batch 2399/2399 (100.00%), loss = 6.30.\n",
      "\n",
      "++ predicting, batch 2399/2399 (100.00%).\n",
      "F1 scores\n",
      "------------------------\n",
      "            ASP = 83.04\n",
      "           NONE = 98.64\n",
      "           PRED = 90.26\n",
      "          PROD1 = 95.87\n",
      "          PROD2 = 95.67\n",
      "------------------------\n",
      "Macro-F1 = 92.698\n",
      "\n",
      "\n",
      "++ predicting, batch 180/180 (100.00%).\n",
      "F1 scores\n",
      "------------------------\n",
      "            ASP = 73.56\n",
      "           NONE = 97.69\n",
      "           PRED = 86.66\n",
      "          PROD1 = 90.09\n",
      "          PROD2 = 89.35\n",
      "------------------------\n",
      "Macro-F1 = 87.470\n",
      "\n",
      "\n",
      "++ predicting, batch 109/109 (100.00%).\n",
      "F1 scores\n",
      "------------------------\n",
      "            ASP = 46.17\n",
      "           NONE = 86.59\n",
      "           PRED = 72.16\n",
      "          PROD1 = 57.14\n",
      "          PROD2 = 61.14\n",
      "------------------------\n",
      "Macro-F1 = 64.640\n",
      "\n",
      "== eval epoch 20/100 \"f1-macro\" train / dev / test | 92.70 / 87.47 / 64.64.\n",
      "## [no improvement micro-f1 on DEV during the last 1 epochs (best_f1_dev=87.53), 586 seconds].\n",
      "\n",
      "-- train epoch 21/100, batch 2399/2399 (100.00%), loss = 6.11.\n",
      "\n",
      "++ predicting, batch 2399/2399 (100.00%).\n",
      "F1 scores\n",
      "------------------------\n",
      "            ASP = 83.50\n",
      "           NONE = 98.68\n",
      "           PRED = 90.61\n",
      "          PROD1 = 95.74\n",
      "          PROD2 = 95.64\n",
      "------------------------\n",
      "Macro-F1 = 92.836\n",
      "\n",
      "\n",
      "++ predicting, batch 180/180 (100.00%).\n",
      "F1 scores\n",
      "------------------------\n",
      "            ASP = 73.64\n",
      "           NONE = 97.70\n",
      "           PRED = 86.70\n",
      "          PROD1 = 90.12\n",
      "          PROD2 = 88.74\n",
      "------------------------\n",
      "Macro-F1 = 87.379\n",
      "\n",
      "\n",
      "++ predicting, batch 109/109 (100.00%).\n",
      "F1 scores\n",
      "------------------------\n",
      "            ASP = 44.51\n",
      "           NONE = 86.54\n",
      "           PRED = 70.12\n",
      "          PROD1 = 56.01\n",
      "          PROD2 = 62.25\n",
      "------------------------\n",
      "Macro-F1 = 63.888\n",
      "\n",
      "== eval epoch 21/100 \"f1-macro\" train / dev / test | 92.84 / 87.38 / 63.89.\n",
      "## [no improvement micro-f1 on DEV during the last 2 epochs (best_f1_dev=87.53), 596 seconds].\n",
      "\n",
      "-- train epoch 22/100, batch 2399/2399 (100.00%), loss = 6.45.\n",
      "\n",
      "++ predicting, batch 2399/2399 (100.00%).\n",
      "F1 scores\n",
      "------------------------\n",
      "            ASP = 83.66\n",
      "           NONE = 98.77\n",
      "           PRED = 91.34\n",
      "          PROD1 = 96.11\n",
      "          PROD2 = 95.97\n",
      "------------------------\n",
      "Macro-F1 = 93.168\n",
      "\n",
      "\n",
      "++ predicting, batch 180/180 (100.00%).\n",
      "F1 scores\n",
      "------------------------\n",
      "            ASP = 74.21\n",
      "           NONE = 97.70\n",
      "           PRED = 86.99\n",
      "          PROD1 = 89.41\n",
      "          PROD2 = 88.75\n",
      "------------------------\n",
      "Macro-F1 = 87.412\n",
      "\n",
      "\n",
      "++ predicting, batch 109/109 (100.00%).\n",
      "F1 scores\n",
      "------------------------\n",
      "            ASP = 43.60\n",
      "           NONE = 86.80\n",
      "           PRED = 72.56\n",
      "          PROD1 = 56.42\n",
      "          PROD2 = 57.10\n",
      "------------------------\n",
      "Macro-F1 = 63.296\n",
      "\n",
      "== eval epoch 22/100 \"f1-macro\" train / dev / test | 93.17 / 87.41 / 63.30.\n",
      "## [no improvement micro-f1 on DEV during the last 3 epochs (best_f1_dev=87.53), 599 seconds].\n",
      "\n",
      "-- train epoch 23/100, batch 2399/2399 (100.00%), loss = 6.37.\n",
      "\n",
      "++ predicting, batch 2399/2399 (100.00%).\n",
      "F1 scores\n",
      "------------------------\n",
      "            ASP = 84.38\n",
      "           NONE = 98.81\n",
      "           PRED = 91.11\n",
      "          PROD1 = 96.49\n",
      "          PROD2 = 96.15\n",
      "------------------------\n",
      "Macro-F1 = 93.391\n",
      "\n",
      "\n",
      "++ predicting, batch 180/180 (100.00%).\n",
      "F1 scores\n",
      "------------------------\n",
      "            ASP = 73.93\n",
      "           NONE = 97.65\n",
      "           PRED = 85.89\n",
      "          PROD1 = 89.93\n",
      "          PROD2 = 88.14\n",
      "------------------------\n",
      "Macro-F1 = 87.109\n",
      "\n",
      "\n",
      "++ predicting, batch 109/109 (100.00%).\n",
      "F1 scores\n",
      "------------------------\n",
      "            ASP = 47.71\n",
      "           NONE = 86.88\n",
      "           PRED = 72.89\n",
      "          PROD1 = 57.05\n",
      "          PROD2 = 58.40\n",
      "------------------------\n",
      "Macro-F1 = 64.585\n",
      "\n",
      "== eval epoch 23/100 \"f1-macro\" train / dev / test | 93.39 / 87.11 / 64.58.\n",
      "## [no improvement micro-f1 on DEV during the last 4 epochs (best_f1_dev=87.53), 583 seconds].\n",
      "\n",
      "-- train epoch 24/100, batch 2399/2399 (100.00%), loss = 6.77.\n",
      "\n",
      "++ predicting, batch 2399/2399 (100.00%).\n",
      "F1 scores\n",
      "------------------------\n",
      "            ASP = 84.49\n",
      "           NONE = 98.80\n",
      "           PRED = 91.29\n",
      "          PROD1 = 96.73\n",
      "          PROD2 = 96.59\n",
      "------------------------\n",
      "Macro-F1 = 93.581\n",
      "\n",
      "\n",
      "++ predicting, batch 180/180 (100.00%).\n",
      "F1 scores\n",
      "------------------------\n",
      "            ASP = 73.82\n",
      "           NONE = 97.61\n",
      "           PRED = 86.29\n",
      "          PROD1 = 90.04\n",
      "          PROD2 = 89.16\n",
      "------------------------\n",
      "Macro-F1 = 87.384\n",
      "\n",
      "\n",
      "++ predicting, batch 109/109 (100.00%).\n",
      "F1 scores\n",
      "------------------------\n",
      "            ASP = 45.91\n",
      "           NONE = 86.92\n",
      "           PRED = 71.44\n",
      "          PROD1 = 53.83\n",
      "          PROD2 = 61.61\n",
      "------------------------\n",
      "Macro-F1 = 63.943\n",
      "\n",
      "== eval epoch 24/100 \"f1-macro\" train / dev / test | 93.58 / 87.38 / 63.94.\n",
      "## [no improvement micro-f1 on DEV during the last 5 epochs (best_f1_dev=87.53), 572 seconds].\n",
      "\n",
      "++ predicting, batch 2399/2399 (100.00%).\n",
      "F1 scores\n",
      "------------------------\n",
      "            ASP = 84.43\n",
      "           NONE = 98.86\n",
      "           PRED = 91.84\n",
      "          PROD1 = 96.78\n",
      "          PROD2 = 96.43\n",
      "------------------------\n",
      "Macro-F1 = 93.670\n",
      "\n",
      "\n",
      "++ predicting, batch 180/180 (100.00%).\n",
      "F1 scores\n",
      "------------------------\n",
      "            ASP = 73.70\n",
      "           NONE = 97.74\n",
      "           PRED = 87.30\n",
      "          PROD1 = 90.01\n",
      "          PROD2 = 88.88\n",
      "------------------------\n",
      "Macro-F1 = 87.524\n",
      "\n",
      "\n",
      "++ predicting, batch 109/109 (100.00%).\n",
      "F1 scores\n",
      "------------------------\n",
      "            ASP = 43.17\n",
      "           NONE = 86.85\n",
      "           PRED = 71.79\n",
      "          PROD1 = 56.20\n",
      "          PROD2 = 52.99\n",
      "------------------------\n",
      "Macro-F1 = 62.199\n",
      "\n",
      "== eval epoch 25/100 \"f1-macro\" train / dev / test | 93.67 / 87.52 / 62.20.\n",
      "## [no improvement micro-f1 on DEV during the last 6 epochs (best_f1_dev=87.53), 569 seconds].\n",
      "\n",
      "-- train epoch 26/100, batch 2399/2399 (100.00%), loss = 5.30.\n",
      "\n",
      "++ predicting, batch 2399/2399 (100.00%).\n",
      "F1 scores\n",
      "------------------------\n",
      "            ASP = 85.07\n",
      "           NONE = 98.93\n",
      "           PRED = 91.83\n",
      "          PROD1 = 97.09\n",
      "          PROD2 = 96.70\n",
      "------------------------\n",
      "Macro-F1 = 93.923\n",
      "\n",
      "\n",
      "++ predicting, batch 180/180 (100.00%).\n",
      "F1 scores\n",
      "------------------------\n",
      "            ASP = 72.36\n",
      "           NONE = 97.75\n",
      "           PRED = 87.21\n",
      "          PROD1 = 90.49\n",
      "          PROD2 = 88.35\n",
      "------------------------\n",
      "Macro-F1 = 87.231\n",
      "\n",
      "\n",
      "++ predicting, batch 109/109 (100.00%).\n",
      "F1 scores\n",
      "------------------------\n",
      "            ASP = 41.26\n",
      "           NONE = 87.02\n",
      "           PRED = 72.96\n",
      "          PROD1 = 57.91\n",
      "          PROD2 = 57.53\n",
      "------------------------\n",
      "Macro-F1 = 63.336\n",
      "\n",
      "== eval epoch 26/100 \"f1-macro\" train / dev / test | 93.92 / 87.23 / 63.34.\n",
      "## [no improvement micro-f1 on DEV during the last 7 epochs (best_f1_dev=87.53), 591 seconds].\n",
      "\n",
      "-- train epoch 27/100, batch 2399/2399 (100.00%), loss = 5.69.\n",
      "\n",
      "++ predicting, batch 2399/2399 (100.00%).\n",
      "F1 scores\n",
      "------------------------\n",
      "            ASP = 85.11\n",
      "           NONE = 98.94\n",
      "           PRED = 92.28\n",
      "          PROD1 = 96.81\n",
      "          PROD2 = 96.84\n",
      "------------------------\n",
      "Macro-F1 = 93.997\n",
      "\n",
      "\n",
      "++ predicting, batch 180/180 (100.00%).\n",
      "F1 scores\n",
      "------------------------\n",
      "            ASP = 71.74\n",
      "           NONE = 97.72\n",
      "           PRED = 87.34\n",
      "          PROD1 = 89.78\n",
      "          PROD2 = 88.90\n",
      "------------------------\n",
      "Macro-F1 = 87.095\n",
      "\n",
      "\n",
      "++ predicting, batch 109/109 (100.00%).\n",
      "F1 scores\n",
      "------------------------\n",
      "            ASP = 47.10\n",
      "           NONE = 86.61\n",
      "           PRED = 74.73\n",
      "          PROD1 = 58.74\n",
      "          PROD2 = 63.33\n",
      "------------------------\n",
      "Macro-F1 = 66.101\n",
      "\n",
      "== eval epoch 27/100 \"f1-macro\" train / dev / test | 94.00 / 87.09 / 66.10.\n",
      "## [no improvement micro-f1 on DEV during the last 8 epochs (best_f1_dev=87.53), 600 seconds].\n",
      "\n",
      "-- train epoch 28/100, batch 2399/2399 (100.00%), loss = 5.18.\n",
      "\n",
      "++ predicting, batch 2399/2399 (100.00%).\n",
      "F1 scores\n",
      "------------------------\n",
      "            ASP = 85.23\n",
      "           NONE = 98.97\n",
      "           PRED = 92.16\n",
      "          PROD1 = 97.28\n",
      "          PROD2 = 96.89\n",
      "------------------------\n",
      "Macro-F1 = 94.105\n",
      "\n",
      "\n",
      "++ predicting, batch 180/180 (100.00%).\n",
      "F1 scores\n",
      "------------------------\n",
      "            ASP = 73.67\n",
      "           NONE = 97.74\n",
      "           PRED = 87.18\n",
      "          PROD1 = 89.85\n",
      "          PROD2 = 88.66\n",
      "------------------------\n",
      "Macro-F1 = 87.419\n",
      "\n",
      "\n",
      "++ predicting, batch 109/109 (100.00%).\n",
      "F1 scores\n",
      "------------------------\n",
      "            ASP = 49.70\n",
      "           NONE = 87.19\n",
      "           PRED = 74.76\n",
      "          PROD1 = 55.16\n",
      "          PROD2 = 56.53\n",
      "------------------------\n",
      "Macro-F1 = 64.668\n",
      "\n",
      "== eval epoch 28/100 \"f1-macro\" train / dev / test | 94.10 / 87.42 / 64.67.\n",
      "## [no improvement micro-f1 on DEV during the last 9 epochs (best_f1_dev=87.53), 591 seconds].\n",
      "\n",
      "-- train epoch 29/100, batch 2399/2399 (100.00%), loss = 4.85.\n",
      "\n",
      "++ predicting, batch 2399/2399 (100.00%).\n",
      "F1 scores\n",
      "------------------------\n",
      "            ASP = 85.73\n",
      "           NONE = 99.00\n",
      "           PRED = 92.15\n",
      "          PROD1 = 97.28\n",
      "          PROD2 = 97.16\n",
      "------------------------\n",
      "Macro-F1 = 94.267\n",
      "\n",
      "\n",
      "++ predicting, batch 180/180 (100.00%).\n",
      "F1 scores\n",
      "------------------------\n",
      "            ASP = 72.90\n",
      "           NONE = 97.74\n",
      "           PRED = 86.40\n",
      "          PROD1 = 90.56\n",
      "          PROD2 = 88.52\n",
      "------------------------\n",
      "Macro-F1 = 87.224\n",
      "\n",
      "\n",
      "++ predicting, batch 109/109 (100.00%).\n",
      "F1 scores\n",
      "------------------------\n",
      "            ASP = 45.78\n",
      "           NONE = 87.04\n",
      "           PRED = 72.64\n",
      "          PROD1 = 55.32\n",
      "          PROD2 = 59.58\n",
      "------------------------\n",
      "Macro-F1 = 64.073\n",
      "\n",
      "== eval epoch 29/100 \"f1-macro\" train / dev / test | 94.27 / 87.22 / 64.07.\n",
      "## [no improvement micro-f1 on DEV during the last 10 epochs (best_f1_dev=87.53), 585 seconds].\n",
      "\n",
      "-- train epoch 30/100, batch 2399/2399 (100.00%), loss = 4.93.\n",
      "\n",
      "++ predicting, batch 2399/2399 (100.00%).\n",
      "F1 scores\n",
      "------------------------\n",
      "            ASP = 85.64\n",
      "           NONE = 99.03\n",
      "           PRED = 92.38\n",
      "          PROD1 = 97.59\n",
      "          PROD2 = 97.55\n",
      "------------------------\n",
      "Macro-F1 = 94.436\n",
      "\n",
      "\n",
      "++ predicting, batch 180/180 (100.00%).\n",
      "F1 scores\n",
      "------------------------\n",
      "            ASP = 72.58\n",
      "           NONE = 97.80\n",
      "           PRED = 87.47\n",
      "          PROD1 = 90.27\n",
      "          PROD2 = 88.61\n",
      "------------------------\n",
      "Macro-F1 = 87.346\n",
      "\n",
      "\n",
      "++ predicting, batch 109/109 (100.00%).\n",
      "F1 scores\n",
      "------------------------\n",
      "            ASP = 43.96\n",
      "           NONE = 87.29\n",
      "           PRED = 73.46\n",
      "          PROD1 = 56.84\n",
      "          PROD2 = 62.19\n",
      "------------------------\n",
      "Macro-F1 = 64.748\n",
      "\n",
      "== eval epoch 30/100 \"f1-macro\" train / dev / test | 94.44 / 87.35 / 64.75.\n",
      "## [no improvement micro-f1 on DEV during the last 11 epochs (best_f1_dev=87.53), 583 seconds].\n",
      "\n",
      "-- train epoch 31/100, batch 2399/2399 (100.00%), loss = 4.82.\n",
      "\n",
      "++ predicting, batch 1363/2399 (57.00%)."
     ]
    }
   ],
   "source": [
    "! python3 main.py --train \"/home/vika/targer/data/NER/Indian_dataset/train.csv\" --dev \"/home/vika/targer/data/NER/Indian_dataset/dev.csv\" --data-io connl-ner-2003 --evaluator f1-alpha-match-10 --model 'BiRNN' --opt adam --lr 0.001 --save-best yes --patience 20 --rnn-hidden-dim 200 --gpu 3 --test \"/home/vika/targer/data/NER/Indian_dataset/test.csv\" --elmo True --evaluator f1-macro "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.modules.elmo import Elmo, batch_to_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [['1tb', 'of', 'mechanical', 'storage', 'is', \"n't\", 'bad', '\"', 'but', 'toshiba', 'hard', 'drives', 'really', 'are', \"n't\", 'what', 'we', 'want', 'to', 'be', 'seeing', 'as', 'they', 'tend', 'to', 'be', 'a', 'bit', 'slower', 'than', 'competing', 'drives', 'from', 'hgst', '\"', 'western', 'digital', '\"', 'samsung', '\"', 'and', 'seagate', '.'],  ['we', 'have', 'found', 'that', 'blood', 'carbon', 'and', 'nitrogen', 'turnover', 'was', 'roughly', 'equivalent', 'in', 'mice', 'and', 'in', 'rats', '\"', 'and', 'that', 'mouse', 'blood', 'turned', 'over', 'slightly', 'faster', 'than', 'rat', 'blood', '.']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('1tb', 'O')\n",
      "('of', 'O')\n",
      "('mechanical', 'O')\n",
      "('storage', 'O')\n",
      "('is', 'O')\n",
      "(\"n't\", 'O')\n",
      "('bad', 'O')\n",
      "('\"', 'O')\n",
      "('but', 'O')\n",
      "('toshiba', 'B-OTHOBJ')\n",
      "('hard', 'O')\n",
      "('drives', 'O')\n",
      "('really', 'O')\n",
      "('are', 'O')\n",
      "(\"n't\", 'O')\n",
      "('what', 'O')\n",
      "('we', 'O')\n",
      "('want', 'O')\n",
      "('to', 'O')\n",
      "('be', 'O')\n",
      "('seeing', 'O')\n",
      "('as', 'O')\n",
      "('they', 'O')\n",
      "('tend', 'O')\n",
      "('to', 'O')\n",
      "('be', 'O')\n",
      "('a', 'O')\n",
      "('bit', 'O')\n",
      "('slower', 'O')\n",
      "('than', 'O')\n",
      "('competing', 'O')\n",
      "('drives', 'O')\n",
      "('from', 'O')\n",
      "('hgst', 'O')\n",
      "('\"', 'O')\n",
      "('western', 'O')\n",
      "('digital', 'O')\n",
      "('\"', 'O')\n",
      "('samsung', 'B-ASPOBJ')\n",
      "('\"', 'O')\n",
      "('and', 'O')\n",
      "('seagate', 'O')\n",
      "('.', 'O')\n"
     ]
    }
   ],
   "source": [
    "for elem in zip(sentences[0], tags[0]):\n",
    "    print (elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "++ predicting, batch 1/1 (0.00%)."
     ]
    }
   ],
   "source": [
    "from src.factories.factory_tagger import TaggerFactory\n",
    "import torch\n",
    "\n",
    "model = TaggerFactory.load(\"2019_09_24_10-48_48_tagger.hdf5\")\n",
    "model.gpu = 1\n",
    "model.cuda(device = 1)\n",
    "\n",
    "\n",
    "tags = model.predict_tags_from_words(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /opt/anaconda/lib/python3.6/site-packages (18.1)\n",
      "Collecting install\n",
      "\u001b[33m  Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ConnectTimeoutError(<pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x7f23f7295240>, 'Connection to pypi.org timed out. (connect timeout=15)')': /simple/install/\u001b[0m\n",
      "\u001b[33m  Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ConnectTimeoutError(<pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x7f23f72951d0>, 'Connection to pypi.org timed out. (connect timeout=15)')': /simple/install/\u001b[0m\n",
      "\u001b[33m  Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ConnectTimeoutError(<pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x7f23f72950f0>, 'Connection to pypi.org timed out. (connect timeout=15)')': /simple/install/\u001b[0m\n",
      "\u001b[33m  Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ConnectTimeoutError(<pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x7f23f7295eb8>, 'Connection to pypi.org timed out. (connect timeout=15)')': /simple/install/\u001b[0m\n",
      "^C\n",
      "\u001b[31mOperation cancelled by user\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip3 install pip install --trusted-host=pypi.python.org --trusted-host=pypi.org --trusted-host=files.pythonhosted.org pytorch-pretrained-bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-51f29d9cf921>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Let's see all hidden-states and attentions on this text\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "input_ids = torch.tensor([tokenizer.encode(\"Let's see all hidden-states and attentions on this text\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'string' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-7d756f7a8a8a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bert-base-uncased'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'string' is not defined"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "words = string.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = ['we have found that blood carbon and nitrogen turnover was roughly equivalent', 'in mice and in rats \"and that mouse blood turned over .']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\"Here\", \"is\", \"the\", \"sentence\", \"I\", \"want\", \"embeddings\", \"for\", \".\"]\n",
    "#text = \"After stealing money from the bank vault, the bank robber was seen fishing on the Mississippi river bank.\"\n",
    "marked_text = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['here', 'is', 'the', 'sentence', 'i', 'want', 'em', '##bed', '##ding', '##s', 'for', '.']\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 7, 7, 7, 8]\n"
     ]
    }
   ],
   "source": [
    "tokenized_texts = tokenizer.tokenize(\"\".join(text))\n",
    "print(tokenized_texts)\n",
    "i = 0\n",
    "word_index = []\n",
    "next_word = False\n",
    "for token in tokenized_texts:\n",
    "    if (next_word):\n",
    "        i += 1\n",
    "    word_index.append(i)\n",
    "    if ('##' in token):\n",
    "        next_word = False\n",
    "    else:\n",
    "        next_word = True\n",
    "print (word_index)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "MAX_LEN = np.max(np.array([len(seq) for seq in tokenized_texts]))\n",
    "input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts], maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_LEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sequence item 0: expected str instance, list found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-d45856db5fe6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: sequence item 0: expected str instance, list found"
     ]
    }
   ],
   "source": [
    "words = ''.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(indexed_tokens2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_tensor = torch.tensor(input_ids)\n",
    "segments_tensors = torch.tensor(np.ones(input_ids.shape)).to(torch.int64)\n",
    "\n",
    "# Load pre-trained model (weights)\n",
    "model = BertModel.from_pretrained(\"pretrained\")#BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
    "#model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tokens_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(segments_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    encoded_layers, _ = model(tokens_tensor, segments_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoded_layers[2][1][21])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30522"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.vocab.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_embeddings = [] \n",
    "\n",
    "# For each token in the sentence...\n",
    "for token_i in range(len(tokenized_text)):  \n",
    "    # Holds 12 layers of hidden states for each token \n",
    "    hidden_layers = [] \n",
    "    # For each of the 12 layers...\n",
    "    for layer_i in range(len(encoded_layers)):\n",
    "    # Lookup the vector for `token_i` in `layer_i`\n",
    "        vec = encoded_layers[layer_i][0][token_i]\n",
    "        hidden_layers.append(vec)\n",
    "        token_embeddings.append(hidden_layers)\n",
    "\n",
    "summed_last_4_layers = [torch.sum(torch.stack(layer)[-4:], 0) for layer in token_embeddings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "540"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(summed_last_4_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Here is the sentence I want embeddings for.\"\n",
    "text = \"After stealing money from the bank vault, the bank robber was seen fishing on the Mississippi river bank.\"\n",
    "marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "\n",
    "print (marked_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.modules.elmo import Elmo, batch_to_ids\n",
    "\n",
    "options_file = \"embeddings/elmo_2x4096_512_2048cnn_2xhighway_5.5B_options.json\"\n",
    "weight_file = \"/home/vika/targer/embeddings/elmo_2x4096_512_2048cnn_2xhighway_5.5B_weights.hdf5\"\n",
    "\n",
    "# Compute two different representation for each token.\n",
    "# Each representation is a linear weighted combination for the\n",
    "# 3 layers in ELMo (i.e., charcnn, the outputs of the two BiLSTM))\n",
    "elmo = Elmo(options_file, weight_file, 2, dropout=0)\n",
    "\n",
    "# use batch_to_ids to convert sentences to character ids\n",
    "sentences = [['1tb', 'of', 'mechanical', 'storage', 'is', \"n't\", 'bad', '\"', 'but', 'toshiba', 'hard', 'drives', 'really', 'are', \"n't\", 'what', 'we', 'want', 'to', 'be', 'seeing', 'as', 'they', 'tend', 'to', 'be', 'a', 'bit', 'slower', 'than', 'competing', 'drives', 'from', 'hgst', '\"', 'western', 'digital', '\"', 'samsung', '\"', 'and', 'seagate', '.'], ['amazon', 'is', 'another', 'good', 'choice', '\"', 'and', 'i', 'actually', 'prefer', 'their', 'interface', 'better', 'than', 'itunes', '', '(', 'it', 'loads', 'faster', 'for', 'me', ')', '.'], ['well-designed', '\"', 'properly', 'constructed', 'timber', 'buildings', 'can', 'be', 'even', 'safer', 'in', 'earthquakes', 'than', 'ferro-concrete', 'ones', '.'], ['maybe', 'it', 's', 'because', 'i', 'm', 'a', 'ku', 'fan', '\"', 'but', 'i', 'honestly', 'like', 'the', 'looks', 'of', 'adidas', 'uniforms', 'better', 'than', 'nike', '.'], ['soft', 'drinks', 'do', 'not', 'include', 'beverages', 'that', 'contain', 'milk', 'or', 'milk', 'products', '\"', 'soy', '\"', 'rice', 'or', 'similar', 'milk', 'substitutes', '\"', 'or', 'greater', 'than', '50', '%', 'of', 'vegetable', 'or', 'fruit', 'juice', 'by', 'volume', '.'], ['partly', 'because', 'i', 'like', 'apple', 's', 'choice', 'of', 'file', 'format', '-', 'aac', 'is', 'much', 'more', 'modern', 'than', 'mp3', '\"', 'and', 'theoretically', 'an', 'aac', 'file', 'at', '256kbps', '(', 'from', 'itunes', 'plus', ')', 'sounds', 'better', 'than', 'an', 'mp3', 'file', 'at', '256kbps', '(', 'from', 'amazon', ')', '.'], ['and', 'nokia', 'is', 'easier', 'to', 'use', '...', '...', 'it', 'easily', 'gets', 'infected', 'with', 'virus', 'and', 'the', 'net', 'speed', 'is', 'not', 'fast', '...', '..and', 'ofcourse', 'nokia', 'is', 'way', 'more', 'reliable', 'than', 'samsung', '...', '..go', 'for', 'nokia', '...', '..this', 'phone', 'suxxxxxxxxxxxxxxxxxxxxxx', '!'], ['java', 'is', 'a', 'static', 'type', 'language', 'and', 'it', 'is', 'safer', 'than', 'dynamic', 'type', 'languages', 'like', 'ruby', '.'], ['amazon', 'music', 'will', 'be', 'cheaper', '\"', 'faster', 'and', 'perceived', 'to', 'be', 'more', 'honest', 'than', 'itunes', 'both', 'by', 'customers', 'and', 'musicians', '.'], \n",
    "             ['we', 'have', 'found', 'that', 'blood', 'carbon', 'and', 'nitrogen', 'turnover', 'was', 'roughly', 'equivalent', 'in', 'mice', 'and', 'in', 'rats', '\"', 'and', 'that', 'mouse', 'blood', 'turned', 'over', 'slightly', 'faster', 'than', 'rat', 'blood', '.']]\n",
    "\n",
    "character_ids = batch_to_ids(sentences)\n",
    "\n",
    "embeddings = elmo(character_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elmo.get_output_dim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 44, 50])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "character_ids.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([259, 113, 115, 112, 113, 102, 115, 109, 122, 260, 261, 261, 261, 261,\n",
       "        261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
       "        261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
       "        261, 261, 261, 261, 261, 261, 261, 261])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "character_ids[2][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.4785, -0.4598, -0.0396,  ..., -0.1577, -0.3011, -0.1590],\n",
      "       grad_fn=<SelectBackward>)\n"
     ]
    }
   ],
   "source": [
    "print (embeddings['elmo_representations'][1][5][5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.4785, -0.4598, -0.0396,  ..., -0.1577, -0.3011, -0.1590],\n",
      "       grad_fn=<SelectBackward>)\n"
     ]
    }
   ],
   "source": [
    "print (embeddings['elmo_representations'][0][5][5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## birnn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BiRNNCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "args train = /home/vika/NER_RNN/targer/data/NER/Asqua_CAM_aspects/CAM_train.csv\n",
      "Loading from /home/vika/NER_RNN/targer/data/NER/Asqua_CAM_aspects/CAM_train.csv: 2619 samples, 74616 words.\n",
      "Loading from /home/vika/NER_RNN/targer/data/NER/Asqua_CAM_aspects/CAM_test.csv: 523 samples, 14728 words.\n",
      "Loading from /home/vika/NER_RNN/targer/data/NER/Asqua_CAM_aspects/CAM_test.csv: 523 samples, 14728 words.\n",
      "word_sequences_dev 523 [['and', 'can', 'i', 'just', 'say', 'that', 'doing', 'the', 'inferno', 'move', 'on', 'the', 'ds', 'is', '50', 'times', 'easier', 'than', 'trying', 'to', 'make', 'an', 'infinity', 'symbol', 'using', 'the', 'wii', 'remote', '.'], ['i', 'go', 'to', 'the', 'amazon', 'mp3', 'store', '(', 'better', 'than', 'itunes', '.'], ['tea', 'dinner', '(', 'sorry', '\"', 'my', 'wife', 'is', 'trying', 'to', 'make', 'me', 'posherer', ')', 'and', 'picking', 'up', 'a', 'phone', 'is', 'easier', '-', 'then', 'i', 'can', 'swig', 'my', 'beer', 'and', 'relaaaaxxxx', '...', '.', '.'], ['actually', 'for', 'many', 'windows', 'xp', 'users', 'it', 'is', 'easier', 'to', 'migrate', 'to', 'linux', 'mint', 'than', 'to', 'windows', '8', '.'], ['this', 'is', 'why', 'the', 'better', 'team', 'wins', 'any', 'given', 'basketball', 'game', 'with', 'far', 'greater', 'frequency', 'than', 'it', 'does', 'in', 'baseball', '\"', 'football', 'or', 'hockey', '.']]\n",
      "tag_sequences_train 523 [['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ASPOBJ', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-OTHOBJ', 'O', 'O'], ['O', 'O', 'O', 'O', 'B-ASPOBJ', 'O', 'O', 'O', 'O', 'O', 'B-OTHOBJ', 'O'], ['B-ASPOBJ', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-OTHOBJ', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'B-ASPOBJ', 'I-ASPOBJ', 'O', 'O', 'O', 'B-ASP', 'I-ASP', 'I-ASP', 'I-ASP', 'I-ASP', 'I-ASP', 'O', 'O', 'B-OTHOBJ', 'I-OTHOBJ', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ASPOBJ', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-OTHOBJ', 'O', 'O', 'O', 'O', 'O']]\n",
      "DatasetsBank: len(unique_words_list) = 7143 unique words.\n",
      "DatasetsBank: len(unique_words_list) = 7753 unique words.\n",
      "DatasetsBank: len(unique_words_list) = 7753 unique words.\n",
      "Reading embeddings file embeddings/glove.6B.100d.txt, line = 0\n",
      "Reading embeddings file embeddings/glove.6B.100d.txt, line = 25000\n",
      "Reading embeddings file embeddings/glove.6B.100d.txt, line = 50000\n",
      "Reading embeddings file embeddings/glove.6B.100d.txt, line = 75000\n",
      "Reading embeddings file embeddings/glove.6B.100d.txt, line = 100000\n",
      "Reading embeddings file embeddings/glove.6B.100d.txt, line = 125000\n",
      "Reading embeddings file embeddings/glove.6B.100d.txt, line = 150000\n",
      "Reading embeddings file embeddings/glove.6B.100d.txt, line = 175000\n",
      "Reading embeddings file embeddings/glove.6B.100d.txt, line = 200000\n",
      "Reading embeddings file embeddings/glove.6B.100d.txt, line = 225000\n",
      "Reading embeddings file embeddings/glove.6B.100d.txt, line = 250000\n",
      "Reading embeddings file embeddings/glove.6B.100d.txt, line = 275000\n",
      "Reading embeddings file embeddings/glove.6B.100d.txt, line = 300000\n",
      "Reading embeddings file embeddings/glove.6B.100d.txt, line = 325000\n",
      "Reading embeddings file embeddings/glove.6B.100d.txt, line = 350000\n",
      "Reading embeddings file embeddings/glove.6B.100d.txt, line = 375000\n",
      "Reading embeddings file embeddings/glove.6B.100d.txt, line = 0\n",
      "Reading embeddings file embeddings/glove.6B.100d.txt, line = 25000\n",
      "Reading embeddings file embeddings/glove.6B.100d.txt, line = 50000\n",
      "Reading embeddings file embeddings/glove.6B.100d.txt, line = 75000\n",
      "Reading embeddings file embeddings/glove.6B.100d.txt, line = 100000\n",
      "Reading embeddings file embeddings/glove.6B.100d.txt, line = 125000\n",
      "Reading embeddings file embeddings/glove.6B.100d.txt, line = 150000\n",
      "Reading embeddings file embeddings/glove.6B.100d.txt, line = 175000\n",
      "Reading embeddings file embeddings/glove.6B.100d.txt, line = 200000\n",
      "Reading embeddings file embeddings/glove.6B.100d.txt, line = 225000\n",
      "Reading embeddings file embeddings/glove.6B.100d.txt, line = 250000\n",
      "Reading embeddings file embeddings/glove.6B.100d.txt, line = 275000\n",
      "Reading embeddings file embeddings/glove.6B.100d.txt, line = 300000\n",
      "Reading embeddings file embeddings/glove.6B.100d.txt, line = 325000\n",
      "Reading embeddings file embeddings/glove.6B.100d.txt, line = 350000\n",
      "Reading embeddings file embeddings/glove.6B.100d.txt, line = 375000\n",
      "\n",
      "load_vocabulary_from_embeddings_file_and_unique_words_list:\n",
      "    First 50 OOV words:\n",
      "        out_of_vocabulary_words_list[0] = b'1tb'\n",
      "        out_of_vocabulary_words_list[1] = b'hgst'\n",
      "        out_of_vocabulary_words_list[2] = b''\n",
      "        out_of_vocabulary_words_list[3] = b'256kbps'\n",
      "        out_of_vocabulary_words_list[4] = b'..and'\n",
      "        out_of_vocabulary_words_list[5] = b'ofcourse'\n",
      "        out_of_vocabulary_words_list[6] = b'..go'\n",
      "        out_of_vocabulary_words_list[7] = b'..this'\n",
      "        out_of_vocabulary_words_list[8] = b'suxxxxxxxxxxxxxxxxxxxxxx'\n",
      "        out_of_vocabulary_words_list[9] = b'e2252'\n",
      "        out_of_vocabulary_words_list[10] = b'monkeypatch'\n",
      "        out_of_vocabulary_words_list[11] = b'mssql'\n",
      "        out_of_vocabulary_words_list[12] = b'eccoboard\\xe2\\x84\\xa2'\n",
      "        out_of_vocabulary_words_list[13] = b'bmw-mercedes'\n",
      "        out_of_vocabulary_words_list[14] = b'goung'\n",
      "        out_of_vocabulary_words_list[15] = b'head-'\n",
      "        out_of_vocabulary_words_list[16] = b'jumbo-shrimp'\n",
      "        out_of_vocabulary_words_list[17] = b'discount-lexus'\n",
      "        out_of_vocabulary_words_list[18] = b'quicker-than-a-cayman'\n",
      "        out_of_vocabulary_words_list[19] = b\"cat's-meow\"\n",
      "        out_of_vocabulary_words_list[20] = b'current-season'\n",
      "        out_of_vocabulary_words_list[21] = b'all-plastic'\n",
      "        out_of_vocabulary_words_list[22] = b'fan-friendly'\n",
      "        out_of_vocabulary_words_list[23] = b'siginificantly'\n",
      "        out_of_vocabulary_words_list[24] = b'games.as'\n",
      "        out_of_vocabulary_words_list[25] = b'for.so'\n",
      "        out_of_vocabulary_words_list[26] = b'power/graphics'\n",
      "        out_of_vocabulary_words_list[27] = b'nv3500'\n",
      "        out_of_vocabulary_words_list[28] = b'350c'\n",
      "        out_of_vocabulary_words_list[29] = b'gretest'\n",
      "        out_of_vocabulary_words_list[30] = b'intoduced'\n",
      "        out_of_vocabulary_words_list[31] = b'mw2'\n",
      "        out_of_vocabulary_words_list[32] = b'yet.it'\n",
      "        out_of_vocabulary_words_list[33] = b'incredebly'\n",
      "        out_of_vocabulary_words_list[34] = b'5000000000000'\n",
      "        out_of_vocabulary_words_list[35] = b'fire-power'\n",
      "        out_of_vocabulary_words_list[36] = b'o3d'\n",
      "        out_of_vocabulary_words_list[37] = b'300x'\n",
      "        out_of_vocabulary_words_list[38] = b'frats'\n",
      "        out_of_vocabulary_words_list[39] = b'jaronczyk'\n",
      "        out_of_vocabulary_words_list[40] = b'import-intenders'\n",
      "        out_of_vocabulary_words_list[41] = b'stodgy-looking'\n",
      "        out_of_vocabulary_words_list[42] = b'sloppy-handling'\n",
      "        out_of_vocabulary_words_list[43] = b'platform-sharing'\n",
      "        out_of_vocabulary_words_list[44] = b'all-weather-wood'\n",
      "        out_of_vocabulary_words_list[45] = b'awwf'\n",
      "        out_of_vocabulary_words_list[46] = b'maniculatus'\n",
      "        out_of_vocabulary_words_list[47] = b'rdbsm'\n",
      "        out_of_vocabulary_words_list[48] = b'hardibacker'\n",
      "        out_of_vocabulary_words_list[49] = b'cement/gypsum'\n",
      "        out_of_vocabulary_words_list[50] = b'atii'\n",
      " -- len(out_of_vocabulary_words_list) = 735\n",
      " -- original_words_num = 7013\n",
      " -- lowercase_words_num = 1\n",
      " -- zero_digits_replaced_num = 4\n",
      " -- zero_digits_replaced_lowercase_num = 0\n",
      "\n",
      "load_vocabulary_from_tag_sequences:\n",
      " -- class_num = 7\n",
      " -- {'<pad>': 0, 'O': 1, 'B-OTHOBJ': 2, 'B-ASPOBJ': 3, 'B-ASP': 4, 'I-ASP': 5, 'I-ASPOBJ': 6, 'I-OTHOBJ': 7}\n",
      "Empirical transition matrix from the train dataset:\n",
      "               <pad>         O  B-OTHOBJ  B-ASPOBJ     B-ASP     I-ASP  I-ASPOBJ  I-OTHOBJ     <sos>\n",
      "\n",
      "     <pad>       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0\n",
      "         O       0.0   58639.0    2722.0    2863.0     792.0     165.0      22.0      21.0    2226.0\n",
      "  B-OTHOBJ       0.0    2713.0       0.0       1.0       3.0       1.0       0.0       0.0      47.0\n",
      "  B-ASPOBJ       0.0    2560.0       8.0       0.0       4.0       3.0       0.0       0.0     341.0\n",
      "     B-ASP       0.0     919.0      14.0      28.0       0.0       0.0       0.0       0.0       5.0\n",
      "     I-ASP       0.0       0.0       0.0       2.0     167.0     307.0       0.0       0.0       0.0\n",
      "  I-ASPOBJ       0.0       0.0       0.0      22.0       0.0       0.0       0.0       0.0       0.0\n",
      "  I-OTHOBJ       0.0       0.0      21.0       0.0       0.0       0.0       0.0       0.0       0.0\n",
      "     <sos>       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0\n",
      "\n",
      "Initialized transition matrix:\n",
      "               <pad>         O  B-OTHOBJ  B-ASPOBJ     B-ASP     I-ASP  I-ASPOBJ  I-OTHOBJ     <sos>\n",
      "\n",
      "     <pad>   -9999.0   -9999.0   -9999.0   -9999.0   -9999.0   -9999.0   -9999.0   -9999.0   -9999.0\n",
      "         O   -9999.0      -1.1      -1.1      -0.9      -1.0      -1.0      -1.1      -1.0      -1.0\n",
      "  B-OTHOBJ   -9999.0      -1.0   -9999.0      -1.0      -1.2      -0.9   -9999.0   -9999.0      -0.9\n",
      "  B-ASPOBJ   -9999.0      -0.9      -1.0   -9999.0      -1.0      -0.8   -9999.0   -9999.0      -1.0\n",
      "     B-ASP   -9999.0      -1.1      -1.0      -1.0   -9999.0   -9999.0   -9999.0   -9999.0      -1.1\n",
      "     I-ASP   -9999.0   -9999.0   -9999.0      -1.1      -0.9      -1.0   -9999.0   -9999.0   -9999.0\n",
      "  I-ASPOBJ   -9999.0   -9999.0   -9999.0      -1.0   -9999.0   -9999.0   -9999.0   -9999.0   -9999.0\n",
      "  I-OTHOBJ   -9999.0   -9999.0      -1.0   -9999.0   -9999.0   -9999.0   -9999.0   -9999.0   -9999.0\n",
      "     <sos>   -9999.0   -9999.0   -9999.0   -9999.0   -9999.0   -9999.0   -9999.0   -9999.0   -9999.0\n",
      "\n",
      "Start training...\n",
      "\n",
      "\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 0/100 \"f1-alpha-match-10\" train / dev / test | 3.48 / 3.55 / 3.55.\n",
      "## [BEST epoch], 11 seconds.\n",
      "\n",
      "-- train epoch 1/100, batch 261/261 (100.00%), loss = 709.25.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 1/100 \"f1-alpha-match-10\" train / dev / test | 60.42 / 59.59 / 59.59.\n",
      "## [BEST epoch], 67 seconds.\n",
      "\n",
      "-- train epoch 2/100, batch 261/261 (100.00%), loss = 394.92.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 2/100 \"f1-alpha-match-10\" train / dev / test | 69.55 / 66.87 / 66.87.\n",
      "## [BEST epoch], 69 seconds.\n",
      "\n",
      "-- train epoch 3/100, batch 261/261 (100.00%), loss = 341.32.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 3/100 \"f1-alpha-match-10\" train / dev / test | 72.22 / 67.51 / 67.51.\n",
      "## [BEST epoch], 69 seconds.\n",
      "\n",
      "-- train epoch 4/100, batch 261/261 (100.00%), loss = 328.28.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 4/100 \"f1-alpha-match-10\" train / dev / test | 74.17 / 71.47 / 71.47.\n",
      "## [BEST epoch], 69 seconds.\n",
      "\n",
      "-- train epoch 5/100, batch 261/261 (100.00%), loss = 305.78.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 5/100 \"f1-alpha-match-10\" train / dev / test | 75.36 / 72.44 / 72.44.\n",
      "## [BEST epoch], 74 seconds.\n",
      "\n",
      "-- train epoch 6/100, batch 261/261 (100.00%), loss = 301.25.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 6/100 \"f1-alpha-match-10\" train / dev / test | 77.64 / 76.34 / 76.34.\n",
      "## [BEST epoch], 73 seconds.\n",
      "\n",
      "-- train epoch 7/100, batch 261/261 (100.00%), loss = 250.16.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 7/100 \"f1-alpha-match-10\" train / dev / test | 76.98 / 75.18 / 75.18.\n",
      "## [no improvement micro-f1 on DEV during the last 1 epochs (best_f1_dev=76.34), 71 seconds].\n",
      "\n",
      "-- train epoch 8/100, batch 261/261 (100.00%), loss = 220.17.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 8/100 \"f1-alpha-match-10\" train / dev / test | 79.53 / 77.39 / 77.39.\n",
      "## [BEST epoch], 73 seconds.\n",
      "\n",
      "-- train epoch 9/100, batch 261/261 (100.00%), loss = 221.51.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 9/100 \"f1-alpha-match-10\" train / dev / test | 80.71 / 78.51 / 78.51.\n",
      "## [BEST epoch], 74 seconds.\n",
      "\n",
      "-- train epoch 10/100, batch 261/261 (100.00%), loss = 228.58.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 10/100 \"f1-alpha-match-10\" train / dev / test | 80.51 / 78.53 / 78.53.\n",
      "## [BEST epoch], 67 seconds.\n",
      "\n",
      "-- train epoch 11/100, batch 261/261 (100.00%), loss = 198.93.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 11/100 \"f1-alpha-match-10\" train / dev / test | 81.92 / 78.74 / 78.74.\n",
      "## [BEST epoch], 71 seconds.\n",
      "\n",
      "-- train epoch 12/100, batch 261/261 (100.00%), loss = 221.02.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 12/100 \"f1-alpha-match-10\" train / dev / test | 84.32 / 80.58 / 80.58.\n",
      "## [BEST epoch], 72 seconds.\n",
      "\n",
      "-- train epoch 13/100, batch 261/261 (100.00%), loss = 199.78.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 13/100 \"f1-alpha-match-10\" train / dev / test | 84.21 / 80.75 / 80.75.\n",
      "## [BEST epoch], 72 seconds.\n",
      "\n",
      "-- train epoch 14/100, batch 261/261 (100.00%), loss = 181.75.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 14/100 \"f1-alpha-match-10\" train / dev / test | 83.41 / 80.27 / 80.27.\n",
      "## [no improvement micro-f1 on DEV during the last 1 epochs (best_f1_dev=80.75), 63 seconds].\n",
      "\n",
      "-- train epoch 15/100, batch 261/261 (100.00%), loss = 153.56.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 15/100 \"f1-alpha-match-10\" train / dev / test | 84.70 / 81.14 / 81.14.\n",
      "## [BEST epoch], 71 seconds.\n",
      "\n",
      "-- train epoch 16/100, batch 261/261 (100.00%), loss = 190.08.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 16/100 \"f1-alpha-match-10\" train / dev / test | 85.51 / 82.05 / 82.05.\n",
      "## [BEST epoch], 71 seconds.\n",
      "\n",
      "-- train epoch 17/100, batch 261/261 (100.00%), loss = 179.52.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 17/100 \"f1-alpha-match-10\" train / dev / test | 85.87 / 82.06 / 82.06.\n",
      "## [BEST epoch], 67 seconds.\n",
      "\n",
      "-- train epoch 18/100, batch 261/261 (100.00%), loss = 188.47.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 18/100 \"f1-alpha-match-10\" train / dev / test | 86.33 / 82.51 / 82.51.\n",
      "## [BEST epoch], 71 seconds.\n",
      "\n",
      "-- train epoch 19/100, batch 261/261 (100.00%), loss = 138.68.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 19/100 \"f1-alpha-match-10\" train / dev / test | 86.83 / 82.65 / 82.65.\n",
      "## [BEST epoch], 67 seconds.\n",
      "\n",
      "-- train epoch 20/100, batch 261/261 (100.00%), loss = 152.09.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 20/100 \"f1-alpha-match-10\" train / dev / test | 87.10 / 82.42 / 82.42.\n",
      "## [no improvement micro-f1 on DEV during the last 1 epochs (best_f1_dev=82.65), 72 seconds].\n",
      "\n",
      "-- train epoch 21/100, batch 261/261 (100.00%), loss = 160.41.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 21/100 \"f1-alpha-match-10\" train / dev / test | 87.50 / 82.73 / 82.73.\n",
      "## [BEST epoch], 71 seconds.\n",
      "\n",
      "-- train epoch 22/100, batch 261/261 (100.00%), loss = 171.16.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 22/100 \"f1-alpha-match-10\" train / dev / test | 87.50 / 82.93 / 82.93.\n",
      "## [BEST epoch], 68 seconds.\n",
      "\n",
      "-- train epoch 23/100, batch 261/261 (100.00%), loss = 157.21.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 23/100 \"f1-alpha-match-10\" train / dev / test | 89.01 / 82.67 / 82.67.\n",
      "## [no improvement micro-f1 on DEV during the last 1 epochs (best_f1_dev=82.93), 68 seconds].\n",
      "\n",
      "-- train epoch 24/100, batch 261/261 (100.00%), loss = 148.83.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 24/100 \"f1-alpha-match-10\" train / dev / test | 88.11 / 81.74 / 81.74.\n",
      "## [no improvement micro-f1 on DEV during the last 2 epochs (best_f1_dev=82.93), 69 seconds].\n",
      "\n",
      "-- train epoch 25/100, batch 261/261 (100.00%), loss = 142.88.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 25/100 \"f1-alpha-match-10\" train / dev / test | 89.19 / 83.50 / 83.50.\n",
      "## [BEST epoch], 71 seconds.\n",
      "\n",
      "-- train epoch 26/100, batch 261/261 (100.00%), loss = 129.07.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 26/100 \"f1-alpha-match-10\" train / dev / test | 89.51 / 83.66 / 83.66.\n",
      "## [BEST epoch], 71 seconds.\n",
      "\n",
      "-- train epoch 27/100, batch 261/261 (100.00%), loss = 123.05.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 27/100 \"f1-alpha-match-10\" train / dev / test | 88.88 / 82.74 / 82.74.\n",
      "## [no improvement micro-f1 on DEV during the last 1 epochs (best_f1_dev=83.66), 68 seconds].\n",
      "\n",
      "-- train epoch 28/100, batch 261/261 (100.00%), loss = 122.57.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 28/100 \"f1-alpha-match-10\" train / dev / test | 89.96 / 83.32 / 83.32.\n",
      "## [no improvement micro-f1 on DEV during the last 2 epochs (best_f1_dev=83.66), 62 seconds].\n",
      "\n",
      "-- train epoch 29/100, batch 261/261 (100.00%), loss = 133.82.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 29/100 \"f1-alpha-match-10\" train / dev / test | 89.92 / 83.82 / 83.82.\n",
      "## [BEST epoch], 69 seconds.\n",
      "\n",
      "-- train epoch 30/100, batch 261/261 (100.00%), loss = 123.55.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 30/100 \"f1-alpha-match-10\" train / dev / test | 90.04 / 82.99 / 82.99.\n",
      "## [no improvement micro-f1 on DEV during the last 1 epochs (best_f1_dev=83.82), 64 seconds].\n",
      "\n",
      "-- train epoch 31/100, batch 261/261 (100.00%), loss = 112.52.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 31/100 \"f1-alpha-match-10\" train / dev / test | 90.58 / 83.10 / 83.10.\n",
      "## [no improvement micro-f1 on DEV during the last 2 epochs (best_f1_dev=83.82), 64 seconds].\n",
      "\n",
      "-- train epoch 32/100, batch 261/261 (100.00%), loss = 99.79.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 32/100 \"f1-alpha-match-10\" train / dev / test | 90.62 / 83.30 / 83.30.\n",
      "## [no improvement micro-f1 on DEV during the last 3 epochs (best_f1_dev=83.82), 63 seconds].\n",
      "\n",
      "-- train epoch 33/100, batch 261/261 (100.00%), loss = 121.00.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 33/100 \"f1-alpha-match-10\" train / dev / test | 90.76 / 82.75 / 82.75.\n",
      "## [no improvement micro-f1 on DEV during the last 4 epochs (best_f1_dev=83.82), 66 seconds].\n",
      "\n",
      "-- train epoch 34/100, batch 261/261 (100.00%), loss = 102.97.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 34/100 \"f1-alpha-match-10\" train / dev / test | 91.08 / 82.92 / 82.92.\n",
      "## [no improvement micro-f1 on DEV during the last 5 epochs (best_f1_dev=83.82), 69 seconds].\n",
      "\n",
      "-- train epoch 35/100, batch 261/261 (100.00%), loss = 119.15.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 35/100 \"f1-alpha-match-10\" train / dev / test | 91.61 / 82.79 / 82.79.\n",
      "## [no improvement micro-f1 on DEV during the last 6 epochs (best_f1_dev=83.82), 70 seconds].\n",
      "\n",
      "-- train epoch 36/100, batch 261/261 (100.00%), loss = 110.34.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 36/100 \"f1-alpha-match-10\" train / dev / test | 91.58 / 83.79 / 83.79.\n",
      "## [no improvement micro-f1 on DEV during the last 7 epochs (best_f1_dev=83.82), 69 seconds].\n",
      "\n",
      "-- train epoch 37/100, batch 261/261 (100.00%), loss = 121.83.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 37/100 \"f1-alpha-match-10\" train / dev / test | 91.49 / 82.79 / 82.79.\n",
      "## [no improvement micro-f1 on DEV during the last 8 epochs (best_f1_dev=83.82), 70 seconds].\n",
      "\n",
      "-- train epoch 38/100, batch 261/261 (100.00%), loss = 114.21.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 38/100 \"f1-alpha-match-10\" train / dev / test | 92.35 / 83.30 / 83.30.\n",
      "## [no improvement micro-f1 on DEV during the last 9 epochs (best_f1_dev=83.82), 73 seconds].\n",
      "\n",
      "-- train epoch 39/100, batch 261/261 (100.00%), loss = 117.00.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 39/100 \"f1-alpha-match-10\" train / dev / test | 92.31 / 84.83 / 84.83.\n",
      "## [BEST epoch], 73 seconds.\n",
      "\n",
      "-- train epoch 40/100, batch 261/261 (100.00%), loss = 97.10.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 40/100 \"f1-alpha-match-10\" train / dev / test | 92.69 / 83.19 / 83.19.\n",
      "## [no improvement micro-f1 on DEV during the last 1 epochs (best_f1_dev=84.83), 72 seconds].\n",
      "\n",
      "-- train epoch 41/100, batch 261/261 (100.00%), loss = 94.96.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 41/100 \"f1-alpha-match-10\" train / dev / test | 92.77 / 84.27 / 84.27.\n",
      "## [no improvement micro-f1 on DEV during the last 2 epochs (best_f1_dev=84.83), 67 seconds].\n",
      "\n",
      "-- train epoch 42/100, batch 261/261 (100.00%), loss = 113.93.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 42/100 \"f1-alpha-match-10\" train / dev / test | 92.10 / 82.92 / 82.92.\n",
      "## [no improvement micro-f1 on DEV during the last 3 epochs (best_f1_dev=84.83), 70 seconds].\n",
      "\n",
      "-- train epoch 43/100, batch 261/261 (100.00%), loss = 95.82.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 43/100 \"f1-alpha-match-10\" train / dev / test | 92.90 / 83.62 / 83.62.\n",
      "## [no improvement micro-f1 on DEV during the last 4 epochs (best_f1_dev=84.83), 69 seconds].\n",
      "\n",
      "-- train epoch 44/100, batch 261/261 (100.00%), loss = 96.58.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 44/100 \"f1-alpha-match-10\" train / dev / test | 93.39 / 84.31 / 84.31.\n",
      "## [no improvement micro-f1 on DEV during the last 5 epochs (best_f1_dev=84.83), 68 seconds].\n",
      "\n",
      "-- train epoch 45/100, batch 261/261 (100.00%), loss = 96.02.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 45/100 \"f1-alpha-match-10\" train / dev / test | 93.06 / 83.96 / 83.96.\n",
      "## [no improvement micro-f1 on DEV during the last 6 epochs (best_f1_dev=84.83), 72 seconds].\n",
      "\n",
      "-- train epoch 46/100, batch 261/261 (100.00%), loss = 108.50.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 46/100 \"f1-alpha-match-10\" train / dev / test | 93.27 / 84.47 / 84.47.\n",
      "## [no improvement micro-f1 on DEV during the last 7 epochs (best_f1_dev=84.83), 72 seconds].\n",
      "\n",
      "-- train epoch 47/100, batch 261/261 (100.00%), loss = 106.51.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 47/100 \"f1-alpha-match-10\" train / dev / test | 93.14 / 84.07 / 84.07.\n",
      "## [no improvement micro-f1 on DEV during the last 8 epochs (best_f1_dev=84.83), 72 seconds].\n",
      "\n",
      "-- train epoch 48/100, batch 261/261 (100.00%), loss = 102.06.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 48/100 \"f1-alpha-match-10\" train / dev / test | 93.64 / 84.04 / 84.04.\n",
      "## [no improvement micro-f1 on DEV during the last 9 epochs (best_f1_dev=84.83), 69 seconds].\n",
      "\n",
      "-- train epoch 49/100, batch 261/261 (100.00%), loss = 97.60.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 49/100 \"f1-alpha-match-10\" train / dev / test | 94.11 / 84.24 / 84.24.\n",
      "## [no improvement micro-f1 on DEV during the last 10 epochs (best_f1_dev=84.83), 69 seconds].\n",
      "\n",
      "-- train epoch 50/100, batch 261/261 (100.00%), loss = 92.51.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 50/100 \"f1-alpha-match-10\" train / dev / test | 93.97 / 84.08 / 84.08.\n",
      "## [no improvement micro-f1 on DEV during the last 11 epochs (best_f1_dev=84.83), 67 seconds].\n",
      "\n",
      "-- train epoch 51/100, batch 261/261 (100.00%), loss = 91.43.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 51/100 \"f1-alpha-match-10\" train / dev / test | 94.32 / 85.04 / 85.04.\n",
      "## [BEST epoch], 70 seconds.\n",
      "\n",
      "-- train epoch 52/100, batch 261/261 (100.00%), loss = 88.85.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 52/100 \"f1-alpha-match-10\" train / dev / test | 94.63 / 85.07 / 85.07.\n",
      "## [BEST epoch], 67 seconds.\n",
      "\n",
      "-- train epoch 53/100, batch 261/261 (100.00%), loss = 73.28.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 53/100 \"f1-alpha-match-10\" train / dev / test | 93.51 / 84.07 / 84.07.\n",
      "## [no improvement micro-f1 on DEV during the last 1 epochs (best_f1_dev=85.07), 70 seconds].\n",
      "\n",
      "-- train epoch 54/100, batch 261/261 (100.00%), loss = 94.17.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 54/100 \"f1-alpha-match-10\" train / dev / test | 94.92 / 84.98 / 84.98.\n",
      "## [no improvement micro-f1 on DEV during the last 2 epochs (best_f1_dev=85.07), 73 seconds].\n",
      "\n",
      "-- train epoch 55/100, batch 261/261 (100.00%), loss = 90.98.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 55/100 \"f1-alpha-match-10\" train / dev / test | 94.52 / 84.40 / 84.40.\n",
      "## [no improvement micro-f1 on DEV during the last 3 epochs (best_f1_dev=85.07), 72 seconds].\n",
      "\n",
      "-- train epoch 56/100, batch 261/261 (100.00%), loss = 69.39.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 56/100 \"f1-alpha-match-10\" train / dev / test | 94.66 / 84.66 / 84.66.\n",
      "## [no improvement micro-f1 on DEV during the last 4 epochs (best_f1_dev=85.07), 70 seconds].\n",
      "\n",
      "-- train epoch 57/100, batch 261/261 (100.00%), loss = 81.71.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 57/100 \"f1-alpha-match-10\" train / dev / test | 94.62 / 85.06 / 85.06.\n",
      "## [no improvement micro-f1 on DEV during the last 5 epochs (best_f1_dev=85.07), 70 seconds].\n",
      "\n",
      "-- train epoch 58/100, batch 261/261 (100.00%), loss = 78.84.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 58/100 \"f1-alpha-match-10\" train / dev / test | 94.92 / 83.56 / 83.56.\n",
      "## [no improvement micro-f1 on DEV during the last 6 epochs (best_f1_dev=85.07), 69 seconds].\n",
      "\n",
      "-- train epoch 59/100, batch 261/261 (100.00%), loss = 83.32.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 59/100 \"f1-alpha-match-10\" train / dev / test | 94.87 / 84.47 / 84.47.\n",
      "## [no improvement micro-f1 on DEV during the last 7 epochs (best_f1_dev=85.07), 69 seconds].\n",
      "\n",
      "-- train epoch 60/100, batch 261/261 (100.00%), loss = 82.65.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 60/100 \"f1-alpha-match-10\" train / dev / test | 95.05 / 84.93 / 84.93.\n",
      "## [no improvement micro-f1 on DEV during the last 8 epochs (best_f1_dev=85.07), 70 seconds].\n",
      "\n",
      "-- train epoch 61/100, batch 261/261 (100.00%), loss = 64.26.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 61/100 \"f1-alpha-match-10\" train / dev / test | 95.02 / 84.74 / 84.74.\n",
      "## [no improvement micro-f1 on DEV during the last 9 epochs (best_f1_dev=85.07), 68 seconds].\n",
      "\n",
      "-- train epoch 62/100, batch 261/261 (100.00%), loss = 90.81.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 62/100 \"f1-alpha-match-10\" train / dev / test | 94.59 / 84.58 / 84.58.\n",
      "## [no improvement micro-f1 on DEV during the last 10 epochs (best_f1_dev=85.07), 68 seconds].\n",
      "\n",
      "-- train epoch 63/100, batch 261/261 (100.00%), loss = 77.32.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 63/100 \"f1-alpha-match-10\" train / dev / test | 95.50 / 84.52 / 84.52.\n",
      "## [no improvement micro-f1 on DEV during the last 11 epochs (best_f1_dev=85.07), 69 seconds].\n",
      "\n",
      "-- train epoch 64/100, batch 261/261 (100.00%), loss = 86.82.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 64/100 \"f1-alpha-match-10\" train / dev / test | 95.48 / 84.06 / 84.06.\n",
      "## [no improvement micro-f1 on DEV during the last 12 epochs (best_f1_dev=85.07), 69 seconds].\n",
      "\n",
      "-- train epoch 65/100, batch 261/261 (100.00%), loss = 65.77.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 65/100 \"f1-alpha-match-10\" train / dev / test | 95.64 / 84.29 / 84.29.\n",
      "## [no improvement micro-f1 on DEV during the last 13 epochs (best_f1_dev=85.07), 64 seconds].\n",
      "\n",
      "-- train epoch 66/100, batch 261/261 (100.00%), loss = 82.67.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 66/100 \"f1-alpha-match-10\" train / dev / test | 95.49 / 84.61 / 84.61.\n",
      "## [no improvement micro-f1 on DEV during the last 14 epochs (best_f1_dev=85.07), 68 seconds].\n",
      "\n",
      "-- train epoch 67/100, batch 261/261 (100.00%), loss = 78.83.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 67/100 \"f1-alpha-match-10\" train / dev / test | 95.16 / 84.77 / 84.77.\n",
      "## [no improvement micro-f1 on DEV during the last 15 epochs (best_f1_dev=85.07), 69 seconds].\n",
      "\n",
      "-- train epoch 68/100, batch 261/261 (100.00%), loss = 80.57.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 68/100 \"f1-alpha-match-10\" train / dev / test | 95.31 / 84.56 / 84.56.\n",
      "## [no improvement micro-f1 on DEV during the last 16 epochs (best_f1_dev=85.07), 64 seconds].\n",
      "\n",
      "-- train epoch 69/100, batch 261/261 (100.00%), loss = 72.96.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 69/100 \"f1-alpha-match-10\" train / dev / test | 95.56 / 84.60 / 84.60.\n",
      "## [no improvement micro-f1 on DEV during the last 17 epochs (best_f1_dev=85.07), 66 seconds].\n",
      "\n",
      "-- train epoch 70/100, batch 261/261 (100.00%), loss = 79.23.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 70/100 \"f1-alpha-match-10\" train / dev / test | 95.49 / 84.55 / 84.55.\n",
      "## [no improvement micro-f1 on DEV during the last 18 epochs (best_f1_dev=85.07), 69 seconds].\n",
      "\n",
      "-- train epoch 71/100, batch 261/261 (100.00%), loss = 71.18.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 71/100 \"f1-alpha-match-10\" train / dev / test | 95.55 / 84.34 / 84.34.\n",
      "## [no improvement micro-f1 on DEV during the last 19 epochs (best_f1_dev=85.07), 71 seconds].\n",
      "\n",
      "-- train epoch 72/100, batch 261/261 (100.00%), loss = 68.98.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 72/100 \"f1-alpha-match-10\" train / dev / test | 95.71 / 84.57 / 84.57.\n",
      "## [no improvement micro-f1 on DEV during the last 20 epochs (best_f1_dev=85.07), 65 seconds].\n",
      "\n",
      "-- train epoch 73/100, batch 261/261 (100.00%), loss = 63.54.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 73/100 \"f1-alpha-match-10\" train / dev / test | 95.56 / 84.14 / 84.14.\n",
      "## [no improvement micro-f1 on DEV during the last 21 epochs (best_f1_dev=85.07), 69 seconds].\n",
      "\n",
      "Evaluation\n",
      "\n",
      "batch_size=10\n",
      "char_cnn_filter_num=30\n",
      "char_embeddings_dim=25\n",
      "char_window_size=3\n",
      "check_for_lowercase=True\n",
      "clip_grad=5\n",
      "cross_fold_id=-1\n",
      "cross_folds_num=-1\n",
      "data_io='connl-ner-2003'\n",
      "dataset_sort=False\n",
      "dev='/home/vika/NER_RNN/targer/data/NER/Asqua_CAM_aspects/CAM_test.csv'\n",
      "dropout_ratio=0.5\n",
      "emb_delimiter=' '\n",
      "emb_dim=100\n",
      "emb_fn='embeddings/glove.6B.100d.txt'\n",
      "emb_load_all=False\n",
      "epoch_num=100\n",
      "evaluator='f1-alpha-match-10'\n",
      "freeze_char_embeddings=False\n",
      "freeze_word_embeddings=False\n",
      "gpu=1\n",
      "load=None\n",
      "lr=0.001\n",
      "lr_decay=0.05\n",
      "min_epoch_num=50\n",
      "model='BiRNNCRF'\n",
      "momentum=0.9\n",
      "opt='adam'\n",
      "patience=20\n",
      "report_fn='2019_08_22_12-07_55_report.txt'\n",
      "rnn_hidden_dim=200\n",
      "rnn_type='LSTM'\n",
      "save='BiRNNCFR.hdf5'\n",
      "save_best=True\n",
      "seed_num=42\n",
      "test='/home/vika/NER_RNN/targer/data/NER/Asqua_CAM_aspects/CAM_test.csv'\n",
      "train='/home/vika/NER_RNN/targer/data/NER/Asqua_CAM_aspects/CAM_train.csv'\n",
      "verbose=True\n",
      "word_len=20\n",
      "word_seq_indexer=None\n",
      "\n",
      "         epoch  |     train loss | f1-alpha-match-10-train | f1-alpha-match-10-dev | f1-alpha-match-10-test \n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "              0 |           0.00 |           3.48 |           3.55 |           3.55 \n",
      "              1 |         709.25 |          60.42 |          59.59 |          59.59 \n",
      "              2 |         394.92 |          69.55 |          66.87 |          66.87 \n",
      "              3 |         341.32 |          72.22 |          67.51 |          67.51 \n",
      "              4 |         328.28 |          74.17 |          71.47 |          71.47 \n",
      "              5 |         305.78 |          75.36 |          72.44 |          72.44 \n",
      "              6 |         301.25 |          77.64 |          76.34 |          76.34 \n",
      "              7 |         250.16 |          76.98 |          75.18 |          75.18 \n",
      "              8 |         220.17 |          79.53 |          77.39 |          77.39 \n",
      "              9 |         221.51 |          80.71 |          78.51 |          78.51 \n",
      "             10 |         228.58 |          80.51 |          78.53 |          78.53 \n",
      "             11 |         198.93 |          81.92 |          78.74 |          78.74 \n",
      "             12 |         221.02 |          84.32 |          80.58 |          80.58 \n",
      "             13 |         199.78 |          84.21 |          80.75 |          80.75 \n",
      "             14 |         181.75 |          83.41 |          80.27 |          80.27 \n",
      "             15 |         153.56 |          84.70 |          81.14 |          81.14 \n",
      "             16 |         190.08 |          85.51 |          82.05 |          82.05 \n",
      "             17 |         179.52 |          85.87 |          82.06 |          82.06 \n",
      "             18 |         188.47 |          86.33 |          82.51 |          82.51 \n",
      "             19 |         138.68 |          86.83 |          82.65 |          82.65 \n",
      "             20 |         152.09 |          87.10 |          82.42 |          82.42 \n",
      "             21 |         160.41 |          87.50 |          82.73 |          82.73 \n",
      "             22 |         171.16 |          87.50 |          82.93 |          82.93 \n",
      "             23 |         157.21 |          89.01 |          82.67 |          82.67 \n",
      "             24 |         148.83 |          88.11 |          81.74 |          81.74 \n",
      "             25 |         142.88 |          89.19 |          83.50 |          83.50 \n",
      "             26 |         129.07 |          89.51 |          83.66 |          83.66 \n",
      "             27 |         123.05 |          88.88 |          82.74 |          82.74 \n",
      "             28 |         122.57 |          89.96 |          83.32 |          83.32 \n",
      "             29 |         133.82 |          89.92 |          83.82 |          83.82 \n",
      "             30 |         123.55 |          90.04 |          82.99 |          82.99 \n",
      "             31 |         112.52 |          90.58 |          83.10 |          83.10 \n",
      "             32 |          99.79 |          90.62 |          83.30 |          83.30 \n",
      "             33 |         121.00 |          90.76 |          82.75 |          82.75 \n",
      "             34 |         102.97 |          91.08 |          82.92 |          82.92 \n",
      "             35 |         119.15 |          91.61 |          82.79 |          82.79 \n",
      "             36 |         110.34 |          91.58 |          83.79 |          83.79 \n",
      "             37 |         121.83 |          91.49 |          82.79 |          82.79 \n",
      "             38 |         114.21 |          92.35 |          83.30 |          83.30 \n",
      "             39 |         117.00 |          92.31 |          84.83 |          84.83 \n",
      "             40 |          97.10 |          92.69 |          83.19 |          83.19 \n",
      "             41 |          94.96 |          92.77 |          84.27 |          84.27 \n",
      "             42 |         113.93 |          92.10 |          82.92 |          82.92 \n",
      "             43 |          95.82 |          92.90 |          83.62 |          83.62 \n",
      "             44 |          96.58 |          93.39 |          84.31 |          84.31 \n",
      "             45 |          96.02 |          93.06 |          83.96 |          83.96 \n",
      "             46 |         108.50 |          93.27 |          84.47 |          84.47 \n",
      "             47 |         106.51 |          93.14 |          84.07 |          84.07 \n",
      "             48 |         102.06 |          93.64 |          84.04 |          84.04 \n",
      "             49 |          97.60 |          94.11 |          84.24 |          84.24 \n",
      "             50 |          92.51 |          93.97 |          84.08 |          84.08 \n",
      "             51 |          91.43 |          94.32 |          85.04 |          85.04 \n",
      "             52 |          88.85 |          94.63 |          85.07 |          85.07 \n",
      "             53 |          73.28 |          93.51 |          84.07 |          84.07 \n",
      "             54 |          94.17 |          94.92 |          84.98 |          84.98 \n",
      "             55 |          90.98 |          94.52 |          84.40 |          84.40 \n",
      "             56 |          69.39 |          94.66 |          84.66 |          84.66 \n",
      "             57 |          81.71 |          94.62 |          85.06 |          85.06 \n",
      "             58 |          78.84 |          94.92 |          83.56 |          83.56 \n",
      "             59 |          83.32 |          94.87 |          84.47 |          84.47 \n",
      "             60 |          82.65 |          95.05 |          84.93 |          84.93 \n",
      "             61 |          64.26 |          95.02 |          84.74 |          84.74 \n",
      "             62 |          90.81 |          94.59 |          84.58 |          84.58 \n",
      "             63 |          77.32 |          95.50 |          84.52 |          84.52 \n",
      "             64 |          86.82 |          95.48 |          84.06 |          84.06 \n",
      "             65 |          65.77 |          95.64 |          84.29 |          84.29 \n",
      "             66 |          82.67 |          95.49 |          84.61 |          84.61 \n",
      "             67 |          78.83 |          95.16 |          84.77 |          84.77 \n",
      "             68 |          80.57 |          95.31 |          84.56 |          84.56 \n",
      "             69 |          72.96 |          95.56 |          84.60 |          84.60 \n",
      "             70 |          79.23 |          95.49 |          84.55 |          84.55 \n",
      "             71 |          71.18 |          95.55 |          84.34 |          84.34 \n",
      "             72 |          68.98 |          95.71 |          84.57 |          84.57 \n",
      "             73 |          63.54 |          95.56 |          84.14 |          84.14 \n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Final eval on test, \"save best\", best epoch on dev 52, f1-alpha-match-10, test = 85.07)\n",
      "--------------------------------------------------------------------------------------------------------------*** f1 alpha match, alpha = 1.0\n",
      "*** f1 = 85.07, precision = 87.37, recall = 82.89\n",
      "*** TP = 1114, FP = 161, FN = 230\n",
      "Input arguments:\n",
      "python3 main.py --train /home/vika/NER_RNN/targer/data/NER/Asqua_CAM_aspects/CAM_train.csv --dev /home/vika/NER_RNN/targer/data/NER/Asqua_CAM_aspects/CAM_test.csv --test /home/vika/NER_RNN/targer/data/NER/Asqua_CAM_aspects/CAM_dev.csv --data-io connl-ner-2003 --evaluator f1-alpha-match-10 --model BiRNNCRF --opt adam --lr 0.001 --save-best yes --patience 20 --rnn-hidden-dim 200 --gpu 1 --test /home/vika/NER_RNN/targer/data/NER/Asqua_CAM_aspects/CAM_test.csv --save BiRNNCFR.hdf5\n",
      "\n",
      "85.0706\n"
     ]
    }
   ],
   "source": [
    "! python3 main.py --train \"/home/vika/NER_RNN/targer/data/NER/Asqua_CAM_aspects/CAM_train.csv\" --dev \"/home/vika/NER_RNN/targer/data/NER/Asqua_CAM_aspects/CAM_test.csv\" --test \"/home/vika/NER_RNN/targer/data/NER/Asqua_CAM_aspects/CAM_dev.csv\" --data-io connl-ner-2003 --evaluator f1-alpha-match-10 --model BiRNNCRF --opt adam --lr 0.001 --save-best yes --patience 20 --rnn-hidden-dim 200 --gpu 1 --test \"/home/vika/NER_RNN/targer/data/NER/Asqua_CAM_aspects/CAM_test.csv\" --save \"BiRNNCFR.hdf5\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "args train = /home/vika/NER_RNN/targer/data/NER/Asqua_CAM_aspects/CAM_train.csv\n",
      "Loading from /home/vika/NER_RNN/targer/data/NER/Asqua_CAM_aspects/CAM_train.csv: 2619 samples, 74616 words.\n",
      "Loading from /home/vika/NER_RNN/targer/data/NER/Asqua_CAM_aspects/CAM_dev.csv: 350 samples, 10092 words.\n",
      "Loading from /home/vika/NER_RNN/targer/data/NER/Asqua_CAM_aspects/CAM_test.csv: 523 samples, 14728 words.\n",
      "DatasetsBank: len(unique_words_list) = 7143 unique words.\n",
      "DatasetsBank: len(unique_words_list) = 7600 unique words.\n",
      "DatasetsBank: len(unique_words_list) = 8167 unique words.\n",
      "\n",
      "load_vocabulary_from_tag_sequences:\n",
      " -- class_num = 7\n",
      " -- {'<pad>': 0, 'O': 1, 'B-OTHOBJ': 2, 'B-ASPOBJ': 3, 'B-ASP': 4, 'I-ASP': 5, 'I-ASPOBJ': 6, 'I-OTHOBJ': 7}\n",
      "in main\n",
      "True\n",
      "/home/vika/NER_RNN/targer/embeddings/elmo_2x4096_512_2048cnn_2xhighway_5.5B_weights.hdf5\n",
      "/home/vika/NER_RNN/targer/embeddings/elmo_2x4096_512_2048cnn_2xhighway_5.5B_options.json\n",
      "init targer base\n",
      "True\n",
      "/home/vika/NER_RNN/targer/embeddings/elmo_2x4096_512_2048cnn_2xhighway_5.5B_weights.hdf5\n",
      "/home/vika/NER_RNN/targer/embeddings/elmo_2x4096_512_2048cnn_2xhighway_5.5B_options.json\n",
      "elmo is initiated\n",
      "init targer BiRNNCNNCRF\n",
      "True\n",
      "/home/vika/NER_RNN/targer/embeddings/elmo_2x4096_512_2048cnn_2xhighway_5.5B_weights.hdf5\n",
      "/home/vika/NER_RNN/targer/embeddings/elmo_2x4096_512_2048cnn_2xhighway_5.5B_options.json\n",
      "Empirical transition matrix from the train dataset:\n",
      "               <pad>         O  B-OTHOBJ  B-ASPOBJ     B-ASP     I-ASP  I-ASPOBJ  I-OTHOBJ     <sos>\n",
      "\n",
      "     <pad>       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0\n",
      "         O       0.0   58639.0    2722.0    2863.0     792.0     165.0      22.0      21.0    2226.0\n",
      "  B-OTHOBJ       0.0    2713.0       0.0       1.0       3.0       1.0       0.0       0.0      47.0\n",
      "  B-ASPOBJ       0.0    2560.0       8.0       0.0       4.0       3.0       0.0       0.0     341.0\n",
      "     B-ASP       0.0     919.0      14.0      28.0       0.0       0.0       0.0       0.0       5.0\n",
      "     I-ASP       0.0       0.0       0.0       2.0     167.0     307.0       0.0       0.0       0.0\n",
      "  I-ASPOBJ       0.0       0.0       0.0      22.0       0.0       0.0       0.0       0.0       0.0\n",
      "  I-OTHOBJ       0.0       0.0      21.0       0.0       0.0       0.0       0.0       0.0       0.0\n",
      "     <sos>       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0\n",
      "\n",
      "Initialized transition matrix:\n",
      "               <pad>         O  B-OTHOBJ  B-ASPOBJ     B-ASP     I-ASP  I-ASPOBJ  I-OTHOBJ     <sos>\n",
      "\n",
      "     <pad>   -9999.0   -9999.0   -9999.0   -9999.0   -9999.0   -9999.0   -9999.0   -9999.0   -9999.0\n",
      "         O   -9999.0      -1.0      -0.9      -1.1      -1.0      -1.1      -1.1      -1.0      -1.0\n",
      "  B-OTHOBJ   -9999.0      -1.0   -9999.0      -0.8      -1.1      -1.1   -9999.0   -9999.0      -1.1\n",
      "  B-ASPOBJ   -9999.0      -1.0      -1.0   -9999.0      -1.2      -0.9   -9999.0   -9999.0      -0.9\n",
      "     B-ASP   -9999.0      -1.0      -0.8      -1.1   -9999.0   -9999.0   -9999.0   -9999.0      -1.0\n",
      "     I-ASP   -9999.0   -9999.0   -9999.0      -1.1      -1.0      -0.9   -9999.0   -9999.0   -9999.0\n",
      "  I-ASPOBJ   -9999.0   -9999.0   -9999.0      -0.9   -9999.0   -9999.0   -9999.0   -9999.0   -9999.0\n",
      "  I-OTHOBJ   -9999.0   -9999.0      -0.9   -9999.0   -9999.0   -9999.0   -9999.0   -9999.0   -9999.0\n",
      "     <sos>   -9999.0   -9999.0   -9999.0   -9999.0   -9999.0   -9999.0   -9999.0   -9999.0   -9999.0\n",
      "\n",
      "Start training...\n",
      "\n",
      "\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 0/50 \"f1-alpha-match-10\" train / dev / test | 4.90 / 4.60 / 4.71.\n",
      "## [BEST epoch], 18 seconds.\n",
      "\n",
      "-- train epoch 1/50, batch 261/261 (100.00%), loss = 481.85.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 1/50 \"f1-alpha-match-10\" train / dev / test | 59.00 / 55.35 / 57.57.\n",
      "## [BEST epoch], 105 seconds.\n",
      "\n",
      "-- train epoch 2/50, batch 261/261 (100.00%), loss = 295.73.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 2/50 \"f1-alpha-match-10\" train / dev / test | 62.07 / 57.16 / 60.13.\n",
      "## [BEST epoch], 102 seconds.\n",
      "\n",
      "-- train epoch 3/50, batch 261/261 (100.00%), loss = 269.04.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 3/50 \"f1-alpha-match-10\" train / dev / test | 68.30 / 64.60 / 67.97.\n",
      "## [BEST epoch], 105 seconds.\n",
      "\n",
      "-- train epoch 4/50, batch 261/261 (100.00%), loss = 251.46.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 4/50 \"f1-alpha-match-10\" train / dev / test | 68.73 / 61.71 / 64.08.\n",
      "## [no improvement micro-f1 on DEV during the last 1 epochs (best_f1_dev=64.60), 103 seconds].\n",
      "\n",
      "-- train epoch 5/50, batch 261/261 (100.00%), loss = 236.51.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 5/50 \"f1-alpha-match-10\" train / dev / test | 72.35 / 67.72 / 67.84.\n",
      "## [BEST epoch], 106 seconds.\n",
      "\n",
      "-- train epoch 6/50, batch 261/261 (100.00%), loss = 199.35.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 6/50 \"f1-alpha-match-10\" train / dev / test | 76.23 / 70.89 / 73.64.\n",
      "## [BEST epoch], 102 seconds.\n",
      "\n",
      "-- train epoch 7/50, batch 261/261 (100.00%), loss = 191.80.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 7/50 \"f1-alpha-match-10\" train / dev / test | 77.89 / 71.71 / 73.26.\n",
      "## [BEST epoch], 105 seconds.\n",
      "\n",
      "-- train epoch 8/50, batch 261/261 (100.00%), loss = 194.80.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 8/50 \"f1-alpha-match-10\" train / dev / test | 80.15 / 72.80 / 74.81.\n",
      "## [BEST epoch], 105 seconds.\n",
      "\n",
      "-- train epoch 9/50, batch 261/261 (100.00%), loss = 182.43.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 9/50 \"f1-alpha-match-10\" train / dev / test | 82.59 / 75.04 / 76.26.\n",
      "## [BEST epoch], 103 seconds.\n",
      "\n",
      "-- train epoch 10/50, batch 261/261 (100.00%), loss = 161.46.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 10/50 \"f1-alpha-match-10\" train / dev / test | 81.40 / 74.15 / 76.13.\n",
      "## [no improvement micro-f1 on DEV during the last 1 epochs (best_f1_dev=75.04), 103 seconds].\n",
      "\n",
      "-- train epoch 11/50, batch 261/261 (100.00%), loss = 155.96.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 11/50 \"f1-alpha-match-10\" train / dev / test | 83.04 / 75.93 / 75.98.\n",
      "## [BEST epoch], 102 seconds.\n",
      "\n",
      "-- train epoch 12/50, batch 261/261 (100.00%), loss = 133.37.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 12/50 \"f1-alpha-match-10\" train / dev / test | 84.07 / 76.17 / 77.15.\n",
      "## [BEST epoch], 105 seconds.\n",
      "\n",
      "-- train epoch 13/50, batch 261/261 (100.00%), loss = 132.98.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 13/50 \"f1-alpha-match-10\" train / dev / test | 85.94 / 79.86 / 78.64.\n",
      "## [BEST epoch], 101 seconds.\n",
      "\n",
      "-- train epoch 14/50, batch 261/261 (100.00%), loss = 126.90.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 14/50 \"f1-alpha-match-10\" train / dev / test | 85.51 / 79.51 / 78.18.\n",
      "## [no improvement micro-f1 on DEV during the last 1 epochs (best_f1_dev=79.86), 101 seconds].\n",
      "\n",
      "-- train epoch 15/50, batch 261/261 (100.00%), loss = 121.69.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 15/50 \"f1-alpha-match-10\" train / dev / test | 86.60 / 79.70 / 80.15.\n",
      "## [no improvement micro-f1 on DEV during the last 2 epochs (best_f1_dev=79.86), 104 seconds].\n",
      "\n",
      "-- train epoch 16/50, batch 261/261 (100.00%), loss = 138.85.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 16/50 \"f1-alpha-match-10\" train / dev / test | 88.35 / 80.11 / 80.03.\n",
      "## [BEST epoch], 105 seconds.\n",
      "\n",
      "-- train epoch 17/50, batch 261/261 (100.00%), loss = 122.46.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 17/50 \"f1-alpha-match-10\" train / dev / test | 88.16 / 79.81 / 79.69.\n",
      "## [no improvement micro-f1 on DEV during the last 1 epochs (best_f1_dev=80.11), 102 seconds].\n",
      "\n",
      "-- train epoch 18/50, batch 261/261 (100.00%), loss = 131.72.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 18/50 \"f1-alpha-match-10\" train / dev / test | 87.68 / 78.88 / 80.22.\n",
      "## [no improvement micro-f1 on DEV during the last 2 epochs (best_f1_dev=80.11), 100 seconds].\n",
      "\n",
      "-- train epoch 19/50, batch 261/261 (100.00%), loss = 88.13.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 19/50 \"f1-alpha-match-10\" train / dev / test | 89.13 / 79.72 / 81.47.\n",
      "## [no improvement micro-f1 on DEV during the last 3 epochs (best_f1_dev=80.11), 107 seconds].\n",
      "\n",
      "-- train epoch 20/50, batch 261/261 (100.00%), loss = 87.74.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 20/50 \"f1-alpha-match-10\" train / dev / test | 90.42 / 80.82 / 81.98.\n",
      "## [BEST epoch], 107 seconds.\n",
      "\n",
      "-- train epoch 21/50, batch 261/261 (100.00%), loss = 104.64.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 21/50 \"f1-alpha-match-10\" train / dev / test | 90.00 / 79.43 / 79.02.\n",
      "## [no improvement micro-f1 on DEV during the last 1 epochs (best_f1_dev=80.82), 98 seconds].\n",
      "\n",
      "-- train epoch 22/50, batch 261/261 (100.00%), loss = 122.44.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 22/50 \"f1-alpha-match-10\" train / dev / test | 90.90 / 81.32 / 82.05.\n",
      "## [BEST epoch], 103 seconds.\n",
      "\n",
      "-- train epoch 23/50, batch 261/261 (100.00%), loss = 105.05.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 23/50 \"f1-alpha-match-10\" train / dev / test | 91.73 / 81.25 / 82.00.\n",
      "## [no improvement micro-f1 on DEV during the last 1 epochs (best_f1_dev=81.32), 99 seconds].\n",
      "\n",
      "-- train epoch 24/50, batch 261/261 (100.00%), loss = 94.19.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 24/50 \"f1-alpha-match-10\" train / dev / test | 91.42 / 80.27 / 80.49.\n",
      "## [no improvement micro-f1 on DEV during the last 2 epochs (best_f1_dev=81.32), 101 seconds].\n",
      "\n",
      "-- train epoch 25/50, batch 261/261 (100.00%), loss = 90.64.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 25/50 \"f1-alpha-match-10\" train / dev / test | 92.18 / 81.37 / 81.72.\n",
      "## [BEST epoch], 104 seconds.\n",
      "\n",
      "-- train epoch 26/50, batch 261/261 (100.00%), loss = 74.65.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 26/50 \"f1-alpha-match-10\" train / dev / test | 92.34 / 81.23 / 82.26.\n",
      "## [no improvement micro-f1 on DEV during the last 1 epochs (best_f1_dev=81.37), 101 seconds].\n",
      "\n",
      "-- train epoch 27/50, batch 261/261 (100.00%), loss = 86.79.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 27/50 \"f1-alpha-match-10\" train / dev / test | 92.77 / 81.65 / 81.60.\n",
      "## [BEST epoch], 99 seconds.\n",
      "\n",
      "-- train epoch 28/50, batch 261/261 (100.00%), loss = 82.92.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 28/50 \"f1-alpha-match-10\" train / dev / test | 92.88 / 81.36 / 82.40.\n",
      "## [no improvement micro-f1 on DEV during the last 1 epochs (best_f1_dev=81.65), 100 seconds].\n",
      "\n",
      "-- train epoch 29/50, batch 261/261 (100.00%), loss = 85.63.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 29/50 \"f1-alpha-match-10\" train / dev / test | 93.46 / 82.59 / 83.06.\n",
      "## [BEST epoch], 99 seconds.\n",
      "\n",
      "-- train epoch 30/50, batch 261/261 (100.00%), loss = 77.07.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 30/50 \"f1-alpha-match-10\" train / dev / test | 93.13 / 81.27 / 81.97.\n",
      "## [no improvement micro-f1 on DEV during the last 1 epochs (best_f1_dev=82.59), 106 seconds].\n",
      "\n",
      "-- train epoch 31/50, batch 261/261 (100.00%), loss = 68.34.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 31/50 \"f1-alpha-match-10\" train / dev / test | 94.18 / 82.32 / 82.26.\n",
      "## [no improvement micro-f1 on DEV during the last 2 epochs (best_f1_dev=82.59), 104 seconds].\n",
      "\n",
      "-- train epoch 32/50, batch 261/261 (100.00%), loss = 47.47.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 32/50 \"f1-alpha-match-10\" train / dev / test | 94.13 / 82.28 / 83.40.\n",
      "## [no improvement micro-f1 on DEV during the last 3 epochs (best_f1_dev=82.59), 106 seconds].\n",
      "\n",
      "-- train epoch 33/50, batch 261/261 (100.00%), loss = 63.96.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 33/50 \"f1-alpha-match-10\" train / dev / test | 93.06 / 80.52 / 81.88.\n",
      "## [no improvement micro-f1 on DEV during the last 4 epochs (best_f1_dev=82.59), 96 seconds].\n",
      "\n",
      "-- train epoch 34/50, batch 261/261 (100.00%), loss = 52.10.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 34/50 \"f1-alpha-match-10\" train / dev / test | 94.05 / 81.69 / 82.91.\n",
      "## [no improvement micro-f1 on DEV during the last 5 epochs (best_f1_dev=82.59), 99 seconds].\n",
      "\n",
      "-- train epoch 35/50, batch 261/261 (100.00%), loss = 73.22.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 35/50 \"f1-alpha-match-10\" train / dev / test | 94.47 / 81.35 / 83.31.\n",
      "## [no improvement micro-f1 on DEV during the last 6 epochs (best_f1_dev=82.59), 101 seconds].\n",
      "\n",
      "-- train epoch 36/50, batch 261/261 (100.00%), loss = 60.37.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 36/50 \"f1-alpha-match-10\" train / dev / test | 94.69 / 82.40 / 83.21.\n",
      "## [no improvement micro-f1 on DEV during the last 7 epochs (best_f1_dev=82.59), 101 seconds].\n",
      "\n",
      "-- train epoch 37/50, batch 71/261 (27.00%), loss = 10.78."
     ]
    }
   ],
   "source": [
    "! python3 main.py --train \"/home/vika/NER_RNN/targer/data/NER/Asqua_CAM_aspects/CAM_train.csv\" --dev \"/home/vika/NER_RNN/targer/data/NER/Asqua_CAM_aspects/CAM_dev.csv\" --data-io connl-ner-2003 --evaluator f1-alpha-match-10 --opt adam --lr 0.001 --save-best yes --patience 20 --rnn-hidden-dim 200 --gpu 1 --test \"/home/vika/NER_RNN/targer/data/NER/Asqua_CAM_aspects/CAM_test.csv\" --epoch-num 50 --isElmo True --save \"regular_elmo.hdf5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "args train = /home/vika/NER_RNN/targer/data/NER/Asqua_CAM_aspects/CAM_train.csv\n",
      "Loading from /home/vika/NER_RNN/targer/data/NER/Asqua_CAM_aspects/CAM_train.csv: 2619 samples, 74616 words.\n",
      "Loading from /home/vika/NER_RNN/targer/data/NER/Asqua_CAM_aspects/CAM_dev.csv: 350 samples, 10092 words.\n",
      "Loading from /home/vika/NER_RNN/targer/data/NER/Asqua_CAM_aspects/CAM_qw_new.csv: 63 samples, 322 words.\n",
      "DatasetsBank: len(unique_words_list) = 7143 unique words.\n",
      "DatasetsBank: len(unique_words_list) = 7600 unique words.\n",
      "DatasetsBank: len(unique_words_list) = 7604 unique words.\n",
      "\n",
      "load_vocabulary_from_tag_sequences:\n",
      " -- class_num = 7\n",
      " -- {'<pad>': 0, 'O': 1, 'B-OTHOBJ': 2, 'B-ASPOBJ': 3, 'B-ASP': 4, 'I-ASP': 5, 'I-ASPOBJ': 6, 'I-OTHOBJ': 7}\n",
      "in main\n",
      "True\n",
      "/home/vika/NER_RNN/targer/embeddings/elmo_2x4096_512_2048cnn_2xhighway_5.5B_weights.hdf5\n",
      "/home/vika/NER_RNN/targer/embeddings/elmo_2x4096_512_2048cnn_2xhighway_5.5B_options.json\n",
      "init targer base\n",
      "True\n",
      "/home/vika/NER_RNN/targer/embeddings/elmo_2x4096_512_2048cnn_2xhighway_5.5B_weights.hdf5\n",
      "/home/vika/NER_RNN/targer/embeddings/elmo_2x4096_512_2048cnn_2xhighway_5.5B_options.json\n",
      "elmo is initiated\n",
      "init targer BiRNNCNNCRF\n",
      "True\n",
      "/home/vika/NER_RNN/targer/embeddings/elmo_2x4096_512_2048cnn_2xhighway_5.5B_weights.hdf5\n",
      "/home/vika/NER_RNN/targer/embeddings/elmo_2x4096_512_2048cnn_2xhighway_5.5B_options.json\n",
      "Empirical transition matrix from the train dataset:\n",
      "               <pad>         O  B-OTHOBJ  B-ASPOBJ     B-ASP     I-ASP  I-ASPOBJ  I-OTHOBJ     <sos>\n",
      "\n",
      "     <pad>       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0\n",
      "         O       0.0   58639.0    2722.0    2863.0     792.0     165.0      22.0      21.0    2226.0\n",
      "  B-OTHOBJ       0.0    2713.0       0.0       1.0       3.0       1.0       0.0       0.0      47.0\n",
      "  B-ASPOBJ       0.0    2560.0       8.0       0.0       4.0       3.0       0.0       0.0     341.0\n",
      "     B-ASP       0.0     919.0      14.0      28.0       0.0       0.0       0.0       0.0       5.0\n",
      "     I-ASP       0.0       0.0       0.0       2.0     167.0     307.0       0.0       0.0       0.0\n",
      "  I-ASPOBJ       0.0       0.0       0.0      22.0       0.0       0.0       0.0       0.0       0.0\n",
      "  I-OTHOBJ       0.0       0.0      21.0       0.0       0.0       0.0       0.0       0.0       0.0\n",
      "     <sos>       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0\n",
      "\n",
      "Initialized transition matrix:\n",
      "               <pad>         O  B-OTHOBJ  B-ASPOBJ     B-ASP     I-ASP  I-ASPOBJ  I-OTHOBJ     <sos>\n",
      "\n",
      "     <pad>   -9999.0   -9999.0   -9999.0   -9999.0   -9999.0   -9999.0   -9999.0   -9999.0   -9999.0\n",
      "         O   -9999.0      -1.0      -0.9      -1.1      -1.0      -1.1      -1.1      -1.0      -1.0\n",
      "  B-OTHOBJ   -9999.0      -1.0   -9999.0      -0.8      -1.1      -1.1   -9999.0   -9999.0      -1.1\n",
      "  B-ASPOBJ   -9999.0      -1.0      -1.0   -9999.0      -1.2      -0.9   -9999.0   -9999.0      -0.9\n",
      "     B-ASP   -9999.0      -1.0      -0.8      -1.1   -9999.0   -9999.0   -9999.0   -9999.0      -1.0\n",
      "     I-ASP   -9999.0   -9999.0   -9999.0      -1.1      -1.0      -0.9   -9999.0   -9999.0   -9999.0\n",
      "  I-ASPOBJ   -9999.0   -9999.0   -9999.0      -0.9   -9999.0   -9999.0   -9999.0   -9999.0   -9999.0\n",
      "  I-OTHOBJ   -9999.0   -9999.0      -0.9   -9999.0   -9999.0   -9999.0   -9999.0   -9999.0   -9999.0\n",
      "     <sos>   -9999.0   -9999.0   -9999.0   -9999.0   -9999.0   -9999.0   -9999.0   -9999.0   -9999.0\n",
      "\n",
      "Start training...\n",
      "\n",
      "\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 1/1 (0.00%).\n",
      "== eval epoch 0/50 \"f1-alpha-match-10\" train / dev / test | 4.90 / 4.60 / 12.08.\n",
      "## [BEST epoch], 16 seconds.\n",
      "\n",
      "-- train epoch 1/50, batch 261/261 (100.00%), loss = 482.29.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 1/1 (0.00%).\n",
      "== eval epoch 1/50 \"f1-alpha-match-10\" train / dev / test | 60.09 / 58.18 / 44.87.\n",
      "## [BEST epoch], 104 seconds.\n",
      "\n",
      "-- train epoch 2/50, batch 261/261 (100.00%), loss = 297.58.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 1/1 (0.00%).\n",
      "== eval epoch 2/50 \"f1-alpha-match-10\" train / dev / test | 63.26 / 58.87 / 42.55.\n",
      "## [BEST epoch], 103 seconds.\n",
      "\n",
      "-- train epoch 3/50, batch 261/261 (100.00%), loss = 271.47.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 1/1 (0.00%).\n",
      "== eval epoch 3/50 \"f1-alpha-match-10\" train / dev / test | 68.38 / 64.74 / 46.43.\n",
      "## [BEST epoch], 102 seconds.\n",
      "\n",
      "-- train epoch 4/50, batch 261/261 (100.00%), loss = 246.57.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 1/1 (0.00%).\n",
      "== eval epoch 4/50 \"f1-alpha-match-10\" train / dev / test | 70.43 / 64.01 / 34.43.\n",
      "## [no improvement micro-f1 on DEV during the last 1 epochs (best_f1_dev=64.74), 106 seconds].\n",
      "\n",
      "-- train epoch 5/50, batch 261/261 (100.00%), loss = 234.79.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 1/1 (0.00%).\n",
      "== eval epoch 5/50 \"f1-alpha-match-10\" train / dev / test | 72.03 / 67.81 / 45.49.\n",
      "## [BEST epoch], 101 seconds.\n",
      "\n",
      "-- train epoch 6/50, batch 261/261 (100.00%), loss = 197.57.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 1/1 (0.00%).\n",
      "== eval epoch 6/50 \"f1-alpha-match-10\" train / dev / test | 76.58 / 70.35 / 45.19.\n",
      "## [BEST epoch], 96 seconds.\n",
      "\n",
      "-- train epoch 7/50, batch 261/261 (100.00%), loss = 182.52.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 1/1 (0.00%).\n",
      "== eval epoch 7/50 \"f1-alpha-match-10\" train / dev / test | 77.64 / 70.21 / 45.28.\n",
      "## [no improvement micro-f1 on DEV during the last 1 epochs (best_f1_dev=70.35), 99 seconds].\n",
      "\n",
      "-- train epoch 8/50, batch 261/261 (100.00%), loss = 193.16.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 1/1 (0.00%).\n",
      "== eval epoch 8/50 \"f1-alpha-match-10\" train / dev / test | 79.05 / 72.93 / 50.97.\n",
      "## [BEST epoch], 98 seconds.\n",
      "\n",
      "-- train epoch 9/50, batch 261/261 (100.00%), loss = 183.23.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 1/1 (0.00%).\n",
      "== eval epoch 9/50 \"f1-alpha-match-10\" train / dev / test | 82.60 / 76.35 / 42.69.\n",
      "## [BEST epoch], 102 seconds.\n",
      "\n",
      "-- train epoch 10/50, batch 261/261 (100.00%), loss = 157.00.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 1/1 (0.00%).\n",
      "== eval epoch 10/50 \"f1-alpha-match-10\" train / dev / test | 81.92 / 74.99 / 42.97.\n",
      "## [no improvement micro-f1 on DEV during the last 1 epochs (best_f1_dev=76.35), 101 seconds].\n",
      "\n",
      "-- train epoch 11/50, batch 261/261 (100.00%), loss = 152.17.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 1/1 (0.00%).\n",
      "== eval epoch 11/50 \"f1-alpha-match-10\" train / dev / test | 83.08 / 76.47 / 42.80.\n",
      "## [BEST epoch], 106 seconds.\n",
      "\n",
      "-- train epoch 12/50, batch 261/261 (100.00%), loss = 127.66.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 1/1 (0.00%).\n",
      "== eval epoch 12/50 \"f1-alpha-match-10\" train / dev / test | 83.59 / 76.03 / 40.34.\n",
      "## [no improvement micro-f1 on DEV during the last 1 epochs (best_f1_dev=76.47), 102 seconds].\n",
      "\n",
      "-- train epoch 13/50, batch 261/261 (100.00%), loss = 132.64.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 1/1 (0.00%).\n",
      "== eval epoch 13/50 \"f1-alpha-match-10\" train / dev / test | 85.58 / 77.95 / 34.75.\n",
      "## [BEST epoch], 99 seconds.\n",
      "\n",
      "-- train epoch 14/50, batch 261/261 (100.00%), loss = 130.28.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 1/1 (0.00%).\n",
      "== eval epoch 14/50 \"f1-alpha-match-10\" train / dev / test | 84.71 / 78.42 / 48.19.\n",
      "## [BEST epoch], 104 seconds.\n",
      "\n",
      "-- train epoch 15/50, batch 261/261 (100.00%), loss = 120.01.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 1/1 (0.00%).\n",
      "== eval epoch 15/50 \"f1-alpha-match-10\" train / dev / test | 85.55 / 78.11 / 49.41.\n",
      "## [no improvement micro-f1 on DEV during the last 1 epochs (best_f1_dev=78.42), 103 seconds].\n",
      "\n",
      "-- train epoch 16/50, batch 261/261 (100.00%), loss = 134.78.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 1/1 (0.00%).\n",
      "== eval epoch 16/50 \"f1-alpha-match-10\" train / dev / test | 88.04 / 79.42 / 46.04.\n",
      "## [BEST epoch], 105 seconds.\n",
      "\n",
      "-- train epoch 17/50, batch 261/261 (100.00%), loss = 124.69.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 1/1 (0.00%).\n",
      "== eval epoch 17/50 \"f1-alpha-match-10\" train / dev / test | 88.31 / 78.95 / 42.19.\n",
      "## [no improvement micro-f1 on DEV during the last 1 epochs (best_f1_dev=79.42), 97 seconds].\n",
      "\n",
      "-- train epoch 18/50, batch 261/261 (100.00%), loss = 137.27.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 1/1 (0.00%).\n",
      "== eval epoch 18/50 \"f1-alpha-match-10\" train / dev / test | 88.30 / 80.43 / 38.06.\n",
      "## [BEST epoch], 101 seconds.\n",
      "\n",
      "-- train epoch 19/50, batch 261/261 (100.00%), loss = 88.62.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 1/1 (0.00%).\n",
      "== eval epoch 19/50 \"f1-alpha-match-10\" train / dev / test | 89.44 / 79.88 / 32.92.\n",
      "## [no improvement micro-f1 on DEV during the last 1 epochs (best_f1_dev=80.43), 102 seconds].\n",
      "\n",
      "-- train epoch 20/50, batch 261/261 (100.00%), loss = 85.18.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 1/1 (0.00%).\n",
      "== eval epoch 20/50 \"f1-alpha-match-10\" train / dev / test | 90.75 / 81.99 / 41.27.\n",
      "## [BEST epoch], 99 seconds.\n",
      "\n",
      "-- train epoch 21/50, batch 261/261 (100.00%), loss = 107.26.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 1/1 (0.00%).\n",
      "== eval epoch 21/50 \"f1-alpha-match-10\" train / dev / test | 89.94 / 80.27 / 34.78.\n",
      "## [no improvement micro-f1 on DEV during the last 1 epochs (best_f1_dev=81.99), 103 seconds].\n",
      "\n",
      "-- train epoch 22/50, batch 261/261 (100.00%), loss = 120.54.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 1/1 (0.00%).\n",
      "== eval epoch 22/50 \"f1-alpha-match-10\" train / dev / test | 90.99 / 81.55 / 39.66.\n",
      "## [no improvement micro-f1 on DEV during the last 2 epochs (best_f1_dev=81.99), 96 seconds].\n",
      "\n",
      "-- train epoch 23/50, batch 261/261 (100.00%), loss = 103.42.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 1/1 (0.00%).\n",
      "== eval epoch 23/50 \"f1-alpha-match-10\" train / dev / test | 91.88 / 81.28 / 44.36.\n",
      "## [no improvement micro-f1 on DEV during the last 3 epochs (best_f1_dev=81.99), 97 seconds].\n",
      "\n",
      "-- train epoch 24/50, batch 261/261 (100.00%), loss = 94.27.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 1/1 (0.00%).\n",
      "== eval epoch 24/50 \"f1-alpha-match-10\" train / dev / test | 91.23 / 79.44 / 44.26.\n",
      "## [no improvement micro-f1 on DEV during the last 4 epochs (best_f1_dev=81.99), 96 seconds].\n",
      "\n",
      "-- train epoch 25/50, batch 261/261 (100.00%), loss = 90.57.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 1/1 (0.00%).\n",
      "== eval epoch 25/50 \"f1-alpha-match-10\" train / dev / test | 92.44 / 80.88 / 48.00.\n",
      "## [no improvement micro-f1 on DEV during the last 5 epochs (best_f1_dev=81.99), 98 seconds].\n",
      "\n",
      "-- train epoch 26/50, batch 261/261 (100.00%), loss = 74.31.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 1/1 (0.00%).\n",
      "== eval epoch 26/50 \"f1-alpha-match-10\" train / dev / test | 92.19 / 80.35 / 45.45.\n",
      "## [no improvement micro-f1 on DEV during the last 6 epochs (best_f1_dev=81.99), 97 seconds].\n",
      "\n",
      "-- train epoch 27/50, batch 261/261 (100.00%), loss = 79.91.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 1/1 (0.00%).\n",
      "== eval epoch 27/50 \"f1-alpha-match-10\" train / dev / test | 92.21 / 80.28 / 47.88.\n",
      "## [no improvement micro-f1 on DEV during the last 7 epochs (best_f1_dev=81.99), 96 seconds].\n",
      "\n",
      "-- train epoch 28/50, batch 261/261 (100.00%), loss = 86.83.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 1/1 (0.00%).\n",
      "== eval epoch 28/50 \"f1-alpha-match-10\" train / dev / test | 93.30 / 81.06 / 43.33.\n",
      "## [no improvement micro-f1 on DEV during the last 8 epochs (best_f1_dev=81.99), 97 seconds].\n",
      "\n",
      "-- train epoch 29/50, batch 261/261 (100.00%), loss = 79.07.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 1/1 (0.00%).\n",
      "== eval epoch 29/50 \"f1-alpha-match-10\" train / dev / test | 92.80 / 81.12 / 43.65.\n",
      "## [no improvement micro-f1 on DEV during the last 9 epochs (best_f1_dev=81.99), 102 seconds].\n",
      "\n",
      "-- train epoch 30/50, batch 261/261 (100.00%), loss = 79.53.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 1/1 (0.00%).\n",
      "== eval epoch 30/50 \"f1-alpha-match-10\" train / dev / test | 93.12 / 81.97 / 45.45.\n",
      "## [no improvement micro-f1 on DEV during the last 10 epochs (best_f1_dev=81.99), 99 seconds].\n",
      "\n",
      "-- train epoch 31/50, batch 261/261 (100.00%), loss = 71.21.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 1/1 (0.00%).\n",
      "== eval epoch 31/50 \"f1-alpha-match-10\" train / dev / test | 94.07 / 82.15 / 41.43.\n",
      "## [BEST epoch], 101 seconds.\n",
      "\n",
      "-- train epoch 32/50, batch 261/261 (100.00%), loss = 47.15.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 1/1 (0.00%).\n",
      "== eval epoch 32/50 \"f1-alpha-match-10\" train / dev / test | 94.20 / 82.37 / 38.89.\n",
      "## [BEST epoch], 104 seconds.\n",
      "\n",
      "-- train epoch 33/50, batch 261/261 (100.00%), loss = 64.21.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 1/1 (0.00%).\n",
      "== eval epoch 33/50 \"f1-alpha-match-10\" train / dev / test | 93.03 / 81.21 / 47.19.\n",
      "## [no improvement micro-f1 on DEV during the last 1 epochs (best_f1_dev=82.37), 102 seconds].\n",
      "\n",
      "-- train epoch 34/50, batch 261/261 (100.00%), loss = 51.36.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 1/1 (0.00%).\n",
      "== eval epoch 34/50 \"f1-alpha-match-10\" train / dev / test | 94.08 / 81.68 / 44.44.\n",
      "## [no improvement micro-f1 on DEV during the last 2 epochs (best_f1_dev=82.37), 97 seconds].\n",
      "\n",
      "-- train epoch 35/50, batch 261/261 (100.00%), loss = 69.79.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 1/1 (0.00%).\n",
      "== eval epoch 35/50 \"f1-alpha-match-10\" train / dev / test | 94.54 / 81.42 / 46.64.\n",
      "## [no improvement micro-f1 on DEV during the last 3 epochs (best_f1_dev=82.37), 97 seconds].\n",
      "\n",
      "-- train epoch 36/50, batch 261/261 (100.00%), loss = 61.80.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 1/1 (0.00%).\n",
      "== eval epoch 36/50 \"f1-alpha-match-10\" train / dev / test | 94.93 / 80.98 / 46.88.\n",
      "## [no improvement micro-f1 on DEV during the last 4 epochs (best_f1_dev=82.37), 95 seconds].\n",
      "\n",
      "-- train epoch 37/50, batch 261/261 (100.00%), loss = 77.17.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 1/1 (0.00%).\n",
      "== eval epoch 37/50 \"f1-alpha-match-10\" train / dev / test | 94.04 / 81.85 / 43.92.\n",
      "## [no improvement micro-f1 on DEV during the last 5 epochs (best_f1_dev=82.37), 97 seconds].\n",
      "\n",
      "-- train epoch 38/50, batch 261/261 (100.00%), loss = 67.29.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 1/1 (0.00%).\n",
      "== eval epoch 38/50 \"f1-alpha-match-10\" train / dev / test | 94.54 / 81.54 / 41.60.\n",
      "## [no improvement micro-f1 on DEV during the last 6 epochs (best_f1_dev=82.37), 99 seconds].\n",
      "\n",
      "-- train epoch 39/50, batch 261/261 (100.00%), loss = 70.26.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 1/1 (0.00%).\n",
      "== eval epoch 39/50 \"f1-alpha-match-10\" train / dev / test | 94.75 / 82.11 / 41.32.\n",
      "## [no improvement micro-f1 on DEV during the last 7 epochs (best_f1_dev=82.37), 98 seconds].\n",
      "\n",
      "-- train epoch 40/50, batch 261/261 (100.00%), loss = 61.19.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 1/1 (0.00%).\n",
      "== eval epoch 40/50 \"f1-alpha-match-10\" train / dev / test | 95.10 / 81.77 / 38.40.\n",
      "## [no improvement micro-f1 on DEV during the last 8 epochs (best_f1_dev=82.37), 101 seconds].\n",
      "\n",
      "-- train epoch 41/50, batch 261/261 (100.00%), loss = 48.37.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 1/1 (0.00%).\n",
      "== eval epoch 41/50 \"f1-alpha-match-10\" train / dev / test | 95.26 / 81.73 / 38.91.\n",
      "## [no improvement micro-f1 on DEV during the last 9 epochs (best_f1_dev=82.37), 96 seconds].\n",
      "\n",
      "-- train epoch 42/50, batch 261/261 (100.00%), loss = 65.36.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 1/1 (0.00%).\n",
      "== eval epoch 42/50 \"f1-alpha-match-10\" train / dev / test | 95.11 / 81.01 / 37.45.\n",
      "## [no improvement micro-f1 on DEV during the last 10 epochs (best_f1_dev=82.37), 99 seconds].\n",
      "\n",
      "-- train epoch 43/50, batch 261/261 (100.00%), loss = 44.95.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 1/1 (0.00%).\n",
      "== eval epoch 43/50 \"f1-alpha-match-10\" train / dev / test | 95.16 / 82.13 / 37.94.\n",
      "## [no improvement micro-f1 on DEV during the last 11 epochs (best_f1_dev=82.37), 97 seconds].\n",
      "\n",
      "-- train epoch 44/50, batch 261/261 (100.00%), loss = 49.34.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 1/1 (0.00%).\n",
      "== eval epoch 44/50 \"f1-alpha-match-10\" train / dev / test | 95.17 / 81.70 / 42.47.\n",
      "## [no improvement micro-f1 on DEV during the last 12 epochs (best_f1_dev=82.37), 98 seconds].\n",
      "\n",
      "-- train epoch 45/50, batch 261/261 (100.00%), loss = 52.43.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 1/1 (0.00%).\n",
      "== eval epoch 45/50 \"f1-alpha-match-10\" train / dev / test | 95.61 / 81.37 / 34.24.\n",
      "## [no improvement micro-f1 on DEV during the last 13 epochs (best_f1_dev=82.37), 98 seconds].\n",
      "\n",
      "-- train epoch 46/50, batch 261/261 (100.00%), loss = 70.05.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 1/1 (0.00%).\n",
      "== eval epoch 46/50 \"f1-alpha-match-10\" train / dev / test | 95.10 / 81.02 / 41.86.\n",
      "## [no improvement micro-f1 on DEV during the last 14 epochs (best_f1_dev=82.37), 99 seconds].\n",
      "\n",
      "-- train epoch 47/50, batch 261/261 (100.00%), loss = 68.13.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 1/1 (0.00%).\n",
      "== eval epoch 47/50 \"f1-alpha-match-10\" train / dev / test | 95.54 / 82.24 / 36.58.\n",
      "## [no improvement micro-f1 on DEV during the last 15 epochs (best_f1_dev=82.37), 100 seconds].\n",
      "\n",
      "-- train epoch 48/50, batch 261/261 (100.00%), loss = 51.89.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 1/1 (0.00%).\n",
      "== eval epoch 48/50 \"f1-alpha-match-10\" train / dev / test | 95.67 / 80.87 / 41.70.\n",
      "## [no improvement micro-f1 on DEV during the last 16 epochs (best_f1_dev=82.37), 102 seconds].\n",
      "\n",
      "-- train epoch 49/50, batch 261/261 (100.00%), loss = 51.79.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 1/1 (0.00%).\n",
      "== eval epoch 49/50 \"f1-alpha-match-10\" train / dev / test | 95.84 / 81.43 / 41.30.\n",
      "## [no improvement micro-f1 on DEV during the last 17 epochs (best_f1_dev=82.37), 102 seconds].\n",
      "\n",
      "-- train epoch 50/50, batch 261/261 (100.00%), loss = 66.44.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 1/1 (0.00%).\n",
      "== eval epoch 50/50 \"f1-alpha-match-10\" train / dev / test | 95.83 / 81.55 / 46.74.\n",
      "## [no improvement micro-f1 on DEV during the last 18 epochs (best_f1_dev=82.37), 95 seconds].\n",
      "\n",
      "Evaluation\n",
      "\n",
      "batch_size=10\n",
      "char_cnn_filter_num=30\n",
      "char_embeddings_dim=25\n",
      "char_window_size=3\n",
      "check_for_lowercase=True\n",
      "clip_grad=5\n",
      "cross_fold_id=-1\n",
      "cross_folds_num=-1\n",
      "data_io='connl-ner-2003'\n",
      "dataset_sort=False\n",
      "dev='/home/vika/NER_RNN/targer/data/NER/Asqua_CAM_aspects/CAM_dev.csv'\n",
      "dropout_ratio=0.5\n",
      "elmo_options='/home/vika/NER_RNN/targer/embeddings/elmo_2x4096_512_2048cnn_2xhighway_5.5B_options.json'\n",
      "elmo_weights='/home/vika/NER_RNN/targer/embeddings/elmo_2x4096_512_2048cnn_2xhighway_5.5B_weights.hdf5'\n",
      "emb_delimiter=' '\n",
      "emb_dim=100\n",
      "emb_fn='embeddings/glove.6B.100d.txt'\n",
      "emb_load_all=False\n",
      "epoch_num=50\n",
      "evaluator='f1-alpha-match-10'\n",
      "freeze_char_embeddings=False\n",
      "freeze_word_embeddings=False\n",
      "gpu=1\n",
      "isElmo=True\n",
      "load=None\n",
      "lr=0.001\n",
      "lr_decay=0.05\n",
      "min_epoch_num=50\n",
      "model='BiRNNCNNCRF'\n",
      "momentum=0.9\n",
      "opt='adam'\n",
      "patience=20\n",
      "report_fn='2019_09_06_05-34_58_report.txt'\n",
      "rnn_hidden_dim=200\n",
      "rnn_type='LSTM'\n",
      "save='new_tagger1.hdf5'\n",
      "save_best=True\n",
      "seed_num=42\n",
      "test='/home/vika/NER_RNN/targer/data/NER/Asqua_CAM_aspects/CAM_qw_new.csv'\n",
      "train='/home/vika/NER_RNN/targer/data/NER/Asqua_CAM_aspects/CAM_train.csv'\n",
      "verbose=True\n",
      "word_len=20\n",
      "word_seq_indexer=None\n",
      "\n",
      "         epoch  |     train loss | f1-alpha-match-10-train | f1-alpha-match-10-dev | f1-alpha-match-10-test \n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "              0 |           0.00 |           4.90 |           4.60 |          12.08 \n",
      "              1 |         482.29 |          60.09 |          58.18 |          44.87 \n",
      "              2 |         297.58 |          63.26 |          58.87 |          42.55 \n",
      "              3 |         271.47 |          68.38 |          64.74 |          46.43 \n",
      "              4 |         246.57 |          70.43 |          64.01 |          34.43 \n",
      "              5 |         234.79 |          72.03 |          67.81 |          45.49 \n",
      "              6 |         197.57 |          76.58 |          70.35 |          45.19 \n",
      "              7 |         182.52 |          77.64 |          70.21 |          45.28 \n",
      "              8 |         193.16 |          79.05 |          72.93 |          50.97 \n",
      "              9 |         183.23 |          82.60 |          76.35 |          42.69 \n",
      "             10 |         157.00 |          81.92 |          74.99 |          42.97 \n",
      "             11 |         152.17 |          83.08 |          76.47 |          42.80 \n",
      "             12 |         127.66 |          83.59 |          76.03 |          40.34 \n",
      "             13 |         132.64 |          85.58 |          77.95 |          34.75 \n",
      "             14 |         130.28 |          84.71 |          78.42 |          48.19 \n",
      "             15 |         120.01 |          85.55 |          78.11 |          49.41 \n",
      "             16 |         134.78 |          88.04 |          79.42 |          46.04 \n",
      "             17 |         124.69 |          88.31 |          78.95 |          42.19 \n",
      "             18 |         137.27 |          88.30 |          80.43 |          38.06 \n",
      "             19 |          88.62 |          89.44 |          79.88 |          32.92 \n",
      "             20 |          85.18 |          90.75 |          81.99 |          41.27 \n",
      "             21 |         107.26 |          89.94 |          80.27 |          34.78 \n",
      "             22 |         120.54 |          90.99 |          81.55 |          39.66 \n",
      "             23 |         103.42 |          91.88 |          81.28 |          44.36 \n",
      "             24 |          94.27 |          91.23 |          79.44 |          44.26 \n",
      "             25 |          90.57 |          92.44 |          80.88 |          48.00 \n",
      "             26 |          74.31 |          92.19 |          80.35 |          45.45 \n",
      "             27 |          79.91 |          92.21 |          80.28 |          47.88 \n",
      "             28 |          86.83 |          93.30 |          81.06 |          43.33 \n",
      "             29 |          79.07 |          92.80 |          81.12 |          43.65 \n",
      "             30 |          79.53 |          93.12 |          81.97 |          45.45 \n",
      "             31 |          71.21 |          94.07 |          82.15 |          41.43 \n",
      "             32 |          47.15 |          94.20 |          82.37 |          38.89 \n",
      "             33 |          64.21 |          93.03 |          81.21 |          47.19 \n",
      "             34 |          51.36 |          94.08 |          81.68 |          44.44 \n",
      "             35 |          69.79 |          94.54 |          81.42 |          46.64 \n",
      "             36 |          61.80 |          94.93 |          80.98 |          46.88 \n",
      "             37 |          77.17 |          94.04 |          81.85 |          43.92 \n",
      "             38 |          67.29 |          94.54 |          81.54 |          41.60 \n",
      "             39 |          70.26 |          94.75 |          82.11 |          41.32 \n",
      "             40 |          61.19 |          95.10 |          81.77 |          38.40 \n",
      "             41 |          48.37 |          95.26 |          81.73 |          38.91 \n",
      "             42 |          65.36 |          95.11 |          81.01 |          37.45 \n",
      "             43 |          44.95 |          95.16 |          82.13 |          37.94 \n",
      "             44 |          49.34 |          95.17 |          81.70 |          42.47 \n",
      "             45 |          52.43 |          95.61 |          81.37 |          34.24 \n",
      "             46 |          70.05 |          95.10 |          81.02 |          41.86 \n",
      "             47 |          68.13 |          95.54 |          82.24 |          36.58 \n",
      "             48 |          51.89 |          95.67 |          80.87 |          41.70 \n",
      "             49 |          51.79 |          95.84 |          81.43 |          41.30 \n",
      "             50 |          66.44 |          95.83 |          81.55 |          46.74 \n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Final eval on test, \"save best\", best epoch on dev 32, f1-alpha-match-10, test = 38.89)\n",
      "--------------------------------------------------------------------------------------------------------------*** f1 alpha match, alpha = 1.0\n",
      "*** f1 = 38.89, precision = 51.58, recall = 31.21\n",
      "*** TP = 49, FP = 46, FN = 108\n",
      "Input arguments:\n",
      "python3 main.py --train /home/vika/NER_RNN/targer/data/NER/Asqua_CAM_aspects/CAM_train.csv --dev /home/vika/NER_RNN/targer/data/NER/Asqua_CAM_aspects/CAM_dev.csv --data-io connl-ner-2003 --evaluator f1-alpha-match-10 --opt adam --lr 0.001 --save-best yes --patience 20 --rnn-hidden-dim 200 --gpu 1 --test /home/vika/NER_RNN/targer/data/NER/Asqua_CAM_aspects/CAM_qw_new.csv --epoch-num 50 --isElmo True --save new_tagger1.hdf5\n",
      "\n",
      "38.8889\n"
     ]
    }
   ],
   "source": [
    "! python3 main.py --train \"/home/vika/NER_RNN/targer/data/NER/Asqua_CAM_aspects/CAM_train.csv\" --dev \"/home/vika/NER_RNN/targer/data/NER/Asqua_CAM_aspects/CAM_dev.csv\" --data-io connl-ner-2003 --evaluator f1-alpha-match-10 --opt adam --lr 0.001 --save-best yes --patience 20 --rnn-hidden-dim 200 --gpu 1 --test \"/home/vika/NER_RNN/targer/data/NER/Asqua_CAM_aspects/CAM_qw_new.csv\" --epoch-num 50 --isElmo True --save \"new_tagger1.hdf5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.modules.elmo import Elmo, batch_to_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/vika/NER_RNN/targer')\n",
    "from src.factories.factory_tagger import TaggerFactory\n",
    "\n",
    "model = TaggerFactory.load(PATH_TO_PRETRAINED + MODEL_NAME)\n",
    "model.cuda(device=1)\n",
    "model.gpu = 1\n",
    "tags = model.predict_tags_from_words(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "args train = /home/vika/NER_RNN/targer/data/NER/Asqua_CAM_aspects/CAM_train_new.csv\n",
      "Loading from /home/vika/NER_RNN/targer/data/NER/Asqua_CAM_aspects/CAM_train_new.csv: 2719 samples, 75117 words.\n",
      "Loading from /home/vika/NER_RNN/targer/data/NER/Asqua_CAM_aspects/CAM_dev.csv: 350 samples, 10092 words.\n",
      "Loading from /home/vika/NER_RNN/targer/data/NER/Asqua_CAM_aspects/CAM_test_new.csv: 584 samples, 15032 words.\n",
      "DatasetsBank: len(unique_words_list) = 7144 unique words.\n",
      "DatasetsBank: len(unique_words_list) = 7601 unique words.\n",
      "DatasetsBank: len(unique_words_list) = 8171 unique words.\n",
      "\n",
      "load_vocabulary_from_tag_sequences:\n",
      " -- class_num = 8\n",
      " -- {'<pad>': 0, 'tag': 1, 'O': 2, 'B-OTHOBJ': 3, 'B-ASPOBJ': 4, 'B-ASP': 5, 'I-ASP': 6, 'I-ASPOBJ': 7, 'I-OTHOBJ': 8}\n",
      "in main\n",
      "True\n",
      "/home/vika/NER_RNN/targer/embeddings/elmo_2x4096_512_2048cnn_2xhighway_5.5B_weights.hdf5\n",
      "/home/vika/NER_RNN/targer/embeddings/elmo_2x4096_512_2048cnn_2xhighway_5.5B_options.json\n",
      "init targer base\n",
      "True\n",
      "/home/vika/NER_RNN/targer/embeddings/elmo_2x4096_512_2048cnn_2xhighway_5.5B_weights.hdf5\n",
      "/home/vika/NER_RNN/targer/embeddings/elmo_2x4096_512_2048cnn_2xhighway_5.5B_options.json\n",
      "elmo is initiated\n",
      "init targer BiRNNCNNCRF\n",
      "True\n",
      "/home/vika/NER_RNN/targer/embeddings/elmo_2x4096_512_2048cnn_2xhighway_5.5B_weights.hdf5\n",
      "/home/vika/NER_RNN/targer/embeddings/elmo_2x4096_512_2048cnn_2xhighway_5.5B_options.json\n",
      "Empirical transition matrix from the train dataset:\n",
      "               <pad>       tag         O  B-OTHOBJ  B-ASPOBJ     B-ASP     I-ASP  I-ASPOBJ  I-OTHOBJ     <sos>\n",
      "\n",
      "     <pad>       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0\n",
      "       tag       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       1.0\n",
      "         O       0.0       1.0   58723.0    2722.0    2947.0     808.0     165.0      22.0      21.0    2307.0\n",
      "  B-OTHOBJ       0.0       0.0    2813.0       0.0       1.0       3.0       1.0       0.0       0.0      47.0\n",
      "  B-ASPOBJ       0.0       0.0    2624.0       8.0       0.0      22.0       3.0       0.0       0.0     359.0\n",
      "     B-ASP       0.0       0.0     937.0      14.0      44.0       0.0       0.0       0.0       0.0       5.0\n",
      "     I-ASP       0.0       0.0       0.0       0.0       2.0     167.0     307.0       0.0       0.0       0.0\n",
      "  I-ASPOBJ       0.0       0.0       0.0       0.0      22.0       0.0       0.0       0.0       0.0       0.0\n",
      "  I-OTHOBJ       0.0       0.0       0.0      21.0       0.0       0.0       0.0       0.0       0.0       0.0\n",
      "     <sos>       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0\n",
      "\n",
      "Initialized transition matrix:\n",
      "               <pad>       tag         O  B-OTHOBJ  B-ASPOBJ     B-ASP     I-ASP  I-ASPOBJ  I-OTHOBJ     <sos>\n",
      "\n",
      "     <pad>   -9999.0   -9999.0   -9999.0   -9999.0   -9999.0   -9999.0   -9999.0   -9999.0   -9999.0   -9999.0\n",
      "       tag   -9999.0   -9999.0   -9999.0   -9999.0   -9999.0   -9999.0   -9999.0   -9999.0   -9999.0      -0.9\n",
      "         O   -9999.0      -1.1      -1.1      -0.9      -0.9      -0.8      -0.9      -0.9      -1.0      -0.8\n",
      "  B-OTHOBJ   -9999.0   -9999.0      -1.0   -9999.0      -1.1      -1.1      -0.9   -9999.0   -9999.0      -1.2\n",
      "  B-ASPOBJ   -9999.0   -9999.0      -1.0      -0.9   -9999.0      -1.0      -0.9   -9999.0   -9999.0      -1.2\n",
      "     B-ASP   -9999.0   -9999.0      -0.9      -0.9      -1.0   -9999.0   -9999.0   -9999.0   -9999.0      -1.1\n",
      "     I-ASP   -9999.0   -9999.0   -9999.0   -9999.0      -1.2      -1.0      -1.1   -9999.0   -9999.0   -9999.0\n",
      "  I-ASPOBJ   -9999.0   -9999.0   -9999.0   -9999.0      -1.0   -9999.0   -9999.0   -9999.0   -9999.0   -9999.0\n",
      "  I-OTHOBJ   -9999.0   -9999.0   -9999.0      -0.9   -9999.0   -9999.0   -9999.0   -9999.0   -9999.0   -9999.0\n",
      "     <sos>   -9999.0   -9999.0   -9999.0   -9999.0   -9999.0   -9999.0   -9999.0   -9999.0   -9999.0   -9999.0\n",
      "\n",
      "Start training...\n",
      "\n",
      "\n",
      "\n",
      "++ predicting, batch 27/27 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 0/50 \"f1-alpha-match-10\" train / dev / test | 4.39 / 4.55 / 4.49.\n",
      "## [BEST epoch], 19 seconds.\n",
      "\n",
      "-- train epoch 1/50, batch 271/271 (100.00%), loss = 471.46.\n",
      "\n",
      "++ predicting, batch 27/27 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 1/50 \"f1-alpha-match-10\" train / dev / test | 59.43 / 56.88 / 60.68.\n",
      "## [BEST epoch], 111 seconds.\n",
      "\n",
      "-- train epoch 2/50, batch 271/271 (100.00%), loss = 311.63.\n",
      "\n",
      "++ predicting, batch 27/27 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 2/50 \"f1-alpha-match-10\" train / dev / test | 65.54 / 61.71 / 64.25.\n",
      "## [BEST epoch], 107 seconds.\n",
      "\n",
      "-- train epoch 3/50, batch 271/271 (100.00%), loss = 263.07.\n",
      "\n",
      "++ predicting, batch 27/27 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 3/50 \"f1-alpha-match-10\" train / dev / test | 65.42 / 60.86 / 63.47.\n",
      "## [no improvement micro-f1 on DEV during the last 1 epochs (best_f1_dev=61.71), 111 seconds].\n",
      "\n",
      "-- train epoch 4/50, batch 271/271 (100.00%), loss = 241.10.\n",
      "\n",
      "++ predicting, batch 27/27 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 4/50 \"f1-alpha-match-10\" train / dev / test | 72.40 / 64.60 / 68.90.\n",
      "## [BEST epoch], 112 seconds.\n",
      "\n",
      "-- train epoch 5/50, batch 271/271 (100.00%), loss = 226.24.\n",
      "\n",
      "++ predicting, batch 27/27 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 5/50 \"f1-alpha-match-10\" train / dev / test | 76.05 / 69.95 / 74.28.\n",
      "## [BEST epoch], 104 seconds.\n",
      "\n",
      "-- train epoch 6/50, batch 271/271 (100.00%), loss = 213.57.\n",
      "\n",
      "++ predicting, batch 27/27 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 6/50 \"f1-alpha-match-10\" train / dev / test | 77.31 / 70.24 / 74.36.\n",
      "## [BEST epoch], 111 seconds.\n",
      "\n",
      "-- train epoch 7/50, batch 271/271 (100.00%), loss = 169.04.\n",
      "\n",
      "++ predicting, batch 27/27 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 7/50 \"f1-alpha-match-10\" train / dev / test | 79.29 / 72.59 / 77.10.\n",
      "## [BEST epoch], 127 seconds.\n",
      "\n",
      "-- train epoch 8/50, batch 271/271 (100.00%), loss = 170.33.\n",
      "\n",
      "++ predicting, batch 27/27 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 8/50 \"f1-alpha-match-10\" train / dev / test | 77.94 / 71.17 / 73.04.\n",
      "## [no improvement micro-f1 on DEV during the last 1 epochs (best_f1_dev=72.59), 155 seconds].\n",
      "\n",
      "-- train epoch 9/50, batch 271/271 (100.00%), loss = 134.57.\n",
      "\n",
      "++ predicting, batch 27/27 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 9/50 \"f1-alpha-match-10\" train / dev / test | 82.77 / 74.56 / 76.31.\n",
      "## [BEST epoch], 153 seconds.\n",
      "\n",
      "-- train epoch 10/50, batch 271/271 (100.00%), loss = 179.24.\n",
      "\n",
      "++ predicting, batch 27/27 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 10/50 \"f1-alpha-match-10\" train / dev / test | 83.55 / 76.47 / 78.44.\n",
      "## [BEST epoch], 155 seconds.\n",
      "\n",
      "-- train epoch 11/50, batch 271/271 (100.00%), loss = 140.36.\n",
      "\n",
      "++ predicting, batch 27/27 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 11/50 \"f1-alpha-match-10\" train / dev / test | 86.31 / 77.43 / 80.45.\n",
      "## [BEST epoch], 160 seconds.\n",
      "\n",
      "-- train epoch 12/50, batch 271/271 (100.00%), loss = 147.24.\n",
      "\n",
      "++ predicting, batch 27/27 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 12/50 \"f1-alpha-match-10\" train / dev / test | 86.70 / 79.10 / 80.21.\n",
      "## [BEST epoch], 153 seconds.\n",
      "\n",
      "-- train epoch 13/50, batch 271/271 (100.00%), loss = 134.88.\n",
      "\n",
      "++ predicting, batch 27/27 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 13/50 \"f1-alpha-match-10\" train / dev / test | 86.60 / 77.57 / 79.79.\n",
      "## [no improvement micro-f1 on DEV during the last 1 epochs (best_f1_dev=79.10), 155 seconds].\n",
      "\n",
      "-- train epoch 14/50, batch 271/271 (100.00%), loss = 125.42.\n",
      "\n",
      "++ predicting, batch 27/27 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 14/50 \"f1-alpha-match-10\" train / dev / test | 87.39 / 76.45 / 81.97.\n",
      "## [no improvement micro-f1 on DEV during the last 2 epochs (best_f1_dev=79.10), 154 seconds].\n",
      "\n",
      "-- train epoch 15/50, batch 271/271 (100.00%), loss = 110.34.\n",
      "\n",
      "++ predicting, batch 27/27 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 15/50 \"f1-alpha-match-10\" train / dev / test | 87.51 / 76.90 / 80.79.\n",
      "## [no improvement micro-f1 on DEV during the last 3 epochs (best_f1_dev=79.10), 155 seconds].\n",
      "\n",
      "-- train epoch 16/50, batch 271/271 (100.00%), loss = 114.84.\n",
      "\n",
      "++ predicting, batch 27/27 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 16/50 \"f1-alpha-match-10\" train / dev / test | 88.69 / 78.76 / 81.54.\n",
      "## [no improvement micro-f1 on DEV during the last 4 epochs (best_f1_dev=79.10), 153 seconds].\n",
      "\n",
      "-- train epoch 17/50, batch 271/271 (100.00%), loss = 123.46.\n",
      "\n",
      "++ predicting, batch 27/27 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 17/50 \"f1-alpha-match-10\" train / dev / test | 89.21 / 77.54 / 81.22.\n",
      "## [no improvement micro-f1 on DEV during the last 5 epochs (best_f1_dev=79.10), 155 seconds].\n",
      "\n",
      "-- train epoch 18/50, batch 271/271 (100.00%), loss = 105.34.\n",
      "\n",
      "++ predicting, batch 27/27 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 18/50 \"f1-alpha-match-10\" train / dev / test | 90.26 / 78.83 / 82.23.\n",
      "## [no improvement micro-f1 on DEV during the last 6 epochs (best_f1_dev=79.10), 152 seconds].\n",
      "\n",
      "-- train epoch 19/50, batch 271/271 (100.00%), loss = 94.80.\n",
      "\n",
      "++ predicting, batch 27/27 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 19/50 \"f1-alpha-match-10\" train / dev / test | 90.78 / 79.01 / 82.49.\n",
      "## [no improvement micro-f1 on DEV during the last 7 epochs (best_f1_dev=79.10), 154 seconds].\n",
      "\n",
      "-- train epoch 20/50, batch 271/271 (100.00%), loss = 100.66.\n",
      "\n",
      "++ predicting, batch 27/27 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 20/50 \"f1-alpha-match-10\" train / dev / test | 91.18 / 79.14 / 82.98.\n",
      "## [BEST epoch], 159 seconds.\n",
      "\n",
      "-- train epoch 21/50, batch 271/271 (100.00%), loss = 103.85.\n",
      "\n",
      "++ predicting, batch 27/27 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 21/50 \"f1-alpha-match-10\" train / dev / test | 91.58 / 79.06 / 82.96.\n",
      "## [no improvement micro-f1 on DEV during the last 1 epochs (best_f1_dev=79.14), 159 seconds].\n",
      "\n",
      "-- train epoch 22/50, batch 271/271 (100.00%), loss = 94.58.\n",
      "\n",
      "++ predicting, batch 27/27 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 22/50 \"f1-alpha-match-10\" train / dev / test | 92.08 / 80.40 / 82.67.\n",
      "## [BEST epoch], 159 seconds.\n",
      "\n",
      "-- train epoch 23/50, batch 271/271 (100.00%), loss = 79.92.\n",
      "\n",
      "++ predicting, batch 27/27 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 23/50 \"f1-alpha-match-10\" train / dev / test | 92.18 / 80.04 / 82.04.\n",
      "## [no improvement micro-f1 on DEV during the last 1 epochs (best_f1_dev=80.40), 156 seconds].\n",
      "\n",
      "-- train epoch 24/50, batch 271/271 (100.00%), loss = 86.33.\n",
      "\n",
      "++ predicting, batch 27/27 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 24/50 \"f1-alpha-match-10\" train / dev / test | 93.03 / 80.11 / 82.47.\n",
      "## [no improvement micro-f1 on DEV during the last 2 epochs (best_f1_dev=80.40), 151 seconds].\n",
      "\n",
      "-- train epoch 25/50, batch 271/271 (100.00%), loss = 74.16.\n",
      "\n",
      "++ predicting, batch 27/27 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 25/50 \"f1-alpha-match-10\" train / dev / test | 92.42 / 79.60 / 82.39.\n",
      "## [no improvement micro-f1 on DEV during the last 3 epochs (best_f1_dev=80.40), 153 seconds].\n",
      "\n",
      "-- train epoch 26/50, batch 271/271 (100.00%), loss = 71.81.\n",
      "\n",
      "++ predicting, batch 27/27 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 26/50 \"f1-alpha-match-10\" train / dev / test | 92.62 / 80.86 / 82.83.\n",
      "## [BEST epoch], 156 seconds.\n",
      "\n",
      "-- train epoch 27/50, batch 271/271 (100.00%), loss = 59.15.\n",
      "\n",
      "++ predicting, batch 27/27 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 27/50 \"f1-alpha-match-10\" train / dev / test | 93.20 / 81.53 / 83.38.\n",
      "## [BEST epoch], 125 seconds.\n",
      "\n",
      "-- train epoch 28/50, batch 271/271 (100.00%), loss = 68.64.\n",
      "\n",
      "++ predicting, batch 27/27 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 28/50 \"f1-alpha-match-10\" train / dev / test | 93.54 / 80.92 / 83.33.\n",
      "## [no improvement micro-f1 on DEV during the last 1 epochs (best_f1_dev=81.53), 113 seconds].\n",
      "\n",
      "-- train epoch 29/50, batch 271/271 (100.00%), loss = 75.70.\n",
      "\n",
      "++ predicting, batch 27/27 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 29/50 \"f1-alpha-match-10\" train / dev / test | 94.03 / 79.51 / 83.45.\n",
      "## [no improvement micro-f1 on DEV during the last 2 epochs (best_f1_dev=81.53), 145 seconds].\n",
      "\n",
      "-- train epoch 30/50, batch 271/271 (100.00%), loss = 74.57.\n",
      "\n",
      "++ predicting, batch 27/27 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 30/50 \"f1-alpha-match-10\" train / dev / test | 93.99 / 80.29 / 83.44.\n",
      "## [no improvement micro-f1 on DEV during the last 3 epochs (best_f1_dev=81.53), 154 seconds].\n",
      "\n",
      "-- train epoch 31/50, batch 271/271 (100.00%), loss = 69.73.\n",
      "\n",
      "++ predicting, batch 27/27 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 31/50 \"f1-alpha-match-10\" train / dev / test | 94.17 / 79.95 / 83.26.\n",
      "## [no improvement micro-f1 on DEV during the last 4 epochs (best_f1_dev=81.53), 155 seconds].\n",
      "\n",
      "-- train epoch 32/50, batch 271/271 (100.00%), loss = 49.83.\n",
      "\n",
      "++ predicting, batch 27/27 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 32/50 \"f1-alpha-match-10\" train / dev / test | 94.59 / 79.64 / 83.37.\n",
      "## [no improvement micro-f1 on DEV during the last 5 epochs (best_f1_dev=81.53), 155 seconds].\n",
      "\n",
      "-- train epoch 33/50, batch 271/271 (100.00%), loss = 62.78.\n",
      "\n",
      "++ predicting, batch 27/27 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 33/50 \"f1-alpha-match-10\" train / dev / test | 94.39 / 81.25 / 83.60.\n",
      "## [no improvement micro-f1 on DEV during the last 6 epochs (best_f1_dev=81.53), 154 seconds].\n",
      "\n",
      "-- train epoch 34/50, batch 271/271 (100.00%), loss = 66.57.\n",
      "\n",
      "++ predicting, batch 27/27 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 34/50 \"f1-alpha-match-10\" train / dev / test | 94.53 / 80.56 / 84.14.\n",
      "## [no improvement micro-f1 on DEV during the last 7 epochs (best_f1_dev=81.53), 154 seconds].\n",
      "\n",
      "-- train epoch 35/50, batch 271/271 (100.00%), loss = 66.50.\n",
      "\n",
      "++ predicting, batch 27/27 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 35/50 \"f1-alpha-match-10\" train / dev / test | 95.07 / 81.85 / 84.08.\n",
      "## [BEST epoch], 154 seconds.\n",
      "\n",
      "-- train epoch 36/50, batch 271/271 (100.00%), loss = 58.16.\n",
      "\n",
      "++ predicting, batch 27/27 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 36/50 \"f1-alpha-match-10\" train / dev / test | 95.00 / 82.00 / 83.44.\n",
      "## [BEST epoch], 155 seconds.\n",
      "\n",
      "-- train epoch 37/50, batch 271/271 (100.00%), loss = 61.50.\n",
      "\n",
      "++ predicting, batch 27/27 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 37/50 \"f1-alpha-match-10\" train / dev / test | 95.23 / 81.94 / 83.28.\n",
      "## [no improvement micro-f1 on DEV during the last 1 epochs (best_f1_dev=82.00), 154 seconds].\n",
      "\n",
      "-- train epoch 38/50, batch 271/271 (100.00%), loss = 52.78.\n",
      "\n",
      "++ predicting, batch 27/27 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 38/50 \"f1-alpha-match-10\" train / dev / test | 95.31 / 81.44 / 84.11.\n",
      "## [no improvement micro-f1 on DEV during the last 2 epochs (best_f1_dev=82.00), 150 seconds].\n",
      "\n",
      "-- train epoch 39/50, batch 271/271 (100.00%), loss = 60.88.\n",
      "\n",
      "++ predicting, batch 27/27 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 39/50 \"f1-alpha-match-10\" train / dev / test | 95.74 / 81.48 / 83.83.\n",
      "## [no improvement micro-f1 on DEV during the last 3 epochs (best_f1_dev=82.00), 153 seconds].\n",
      "\n",
      "-- train epoch 40/50, batch 271/271 (100.00%), loss = 62.90.\n",
      "\n",
      "++ predicting, batch 27/27 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 40/50 \"f1-alpha-match-10\" train / dev / test | 95.62 / 81.39 / 83.59.\n",
      "## [no improvement micro-f1 on DEV during the last 4 epochs (best_f1_dev=82.00), 153 seconds].\n",
      "\n",
      "-- train epoch 41/50, batch 271/271 (100.00%), loss = 60.31.\n",
      "\n",
      "++ predicting, batch 27/27 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 41/50 \"f1-alpha-match-10\" train / dev / test | 95.92 / 80.92 / 83.96.\n",
      "## [no improvement micro-f1 on DEV during the last 5 epochs (best_f1_dev=82.00), 153 seconds].\n",
      "\n",
      "-- train epoch 42/50, batch 271/271 (100.00%), loss = 38.39.\n",
      "\n",
      "++ predicting, batch 27/27 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 42/50 \"f1-alpha-match-10\" train / dev / test | 95.92 / 82.22 / 84.12.\n",
      "## [BEST epoch], 154 seconds.\n",
      "\n",
      "-- train epoch 43/50, batch 271/271 (100.00%), loss = 50.95.\n",
      "\n",
      "++ predicting, batch 27/27 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 43/50 \"f1-alpha-match-10\" train / dev / test | 95.46 / 80.78 / 83.16.\n",
      "## [no improvement micro-f1 on DEV during the last 1 epochs (best_f1_dev=82.22), 152 seconds].\n",
      "\n",
      "-- train epoch 44/50, batch 271/271 (100.00%), loss = 52.35.\n",
      "\n",
      "++ predicting, batch 27/27 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 44/50 \"f1-alpha-match-10\" train / dev / test | 95.78 / 81.72 / 83.66.\n",
      "## [no improvement micro-f1 on DEV during the last 2 epochs (best_f1_dev=82.22), 158 seconds].\n",
      "\n",
      "-- train epoch 45/50, batch 271/271 (100.00%), loss = 71.44.\n",
      "\n",
      "++ predicting, batch 27/27 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 45/50 \"f1-alpha-match-10\" train / dev / test | 95.57 / 81.92 / 83.27.\n",
      "## [no improvement micro-f1 on DEV during the last 3 epochs (best_f1_dev=82.22), 152 seconds].\n",
      "\n",
      "-- train epoch 46/50, batch 271/271 (100.00%), loss = 39.98.\n",
      "\n",
      "++ predicting, batch 27/27 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 46/50 \"f1-alpha-match-10\" train / dev / test | 95.96 / 81.67 / 83.85.\n",
      "## [no improvement micro-f1 on DEV during the last 4 epochs (best_f1_dev=82.22), 153 seconds].\n",
      "\n",
      "-- train epoch 47/50, batch 271/271 (100.00%), loss = 45.98.\n",
      "\n",
      "++ predicting, batch 27/27 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 47/50 \"f1-alpha-match-10\" train / dev / test | 96.16 / 81.73 / 83.41.\n",
      "## [no improvement micro-f1 on DEV during the last 5 epochs (best_f1_dev=82.22), 153 seconds].\n",
      "\n",
      "-- train epoch 48/50, batch 271/271 (100.00%), loss = 53.98.\n",
      "\n",
      "++ predicting, batch 27/27 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 48/50 \"f1-alpha-match-10\" train / dev / test | 96.17 / 81.69 / 83.71.\n",
      "## [no improvement micro-f1 on DEV during the last 6 epochs (best_f1_dev=82.22), 152 seconds].\n",
      "\n",
      "-- train epoch 49/50, batch 271/271 (100.00%), loss = 51.61.\n",
      "\n",
      "++ predicting, batch 27/27 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 49/50 \"f1-alpha-match-10\" train / dev / test | 95.92 / 80.31 / 83.83.\n",
      "## [no improvement micro-f1 on DEV during the last 7 epochs (best_f1_dev=82.22), 155 seconds].\n",
      "\n",
      "-- train epoch 50/50, batch 271/271 (100.00%), loss = 43.00.\n",
      "\n",
      "++ predicting, batch 27/27 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 50/50 \"f1-alpha-match-10\" train / dev / test | 96.08 / 81.34 / 84.22.\n",
      "## [no improvement micro-f1 on DEV during the last 8 epochs (best_f1_dev=82.22), 147 seconds].\n",
      "\n",
      "Evaluation\n",
      "\n",
      "batch_size=10\n",
      "char_cnn_filter_num=30\n",
      "char_embeddings_dim=25\n",
      "char_window_size=3\n",
      "check_for_lowercase=True\n",
      "clip_grad=5\n",
      "cross_fold_id=-1\n",
      "cross_folds_num=-1\n",
      "data_io='connl-ner-2003'\n",
      "dataset_sort=False\n",
      "dev='/home/vika/NER_RNN/targer/data/NER/Asqua_CAM_aspects/CAM_dev.csv'\n",
      "dropout_ratio=0.5\n",
      "elmo_options='/home/vika/NER_RNN/targer/embeddings/elmo_2x4096_512_2048cnn_2xhighway_5.5B_options.json'\n",
      "elmo_weights='/home/vika/NER_RNN/targer/embeddings/elmo_2x4096_512_2048cnn_2xhighway_5.5B_weights.hdf5'\n",
      "emb_delimiter=' '\n",
      "emb_dim=100\n",
      "emb_fn='embeddings/glove.6B.100d.txt'\n",
      "emb_load_all=False\n",
      "epoch_num=50\n",
      "evaluator='f1-alpha-match-10'\n",
      "freeze_char_embeddings=False\n",
      "freeze_word_embeddings=False\n",
      "gpu=1\n",
      "isElmo=True\n",
      "load=None\n",
      "lr=0.001\n",
      "lr_decay=0.05\n",
      "min_epoch_num=50\n",
      "model='BiRNNCNNCRF'\n",
      "momentum=0.9\n",
      "opt='adam'\n",
      "patience=20\n",
      "report_fn='2019_09_16_17-51_34_report.txt'\n",
      "rnn_hidden_dim=200\n",
      "rnn_type='LSTM'\n",
      "save='new_tagger1.hdf5'\n",
      "save_best=True\n",
      "seed_num=42\n",
      "test='/home/vika/NER_RNN/targer/data/NER/Asqua_CAM_aspects/CAM_test_new.csv'\n",
      "train='/home/vika/NER_RNN/targer/data/NER/Asqua_CAM_aspects/CAM_train_new.csv'\n",
      "verbose=True\n",
      "word_len=20\n",
      "word_seq_indexer=None\n",
      "\n",
      "         epoch  |     train loss | f1-alpha-match-10-train | f1-alpha-match-10-dev | f1-alpha-match-10-test \n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "              0 |           0.00 |           4.39 |           4.55 |           4.49 \n",
      "              1 |         471.46 |          59.43 |          56.88 |          60.68 \n",
      "              2 |         311.63 |          65.54 |          61.71 |          64.25 \n",
      "              3 |         263.07 |          65.42 |          60.86 |          63.47 \n",
      "              4 |         241.10 |          72.40 |          64.60 |          68.90 \n",
      "              5 |         226.24 |          76.05 |          69.95 |          74.28 \n",
      "              6 |         213.57 |          77.31 |          70.24 |          74.36 \n",
      "              7 |         169.04 |          79.29 |          72.59 |          77.10 \n",
      "              8 |         170.33 |          77.94 |          71.17 |          73.04 \n",
      "              9 |         134.57 |          82.77 |          74.56 |          76.31 \n",
      "             10 |         179.24 |          83.55 |          76.47 |          78.44 \n",
      "             11 |         140.36 |          86.31 |          77.43 |          80.45 \n",
      "             12 |         147.24 |          86.70 |          79.10 |          80.21 \n",
      "             13 |         134.88 |          86.60 |          77.57 |          79.79 \n",
      "             14 |         125.42 |          87.39 |          76.45 |          81.97 \n",
      "             15 |         110.34 |          87.51 |          76.90 |          80.79 \n",
      "             16 |         114.84 |          88.69 |          78.76 |          81.54 \n",
      "             17 |         123.46 |          89.21 |          77.54 |          81.22 \n",
      "             18 |         105.34 |          90.26 |          78.83 |          82.23 \n",
      "             19 |          94.80 |          90.78 |          79.01 |          82.49 \n",
      "             20 |         100.66 |          91.18 |          79.14 |          82.98 \n",
      "             21 |         103.85 |          91.58 |          79.06 |          82.96 \n",
      "             22 |          94.58 |          92.08 |          80.40 |          82.67 \n",
      "             23 |          79.92 |          92.18 |          80.04 |          82.04 \n",
      "             24 |          86.33 |          93.03 |          80.11 |          82.47 \n",
      "             25 |          74.16 |          92.42 |          79.60 |          82.39 \n",
      "             26 |          71.81 |          92.62 |          80.86 |          82.83 \n",
      "             27 |          59.15 |          93.20 |          81.53 |          83.38 \n",
      "             28 |          68.64 |          93.54 |          80.92 |          83.33 \n",
      "             29 |          75.70 |          94.03 |          79.51 |          83.45 \n",
      "             30 |          74.57 |          93.99 |          80.29 |          83.44 \n",
      "             31 |          69.73 |          94.17 |          79.95 |          83.26 \n",
      "             32 |          49.83 |          94.59 |          79.64 |          83.37 \n",
      "             33 |          62.78 |          94.39 |          81.25 |          83.60 \n",
      "             34 |          66.57 |          94.53 |          80.56 |          84.14 \n",
      "             35 |          66.50 |          95.07 |          81.85 |          84.08 \n",
      "             36 |          58.16 |          95.00 |          82.00 |          83.44 \n",
      "             37 |          61.50 |          95.23 |          81.94 |          83.28 \n",
      "             38 |          52.78 |          95.31 |          81.44 |          84.11 \n",
      "             39 |          60.88 |          95.74 |          81.48 |          83.83 \n",
      "             40 |          62.90 |          95.62 |          81.39 |          83.59 \n",
      "             41 |          60.31 |          95.92 |          80.92 |          83.96 \n",
      "             42 |          38.39 |          95.92 |          82.22 |          84.12 \n",
      "             43 |          50.95 |          95.46 |          80.78 |          83.16 \n",
      "             44 |          52.35 |          95.78 |          81.72 |          83.66 \n",
      "             45 |          71.44 |          95.57 |          81.92 |          83.27 \n",
      "             46 |          39.98 |          95.96 |          81.67 |          83.85 \n",
      "             47 |          45.98 |          96.16 |          81.73 |          83.41 \n",
      "             48 |          53.98 |          96.17 |          81.69 |          83.71 \n",
      "             49 |          51.61 |          95.92 |          80.31 |          83.83 \n",
      "             50 |          43.00 |          96.08 |          81.34 |          84.22 \n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Final eval on test, \"save best\", best epoch on dev 42, f1-alpha-match-10, test = 84.12)\n",
      "--------------------------------------------------------------------------------------------------------------*** f1 alpha match, alpha = 1.0\n",
      "*** f1 = 84.12, precision = 86.43, recall = 81.94\n",
      "*** TP = 1216, FP = 191, FN = 268\n",
      "Input arguments:\n",
      "python3 main.py --train /home/vika/NER_RNN/targer/data/NER/Asqua_CAM_aspects/CAM_train_new.csv --dev /home/vika/NER_RNN/targer/data/NER/Asqua_CAM_aspects/CAM_dev.csv --data-io connl-ner-2003 --evaluator f1-alpha-match-10 --opt adam --lr 0.001 --save-best yes --patience 20 --rnn-hidden-dim 200 --gpu 1 --test /home/vika/NER_RNN/targer/data/NER/Asqua_CAM_aspects/CAM_test_new.csv --epoch-num 50 --isElmo True --save new_tagger1.hdf5\n",
      "\n",
      "84.1231\n"
     ]
    }
   ],
   "source": [
    "! python3 main.py --train \"/home/vika/NER_RNN/targer/data/NER/Asqua_CAM_aspects/CAM_train_new.csv\" --dev \"/home/vika/NER_RNN/targer/data/NER/Asqua_CAM_aspects/CAM_dev.csv\" --data-io connl-ner-2003 --evaluator f1-alpha-match-10 --opt adam --lr 0.001 --save-best yes --patience 20 --rnn-hidden-dim 200 --gpu 1 --test \"/home/vika/NER_RNN/targer/data/NER/Asqua_CAM_aspects/CAM_test_new.csv\" --epoch-num 50 --isElmo True --save \"new_tagger1.hdf5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/vika/NER_RNN/targer\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}