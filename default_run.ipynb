{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "args train = /home/vika/targer/data/NER/Indian_dataset/train.csv\n",
      "Loading from /home/vika/targer/data/NER/Indian_dataset/train.csv: 23999 samples, 448609 words.\n",
      "Loading from /home/vika/targer/data/NER/Indian_dataset/dev.csv: 1803 samples, 33827 words.\n",
      "Loading from /home/vika/targer/data/NER/Indian_dataset/test.csv: 1093 samples, 18921 words.\n",
      "DatasetsBank: len(unique_words_list) = 16615 unique words.\n",
      "DatasetsBank: len(unique_words_list) = 17208 unique words.\n",
      "DatasetsBank: len(unique_words_list) = 17829 unique words.\n",
      "qu False\n",
      "3\n",
      "Reading embeddings file embeddings/glove.6B.100d.txt, line = 0\n",
      "Reading embeddings file embeddings/glove.6B.100d.txt, line = 25000\n",
      "Reading embeddings file embeddings/glove.6B.100d.txt, line = 50000\n",
      "Reading embeddings file embeddings/glove.6B.100d.txt, line = 75000\n",
      "Reading embeddings file embeddings/glove.6B.100d.txt, line = 100000\n",
      "Reading embeddings file embeddings/glove.6B.100d.txt, line = 125000\n",
      "Reading embeddings file embeddings/glove.6B.100d.txt, line = 150000\n",
      "Reading embeddings file embeddings/glove.6B.100d.txt, line = 175000\n",
      "Reading embeddings file embeddings/glove.6B.100d.txt, line = 200000\n",
      "Reading embeddings file embeddings/glove.6B.100d.txt, line = 225000\n",
      "Reading embeddings file embeddings/glove.6B.100d.txt, line = 250000\n",
      "Reading embeddings file embeddings/glove.6B.100d.txt, line = 275000\n",
      "Reading embeddings file embeddings/glove.6B.100d.txt, line = 300000\n",
      "Reading embeddings file embeddings/glove.6B.100d.txt, line = 325000\n",
      "Reading embeddings file embeddings/glove.6B.100d.txt, line = 350000\n",
      "Reading embeddings file embeddings/glove.6B.100d.txt, line = 375000\n",
      "Reading embeddings file embeddings/glove.6B.100d.txt, line = 0\n",
      "Reading embeddings file embeddings/glove.6B.100d.txt, line = 25000\n",
      "Reading embeddings file embeddings/glove.6B.100d.txt, line = 50000\n",
      "Reading embeddings file embeddings/glove.6B.100d.txt, line = 75000\n",
      "Reading embeddings file embeddings/glove.6B.100d.txt, line = 100000\n",
      "Reading embeddings file embeddings/glove.6B.100d.txt, line = 125000\n",
      "Reading embeddings file embeddings/glove.6B.100d.txt, line = 150000\n",
      "Reading embeddings file embeddings/glove.6B.100d.txt, line = 175000\n",
      "Reading embeddings file embeddings/glove.6B.100d.txt, line = 200000\n",
      "Reading embeddings file embeddings/glove.6B.100d.txt, line = 225000\n",
      "Reading embeddings file embeddings/glove.6B.100d.txt, line = 250000\n",
      "Reading embeddings file embeddings/glove.6B.100d.txt, line = 275000\n",
      "Reading embeddings file embeddings/glove.6B.100d.txt, line = 300000\n",
      "Reading embeddings file embeddings/glove.6B.100d.txt, line = 325000\n",
      "Reading embeddings file embeddings/glove.6B.100d.txt, line = 350000\n",
      "Reading embeddings file embeddings/glove.6B.100d.txt, line = 375000\n",
      "\n",
      "load_vocabulary_from_embeddings_file_and_unique_words_list:\n",
      "    First 50 OOV words:\n",
      "        out_of_vocabulary_words_list[0] = b'htib'\n",
      "        out_of_vocabulary_words_list[1] = b\"doesn't\"\n",
      "        out_of_vocabulary_words_list[2] = b'olevia'\n",
      "        out_of_vocabulary_words_list[3] = b\"it's\"\n",
      "        out_of_vocabulary_words_list[4] = b\"can't\"\n",
      "        out_of_vocabulary_words_list[5] = b'b+w'\n",
      "        out_of_vocabulary_words_list[6] = b'd2x'\n",
      "        out_of_vocabulary_words_list[7] = b\"that's\"\n",
      "        out_of_vocabulary_words_list[8] = b\"sandisk's\"\n",
      "        out_of_vocabulary_words_list[9] = b'mini-cruzer'\n",
      "        out_of_vocabulary_words_list[10] = b'irispen'\n",
      "        out_of_vocabulary_words_list[11] = b'2mp'\n",
      "        out_of_vocabulary_words_list[12] = b'6mp'\n",
      "        out_of_vocabulary_words_list[13] = b\"knockoff's\"\n",
      "        out_of_vocabulary_words_list[14] = b\"i'll\"\n",
      "        out_of_vocabulary_words_list[15] = b\"i'm\"\n",
      "        out_of_vocabulary_words_list[16] = b\"i've\"\n",
      "        out_of_vocabulary_words_list[17] = b'rewinder'\n",
      "        out_of_vocabulary_words_list[18] = b'positve'\n",
      "        out_of_vocabulary_words_list[19] = b'$375'\n",
      "        out_of_vocabulary_words_list[20] = b'1850mah'\n",
      "        out_of_vocabulary_words_list[21] = b\"1700's\"\n",
      "        out_of_vocabulary_words_list[22] = b\"1850's\"\n",
      "        out_of_vocabulary_words_list[23] = b'better.'\n",
      "        out_of_vocabulary_words_list[24] = b'pentax-kx'\n",
      "        out_of_vocabulary_words_list[25] = b\"haven't\"\n",
      "        out_of_vocabulary_words_list[26] = b\"zune's\"\n",
      "        out_of_vocabulary_words_list[27] = b'e550'\n",
      "        out_of_vocabulary_words_list[28] = b'macally'\n",
      "        out_of_vocabulary_words_list[29] = b'mac-made'\n",
      "        out_of_vocabulary_words_list[30] = b'product.'\n",
      "        out_of_vocabulary_words_list[31] = b\"sony's\"\n",
      "        out_of_vocabulary_words_list[32] = b'dynax'\n",
      "        out_of_vocabulary_words_list[33] = b'applie'\n",
      "        out_of_vocabulary_words_list[34] = b\"there's\"\n",
      "        out_of_vocabulary_words_list[35] = b'okidata'\n",
      "        out_of_vocabulary_words_list[36] = b'convienence'\n",
      "        out_of_vocabulary_words_list[37] = b'sd550'\n",
      "        out_of_vocabulary_words_list[38] = b'maxells'\n",
      "        out_of_vocabulary_words_list[39] = b'30d'\n",
      "        out_of_vocabulary_words_list[40] = b\"logitech's\"\n",
      "        out_of_vocabulary_words_list[41] = b'x-530now'\n",
      "        out_of_vocabulary_words_list[42] = b'z-5500'\n",
      "        out_of_vocabulary_words_list[43] = b'mdr-v250s'\n",
      "        out_of_vocabulary_words_list[44] = b'isint'\n",
      "        out_of_vocabulary_words_list[45] = b\"don't\"\n",
      "        out_of_vocabulary_words_list[46] = b'sd600'\n",
      "        out_of_vocabulary_words_list[47] = b'reccomend'\n",
      "        out_of_vocabulary_words_list[48] = b'exed'\n",
      "        out_of_vocabulary_words_list[49] = b'lenmar'\n",
      "        out_of_vocabulary_words_list[50] = b'rs120'\n",
      " -- len(out_of_vocabulary_words_list) = 6064\n",
      " -- original_words_num = 11708\n",
      " -- lowercase_words_num = 0\n",
      " -- zero_digits_replaced_num = 57\n",
      " -- zero_digits_replaced_lowercase_num = 0\n",
      "\n",
      "load_vocabulary_from_tag_sequences:\n",
      " -- class_num = 5\n",
      " -- {'<pad>': 0, 'NONE': 1, 'PROD1': 2, 'PRED': 3, 'ASP': 4, 'PROD2': 5}\n",
      "word_sequences_train 23999\n",
      "[['however', 'it', 'be', 'better', 'in', 'so', 'many', 'ways', 'such', 'as', 'image', 'quality', 'and', 'handle', 'that', 'i', 'have', 'to', 'say', 'i', 'think', 'it', 'be', 'only', 'marginally', 'overpriced'], ['i', 'upgraded', 'from', 'a', 'smaller', 'sony', 'sub', 'that', 'was', 'part', 'of', 'a', '5', '1', 'speaker', 'set', 'that', 'came', 'with', 'my', 'receiver'], ['as', 'an', 'upgrade', 'to', 'my', 'htib', 'center', 'the', 'midrange', 'and', 'bass', 'as', 'well', 'as', 'high', 'frequency', 'clarity', 'are', 'amazing', 'when', 'compared', 'to', 'what', 'it', 'is', 'replacing'], ['i', 'did', 'some', 'research', 'and', 'learned', 'that', 'uv', 'protection', 'was', 'a', 'much', 'bigger', 'concern', 'with', 'film', 'than', 'it', 'is', 'digital'], ['my', 'monitor', \"doesn't\", 'get', 'any', 'power', 'so', 'i', 'have', 'to', 'buy', 'a', 'new', 'one', 'as', 'it', 'is', 'cheaper', 'to', 'buy', 'one', 'than', 'fixing', 'the', 'existing', 'one'], ['all', 'in', 'all', 'i', 'am', 'very', 'pleased', 'with', 'the', 'upgrade', 'but', 'knocked', 'it', 'to', '4', 'stars', 'because', 'of', 'the', 'battery', 'door'], ['the', 'olevia', 'was', 'by', 'far', 'the', 'least', 'expensive', 'but', 'its', 'picture', 'quality', 'seemed', 'comparatively', 'much', 'worse', 'not', 'to', 'mention', 'external', 'styling'], ['this', 'camera', 'is', 'only', 'slightly', 'larger', 'than', 'a', 'soda', 'can'], [\"it's\", 'what', 'it', 'claims', 'to', 'be', 'and', 'i', 'bought', 'it', 'at', 'a', 'good', 'price', \"can't\", 'expect', 'much', 'more', 'from', 'a', 'cable'], ['at', 'one', 'time', 'i', 'did', 'own', 'a', 'canon', 'only', 'because', 'it', 'was', 'cheaper', 'go', 'ahead', 'and', 'kick', 'me', 'for', 'it', 'i', 'did']]\n",
      "\n",
      "Start training...\n",
      "\n",
      "\n",
      "\n",
      "++ predicting, batch 239/239 (100.00%).\n",
      "\n",
      "++ predicting, batch 18/18 (95.00%).\n",
      "\n",
      "++ predicting, batch 10/10 (90.00%).\n",
      "== eval epoch 0/100 \"f1-alpha-match-10\" train / dev / test | 1.00 / 0.92 / 1.14.\n",
      "## [BEST epoch], 95 seconds.\n",
      "\n",
      "-- train epoch 1/100, batch 2399/2399 (100.00%), loss = 24.81.\n",
      "\n",
      "++ predicting, batch 239/239 (100.00%).\n",
      "\n",
      "++ predicting, batch 18/18 (95.00%).\n",
      "\n",
      "++ predicting, batch 10/10 (90.00%).\n",
      "== eval epoch 1/100 \"f1-alpha-match-10\" train / dev / test | 69.54 / 69.36 / 24.23.\n",
      "## [BEST epoch], 130 seconds.\n",
      "\n",
      "-- train epoch 2/100, batch 2399/2399 (100.00%), loss = 16.69.\n",
      "\n",
      "++ predicting, batch 239/239 (100.00%).\n",
      "\n",
      "++ predicting, batch 18/18 (95.00%).\n",
      "\n",
      "++ predicting, batch 10/10 (90.00%).\n",
      "== eval epoch 2/100 \"f1-alpha-match-10\" train / dev / test | 78.91 / 78.28 / 25.90.\n",
      "## [BEST epoch], 151 seconds.\n",
      "\n",
      "-- train epoch 3/100, batch 2399/2399 (100.00%), loss = 14.36.\n",
      "\n",
      "++ predicting, batch 239/239 (100.00%).\n",
      "\n",
      "++ predicting, batch 18/18 (95.00%).\n",
      "\n",
      "++ predicting, batch 10/10 (90.00%).\n",
      "== eval epoch 3/100 \"f1-alpha-match-10\" train / dev / test | 79.99 / 79.17 / 26.99.\n",
      "## [BEST epoch], 134 seconds.\n",
      "\n",
      "-- train epoch 4/100, batch 2399/2399 (100.00%), loss = 13.25.\n",
      "\n",
      "++ predicting, batch 239/239 (100.00%).\n",
      "\n",
      "++ predicting, batch 18/18 (95.00%).\n",
      "\n",
      "++ predicting, batch 10/10 (90.00%).\n",
      "== eval epoch 4/100 \"f1-alpha-match-10\" train / dev / test | 80.97 / 79.77 / 29.05.\n",
      "## [BEST epoch], 146 seconds.\n",
      "\n",
      "-- train epoch 5/100, batch 2399/2399 (100.00%), loss = 12.39.\n",
      "\n",
      "++ predicting, batch 239/239 (100.00%).\n",
      "\n",
      "++ predicting, batch 18/18 (95.00%).\n",
      "\n",
      "++ predicting, batch 10/10 (90.00%).\n",
      "== eval epoch 5/100 \"f1-alpha-match-10\" train / dev / test | 83.16 / 81.74 / 24.34.\n",
      "## [BEST epoch], 146 seconds.\n",
      "\n",
      "-- train epoch 6/100, batch 2399/2399 (100.00%), loss = 12.49.\n",
      "\n",
      "++ predicting, batch 239/239 (100.00%).\n",
      "\n",
      "++ predicting, batch 18/18 (95.00%).\n",
      "\n",
      "++ predicting, batch 10/10 (90.00%).\n",
      "== eval epoch 6/100 \"f1-alpha-match-10\" train / dev / test | 83.58 / 81.92 / 28.10.\n",
      "## [BEST epoch], 133 seconds.\n",
      "\n",
      "-- train epoch 7/100, batch 2399/2399 (100.00%), loss = 11.92.\n",
      "\n",
      "++ predicting, batch 239/239 (100.00%).\n",
      "\n",
      "++ predicting, batch 18/18 (95.00%).\n",
      "\n",
      "++ predicting, batch 10/10 (90.00%).\n",
      "== eval epoch 7/100 \"f1-alpha-match-10\" train / dev / test | 83.78 / 81.25 / 30.30.\n",
      "## [no improvement micro-f1 on DEV during the last 1 epochs (best_f1_dev=81.92), 149 seconds].\n",
      "\n",
      "-- train epoch 8/100, batch 2399/2399 (100.00%), loss = 10.85.\n",
      "\n",
      "++ predicting, batch 239/239 (100.00%).\n",
      "\n",
      "++ predicting, batch 18/18 (95.00%).\n",
      "\n",
      "++ predicting, batch 10/10 (90.00%).\n",
      "== eval epoch 8/100 \"f1-alpha-match-10\" train / dev / test | 84.91 / 82.79 / 30.97.\n",
      "## [BEST epoch], 129 seconds.\n",
      "\n",
      "-- train epoch 9/100, batch 2399/2399 (100.00%), loss = 10.49.\n",
      "\n",
      "++ predicting, batch 239/239 (100.00%).\n",
      "\n",
      "++ predicting, batch 18/18 (95.00%).\n",
      "\n",
      "++ predicting, batch 10/10 (90.00%).\n",
      "== eval epoch 9/100 \"f1-alpha-match-10\" train / dev / test | 85.21 / 82.75 / 31.37.\n",
      "## [no improvement micro-f1 on DEV during the last 1 epochs (best_f1_dev=82.79), 146 seconds].\n",
      "\n",
      "-- train epoch 10/100, batch 2399/2399 (100.00%), loss = 10.01.\n",
      "\n",
      "++ predicting, batch 239/239 (100.00%).\n",
      "\n",
      "++ predicting, batch 18/18 (95.00%).\n",
      "\n",
      "++ predicting, batch 10/10 (90.00%).\n",
      "== eval epoch 10/100 \"f1-alpha-match-10\" train / dev / test | 85.64 / 82.98 / 32.44.\n",
      "## [BEST epoch], 142 seconds.\n",
      "\n",
      "-- train epoch 11/100, batch 2399/2399 (100.00%), loss = 10.28.\n",
      "\n",
      "++ predicting, batch 239/239 (100.00%).\n",
      "\n",
      "++ predicting, batch 18/18 (95.00%).\n",
      "\n",
      "++ predicting, batch 10/10 (90.00%).\n",
      "== eval epoch 11/100 \"f1-alpha-match-10\" train / dev / test | 85.61 / 83.31 / 32.29.\n",
      "## [BEST epoch], 148 seconds.\n",
      "\n",
      "-- train epoch 12/100, batch 2399/2399 (100.00%), loss = 10.20.\n",
      "\n",
      "++ predicting, batch 239/239 (100.00%).\n",
      "\n",
      "++ predicting, batch 18/18 (95.00%).\n",
      "\n",
      "++ predicting, batch 10/10 (90.00%).\n",
      "== eval epoch 12/100 \"f1-alpha-match-10\" train / dev / test | 86.17 / 83.54 / 34.25.\n",
      "## [BEST epoch], 149 seconds.\n",
      "\n",
      "-- train epoch 13/100, batch 2399/2399 (100.00%), loss = 9.32.\n",
      "\n",
      "++ predicting, batch 239/239 (100.00%).\n",
      "\n",
      "++ predicting, batch 18/18 (95.00%).\n",
      "\n",
      "++ predicting, batch 10/10 (90.00%).\n",
      "== eval epoch 13/100 \"f1-alpha-match-10\" train / dev / test | 86.48 / 83.62 / 30.99.\n",
      "## [BEST epoch], 133 seconds.\n",
      "\n",
      "-- train epoch 14/100, batch 2399/2399 (100.00%), loss = 9.41.\n",
      "\n",
      "++ predicting, batch 239/239 (100.00%).\n",
      "\n",
      "++ predicting, batch 18/18 (95.00%).\n",
      "\n",
      "++ predicting, batch 10/10 (90.00%).\n",
      "== eval epoch 14/100 \"f1-alpha-match-10\" train / dev / test | 86.38 / 82.96 / 32.87.\n",
      "## [no improvement micro-f1 on DEV during the last 1 epochs (best_f1_dev=83.62), 151 seconds].\n",
      "\n",
      "-- train epoch 15/100, batch 2399/2399 (100.00%), loss = 9.67.\n",
      "\n",
      "++ predicting, batch 239/239 (100.00%).\n",
      "\n",
      "++ predicting, batch 18/18 (95.00%).\n",
      "\n",
      "++ predicting, batch 10/10 (90.00%).\n",
      "== eval epoch 15/100 \"f1-alpha-match-10\" train / dev / test | 87.05 / 83.39 / 34.29.\n",
      "## [no improvement micro-f1 on DEV during the last 2 epochs (best_f1_dev=83.62), 125 seconds].\n",
      "\n",
      "-- train epoch 16/100, batch 2399/2399 (100.00%), loss = 9.37.\n",
      "\n",
      "++ predicting, batch 239/239 (100.00%).\n",
      "\n",
      "++ predicting, batch 18/18 (95.00%).\n",
      "\n",
      "++ predicting, batch 10/10 (90.00%).\n",
      "== eval epoch 16/100 \"f1-alpha-match-10\" train / dev / test | 86.58 / 83.20 / 34.79.\n",
      "## [no improvement micro-f1 on DEV during the last 3 epochs (best_f1_dev=83.62), 146 seconds].\n",
      "\n",
      "-- train epoch 17/100, batch 2399/2399 (100.00%), loss = 8.71.\n",
      "\n",
      "++ predicting, batch 239/239 (100.00%).\n",
      "\n",
      "++ predicting, batch 18/18 (95.00%).\n",
      "\n",
      "++ predicting, batch 10/10 (90.00%).\n",
      "== eval epoch 17/100 \"f1-alpha-match-10\" train / dev / test | 86.85 / 83.09 / 35.31.\n",
      "## [no improvement micro-f1 on DEV during the last 4 epochs (best_f1_dev=83.62), 145 seconds].\n",
      "\n",
      "-- train epoch 18/100, batch 2399/2399 (100.00%), loss = 8.82.\n",
      "\n",
      "++ predicting, batch 239/239 (100.00%).\n",
      "\n",
      "++ predicting, batch 18/18 (95.00%).\n",
      "\n",
      "++ predicting, batch 10/10 (90.00%).\n",
      "== eval epoch 18/100 \"f1-alpha-match-10\" train / dev / test | 87.88 / 84.27 / 37.42.\n",
      "## [BEST epoch], 132 seconds.\n",
      "\n",
      "-- train epoch 19/100, batch 2399/2399 (100.00%), loss = 8.79.\n",
      "\n",
      "++ predicting, batch 239/239 (100.00%).\n",
      "\n",
      "++ predicting, batch 18/18 (95.00%).\n",
      "\n",
      "++ predicting, batch 10/10 (90.00%).\n",
      "== eval epoch 19/100 \"f1-alpha-match-10\" train / dev / test | 87.82 / 84.47 / 35.77.\n",
      "## [BEST epoch], 150 seconds.\n",
      "\n",
      "-- train epoch 20/100, batch 2399/2399 (100.00%), loss = 8.11.\n",
      "\n",
      "++ predicting, batch 239/239 (100.00%).\n",
      "\n",
      "++ predicting, batch 18/18 (95.00%).\n",
      "\n",
      "++ predicting, batch 10/10 (90.00%).\n",
      "== eval epoch 20/100 \"f1-alpha-match-10\" train / dev / test | 87.11 / 82.53 / 38.30.\n",
      "## [no improvement micro-f1 on DEV during the last 1 epochs (best_f1_dev=84.47), 131 seconds].\n",
      "\n",
      "-- train epoch 21/100, batch 2399/2399 (100.00%), loss = 8.07.\n",
      "\n",
      "++ predicting, batch 239/239 (100.00%).\n",
      "\n",
      "++ predicting, batch 18/18 (95.00%).\n",
      "\n",
      "++ predicting, batch 10/10 (90.00%).\n",
      "== eval epoch 21/100 \"f1-alpha-match-10\" train / dev / test | 88.05 / 83.21 / 35.79.\n",
      "## [no improvement micro-f1 on DEV during the last 2 epochs (best_f1_dev=84.47), 145 seconds].\n",
      "\n",
      "-- train epoch 22/100, batch 2399/2399 (100.00%), loss = 8.73.\n",
      "\n",
      "++ predicting, batch 239/239 (100.00%).\n",
      "\n",
      "++ predicting, batch 18/18 (95.00%).\n",
      "\n",
      "++ predicting, batch 10/10 (90.00%).\n",
      "== eval epoch 22/100 \"f1-alpha-match-10\" train / dev / test | 88.65 / 83.92 / 37.23.\n",
      "## [no improvement micro-f1 on DEV during the last 3 epochs (best_f1_dev=84.47), 144 seconds].\n",
      "\n",
      "-- train epoch 23/100, batch 2399/2399 (100.00%), loss = 8.45.\n",
      "\n",
      "++ predicting, batch 239/239 (100.00%).\n",
      "\n",
      "++ predicting, batch 18/18 (95.00%).\n",
      "\n",
      "++ predicting, batch 10/10 (90.00%).\n",
      "== eval epoch 23/100 \"f1-alpha-match-10\" train / dev / test | 88.54 / 83.68 / 37.44.\n",
      "## [no improvement micro-f1 on DEV during the last 4 epochs (best_f1_dev=84.47), 136 seconds].\n",
      "\n",
      "-- train epoch 24/100, batch 2399/2399 (100.00%), loss = 8.77.\n",
      "\n",
      "++ predicting, batch 239/239 (100.00%).\n",
      "\n",
      "++ predicting, batch 18/18 (95.00%).\n",
      "\n",
      "++ predicting, batch 10/10 (90.00%).\n",
      "== eval epoch 24/100 \"f1-alpha-match-10\" train / dev / test | 88.34 / 83.44 / 36.79.\n",
      "## [no improvement micro-f1 on DEV during the last 5 epochs (best_f1_dev=84.47), 151 seconds].\n",
      "\n",
      "-- train epoch 25/100, batch 2399/2399 (100.00%), loss = 7.39.\n",
      "\n",
      "++ predicting, batch 239/239 (100.00%).\n",
      "\n",
      "++ predicting, batch 18/18 (95.00%).\n",
      "\n",
      "++ predicting, batch 10/10 (90.00%).\n",
      "== eval epoch 25/100 \"f1-alpha-match-10\" train / dev / test | 88.81 / 84.04 / 37.34.\n",
      "## [no improvement micro-f1 on DEV during the last 6 epochs (best_f1_dev=84.47), 130 seconds].\n",
      "\n",
      "-- train epoch 26/100, batch 2399/2399 (100.00%), loss = 7.33.\n",
      "\n",
      "++ predicting, batch 239/239 (100.00%).\n",
      "\n",
      "++ predicting, batch 18/18 (95.00%).\n",
      "\n",
      "++ predicting, batch 10/10 (90.00%).\n",
      "== eval epoch 26/100 \"f1-alpha-match-10\" train / dev / test | 89.11 / 84.08 / 37.14.\n",
      "## [no improvement micro-f1 on DEV during the last 7 epochs (best_f1_dev=84.47), 147 seconds].\n",
      "\n",
      "-- train epoch 27/100, batch 2399/2399 (100.00%), loss = 7.83.\n",
      "\n",
      "++ predicting, batch 239/239 (100.00%).\n",
      "\n",
      "++ predicting, batch 18/18 (95.00%).\n",
      "\n",
      "++ predicting, batch 10/10 (90.00%).\n",
      "== eval epoch 27/100 \"f1-alpha-match-10\" train / dev / test | 89.45 / 84.14 / 38.19.\n",
      "## [no improvement micro-f1 on DEV during the last 8 epochs (best_f1_dev=84.47), 141 seconds].\n",
      "\n",
      "-- train epoch 28/100, batch 2399/2399 (100.00%), loss = 7.25.\n",
      "\n",
      "++ predicting, batch 239/239 (100.00%).\n",
      "\n",
      "++ predicting, batch 18/18 (95.00%).\n",
      "\n",
      "++ predicting, batch 10/10 (90.00%).\n",
      "== eval epoch 28/100 \"f1-alpha-match-10\" train / dev / test | 89.54 / 83.96 / 37.70.\n",
      "## [no improvement micro-f1 on DEV during the last 9 epochs (best_f1_dev=84.47), 140 seconds].\n",
      "\n",
      "-- train epoch 29/100, batch 2399/2399 (100.00%), loss = 7.26.\n",
      "\n",
      "++ predicting, batch 239/239 (100.00%).\n",
      "\n",
      "++ predicting, batch 18/18 (95.00%).\n",
      "\n",
      "++ predicting, batch 10/10 (90.00%).\n",
      "== eval epoch 29/100 \"f1-alpha-match-10\" train / dev / test | 89.91 / 84.98 / 37.90.\n",
      "## [BEST epoch], 151 seconds.\n",
      "\n",
      "-- train epoch 30/100, batch 2399/2399 (100.00%), loss = 7.18.\n",
      "\n",
      "++ predicting, batch 239/239 (100.00%).\n",
      "\n",
      "++ predicting, batch 18/18 (95.00%).\n",
      "\n",
      "++ predicting, batch 10/10 (90.00%).\n",
      "== eval epoch 30/100 \"f1-alpha-match-10\" train / dev / test | 90.08 / 84.46 / 37.38.\n",
      "## [no improvement micro-f1 on DEV during the last 1 epochs (best_f1_dev=84.98), 130 seconds].\n",
      "\n",
      "-- train epoch 31/100, batch 2399/2399 (100.00%), loss = 6.95.\n",
      "\n",
      "++ predicting, batch 239/239 (100.00%).\n",
      "\n",
      "++ predicting, batch 18/18 (95.00%).\n",
      "\n",
      "++ predicting, batch 10/10 (90.00%).\n",
      "== eval epoch 31/100 \"f1-alpha-match-10\" train / dev / test | 90.13 / 84.52 / 37.55.\n",
      "## [no improvement micro-f1 on DEV during the last 2 epochs (best_f1_dev=84.98), 150 seconds].\n",
      "\n",
      "-- train epoch 32/100, batch 2399/2399 (100.00%), loss = 6.58.\n",
      "\n",
      "++ predicting, batch 239/239 (100.00%).\n",
      "\n",
      "++ predicting, batch 18/18 (95.00%).\n",
      "\n",
      "++ predicting, batch 10/10 (90.00%).\n",
      "== eval epoch 32/100 \"f1-alpha-match-10\" train / dev / test | 89.60 / 83.45 / 39.16.\n",
      "## [no improvement micro-f1 on DEV during the last 3 epochs (best_f1_dev=84.98), 132 seconds].\n",
      "\n",
      "-- train epoch 33/100, batch 2399/2399 (100.00%), loss = 6.18.\n",
      "\n",
      "++ predicting, batch 136/239 (57.00%)."
     ]
    }
   ],
   "source": [
    "! python3 main.py --train \"/home/vika/targer/data/NER/Indian_dataset/train.csv\" --dev \"/home/vika/targer/data/NER/Indian_dataset/dev.csv\" --data-io connl-ner-2003 --evaluator f1-alpha-match-10 --model 'BiRNN' --opt adam --lr 0.001 --save-best yes --patience 20 --rnn-hidden-dim 200 --gpu 1 --test \"/home/vika/targer/data/NER/Indian_dataset/test.csv\" --elmo False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.modules.elmo import Elmo, batch_to_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [['1tb', 'of', 'mechanical', 'storage', 'is', \"n't\", 'bad', '\"', 'but', 'toshiba', 'hard', 'drives', 'really', 'are', \"n't\", 'what', 'we', 'want', 'to', 'be', 'seeing', 'as', 'they', 'tend', 'to', 'be', 'a', 'bit', 'slower', 'than', 'competing', 'drives', 'from', 'hgst', '\"', 'western', 'digital', '\"', 'samsung', '\"', 'and', 'seagate', '.'],  ['we', 'have', 'found', 'that', 'blood', 'carbon', 'and', 'nitrogen', 'turnover', 'was', 'roughly', 'equivalent', 'in', 'mice', 'and', 'in', 'rats', '\"', 'and', 'that', 'mouse', 'blood', 'turned', 'over', 'slightly', 'faster', 'than', 'rat', 'blood', '.']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('1tb', 'O')\n",
      "('of', 'O')\n",
      "('mechanical', 'O')\n",
      "('storage', 'O')\n",
      "('is', 'O')\n",
      "(\"n't\", 'O')\n",
      "('bad', 'O')\n",
      "('\"', 'O')\n",
      "('but', 'O')\n",
      "('toshiba', 'B-OTHOBJ')\n",
      "('hard', 'O')\n",
      "('drives', 'O')\n",
      "('really', 'O')\n",
      "('are', 'O')\n",
      "(\"n't\", 'O')\n",
      "('what', 'O')\n",
      "('we', 'O')\n",
      "('want', 'O')\n",
      "('to', 'O')\n",
      "('be', 'O')\n",
      "('seeing', 'O')\n",
      "('as', 'O')\n",
      "('they', 'O')\n",
      "('tend', 'O')\n",
      "('to', 'O')\n",
      "('be', 'O')\n",
      "('a', 'O')\n",
      "('bit', 'O')\n",
      "('slower', 'O')\n",
      "('than', 'O')\n",
      "('competing', 'O')\n",
      "('drives', 'O')\n",
      "('from', 'O')\n",
      "('hgst', 'O')\n",
      "('\"', 'O')\n",
      "('western', 'O')\n",
      "('digital', 'O')\n",
      "('\"', 'O')\n",
      "('samsung', 'B-ASPOBJ')\n",
      "('\"', 'O')\n",
      "('and', 'O')\n",
      "('seagate', 'O')\n",
      "('.', 'O')\n"
     ]
    }
   ],
   "source": [
    "for elem in zip(sentences[0], tags[0]):\n",
    "    print (elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "++ predicting, batch 1/1 (0.00%)."
     ]
    }
   ],
   "source": [
    "from src.factories.factory_tagger import TaggerFactory\n",
    "import torch\n",
    "\n",
    "model = TaggerFactory.load(\"2019_09_24_10-48_48_tagger.hdf5\")\n",
    "model.gpu = 1\n",
    "model.cuda(device = 1)\n",
    "\n",
    "\n",
    "tags = model.predict_tags_from_words(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /opt/anaconda/lib/python3.6/site-packages (18.1)\n",
      "Collecting install\n",
      "\u001b[33m  Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ConnectTimeoutError(<pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x7f23f7295240>, 'Connection to pypi.org timed out. (connect timeout=15)')': /simple/install/\u001b[0m\n",
      "\u001b[33m  Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ConnectTimeoutError(<pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x7f23f72951d0>, 'Connection to pypi.org timed out. (connect timeout=15)')': /simple/install/\u001b[0m\n",
      "\u001b[33m  Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ConnectTimeoutError(<pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x7f23f72950f0>, 'Connection to pypi.org timed out. (connect timeout=15)')': /simple/install/\u001b[0m\n",
      "\u001b[33m  Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ConnectTimeoutError(<pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x7f23f7295eb8>, 'Connection to pypi.org timed out. (connect timeout=15)')': /simple/install/\u001b[0m\n",
      "^C\n",
      "\u001b[31mOperation cancelled by user\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip3 install pip install --trusted-host=pypi.python.org --trusted-host=pypi.org --trusted-host=files.pythonhosted.org pytorch-pretrained-bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'BertTokenizer' object has no attribute 'encode'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-51f29d9cf921>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Let's see all hidden-states and attentions on this text\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'BertTokenizer' object has no attribute 'encode'"
     ]
    }
   ],
   "source": [
    "input_ids = torch.tensor([tokenizer.encode(\"Let's see all hidden-states and attentions on this text\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'string' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-7d756f7a8a8a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bert-base-uncased'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'string' is not defined"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "words = string.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [['we', 'have', 'found', 'that', 'blood', 'carbon', 'and', 'nitrogen', 'turnover', 'was', 'roughly', 'equivalent'], ['in', 'mice', 'and', 'in', 'rats', '\"', 'and', 'that', 'mouse', 'blood', 'turned', 'over','.']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['we', '##ha', '##ve', '##fo', '##und', '##tha', '##t', '##blood', '##carbon', '##and', '##ni', '##tro', '##gent', '##urn', '##over', '##was', '##rou', '##gh', '##ly', '##e', '##qui', '##valent']\n"
     ]
    }
   ],
   "source": [
    "tokenized_texts = [tokenizer.tokenize(''.join(sent)) for sent in sentences]\n",
    "print(tokenized_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "MAX_LEN = np.max(np.array([len(seq) for seq in tokenized_texts]))\n",
    "input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts], maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_LEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sequence item 0: expected str instance, list found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-d45856db5fe6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: sequence item 0: expected str instance, list found"
     ]
    }
   ],
   "source": [
    "words = ''.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(indexed_tokens2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_tensor = torch.tensor(input_ids)\n",
    "segments_tensors = torch.tensor(np.ones(input_ids.shape)).to(torch.int64)\n",
    "\n",
    "# Load pre-trained model (weights)\n",
    "model = BertModel.from_pretrained(\"pretrained\")#BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
    "#model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tokens_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(segments_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    encoded_layers, _ = model(tokens_tensor, segments_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoded_layers[2][1][21])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30522"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.vocab.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_embeddings = [] \n",
    "\n",
    "# For each token in the sentence...\n",
    "for token_i in range(len(tokenized_text)):  \n",
    "    # Holds 12 layers of hidden states for each token \n",
    "    hidden_layers = [] \n",
    "    # For each of the 12 layers...\n",
    "    for layer_i in range(len(encoded_layers)):\n",
    "    # Lookup the vector for `token_i` in `layer_i`\n",
    "        vec = encoded_layers[layer_i][0][token_i]\n",
    "        hidden_layers.append(vec)\n",
    "        token_embeddings.append(hidden_layers)\n",
    "\n",
    "summed_last_4_layers = [torch.sum(torch.stack(layer)[-4:], 0) for layer in token_embeddings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "540"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(summed_last_4_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Here is the sentence I want embeddings for.\"\n",
    "text = \"After stealing money from the bank vault, the bank robber was seen fishing on the Mississippi river bank.\"\n",
    "marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "\n",
    "print (marked_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.modules.elmo import Elmo, batch_to_ids\n",
    "\n",
    "options_file = \"embeddings/elmo_2x4096_512_2048cnn_2xhighway_5.5B_options.json\"\n",
    "weight_file = \"/home/vika/targer/embeddings/elmo_2x4096_512_2048cnn_2xhighway_5.5B_weights.hdf5\"\n",
    "\n",
    "# Compute two different representation for each token.\n",
    "# Each representation is a linear weighted combination for the\n",
    "# 3 layers in ELMo (i.e., charcnn, the outputs of the two BiLSTM))\n",
    "elmo = Elmo(options_file, weight_file, 2, dropout=0)\n",
    "\n",
    "# use batch_to_ids to convert sentences to character ids\n",
    "sentences = [['1tb', 'of', 'mechanical', 'storage', 'is', \"n't\", 'bad', '\"', 'but', 'toshiba', 'hard', 'drives', 'really', 'are', \"n't\", 'what', 'we', 'want', 'to', 'be', 'seeing', 'as', 'they', 'tend', 'to', 'be', 'a', 'bit', 'slower', 'than', 'competing', 'drives', 'from', 'hgst', '\"', 'western', 'digital', '\"', 'samsung', '\"', 'and', 'seagate', '.'], ['amazon', 'is', 'another', 'good', 'choice', '\"', 'and', 'i', 'actually', 'prefer', 'their', 'interface', 'better', 'than', 'itunes', '', '(', 'it', 'loads', 'faster', 'for', 'me', ')', '.'], ['well-designed', '\"', 'properly', 'constructed', 'timber', 'buildings', 'can', 'be', 'even', 'safer', 'in', 'earthquakes', 'than', 'ferro-concrete', 'ones', '.'], ['maybe', 'it', 's', 'because', 'i', 'm', 'a', 'ku', 'fan', '\"', 'but', 'i', 'honestly', 'like', 'the', 'looks', 'of', 'adidas', 'uniforms', 'better', 'than', 'nike', '.'], ['soft', 'drinks', 'do', 'not', 'include', 'beverages', 'that', 'contain', 'milk', 'or', 'milk', 'products', '\"', 'soy', '\"', 'rice', 'or', 'similar', 'milk', 'substitutes', '\"', 'or', 'greater', 'than', '50', '%', 'of', 'vegetable', 'or', 'fruit', 'juice', 'by', 'volume', '.'], ['partly', 'because', 'i', 'like', 'apple', 's', 'choice', 'of', 'file', 'format', '-', 'aac', 'is', 'much', 'more', 'modern', 'than', 'mp3', '\"', 'and', 'theoretically', 'an', 'aac', 'file', 'at', '256kbps', '(', 'from', 'itunes', 'plus', ')', 'sounds', 'better', 'than', 'an', 'mp3', 'file', 'at', '256kbps', '(', 'from', 'amazon', ')', '.'], ['and', 'nokia', 'is', 'easier', 'to', 'use', '...', '...', 'it', 'easily', 'gets', 'infected', 'with', 'virus', 'and', 'the', 'net', 'speed', 'is', 'not', 'fast', '...', '..and', 'ofcourse', 'nokia', 'is', 'way', 'more', 'reliable', 'than', 'samsung', '...', '..go', 'for', 'nokia', '...', '..this', 'phone', 'suxxxxxxxxxxxxxxxxxxxxxx', '!'], ['java', 'is', 'a', 'static', 'type', 'language', 'and', 'it', 'is', 'safer', 'than', 'dynamic', 'type', 'languages', 'like', 'ruby', '.'], ['amazon', 'music', 'will', 'be', 'cheaper', '\"', 'faster', 'and', 'perceived', 'to', 'be', 'more', 'honest', 'than', 'itunes', 'both', 'by', 'customers', 'and', 'musicians', '.'], \n",
    "             ['we', 'have', 'found', 'that', 'blood', 'carbon', 'and', 'nitrogen', 'turnover', 'was', 'roughly', 'equivalent', 'in', 'mice', 'and', 'in', 'rats', '\"', 'and', 'that', 'mouse', 'blood', 'turned', 'over', 'slightly', 'faster', 'than', 'rat', 'blood', '.']]\n",
    "\n",
    "character_ids = batch_to_ids(sentences)\n",
    "\n",
    "embeddings = elmo(character_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elmo.get_output_dim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 44, 50])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "character_ids.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([259, 113, 115, 112, 113, 102, 115, 109, 122, 260, 261, 261, 261, 261,\n",
       "        261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
       "        261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
       "        261, 261, 261, 261, 261, 261, 261, 261])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "character_ids[2][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.4785, -0.4598, -0.0396,  ..., -0.1577, -0.3011, -0.1590],\n",
      "       grad_fn=<SelectBackward>)\n"
     ]
    }
   ],
   "source": [
    "print (embeddings['elmo_representations'][1][5][5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.4785, -0.4598, -0.0396,  ..., -0.1577, -0.3011, -0.1590],\n",
      "       grad_fn=<SelectBackward>)\n"
     ]
    }
   ],
   "source": [
    "print (embeddings['elmo_representations'][0][5][5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## birnn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BiRNNCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "args train = /home/vika/NER_RNN/targer/data/NER/Asqua_CAM_aspects/CAM_train.csv\n",
      "Loading from /home/vika/NER_RNN/targer/data/NER/Asqua_CAM_aspects/CAM_train.csv: 2619 samples, 74616 words.\n",
      "Loading from /home/vika/NER_RNN/targer/data/NER/Asqua_CAM_aspects/CAM_test.csv: 523 samples, 14728 words.\n",
      "Loading from /home/vika/NER_RNN/targer/data/NER/Asqua_CAM_aspects/CAM_test.csv: 523 samples, 14728 words.\n",
      "word_sequences_dev 523 [['and', 'can', 'i', 'just', 'say', 'that', 'doing', 'the', 'inferno', 'move', 'on', 'the', 'ds', 'is', '50', 'times', 'easier', 'than', 'trying', 'to', 'make', 'an', 'infinity', 'symbol', 'using', 'the', 'wii', 'remote', '.'], ['i', 'go', 'to', 'the', 'amazon', 'mp3', 'store', '(', 'better', 'than', 'itunes', '.'], ['tea', 'dinner', '(', 'sorry', '\"', 'my', 'wife', 'is', 'trying', 'to', 'make', 'me', 'posherer', ')', 'and', 'picking', 'up', 'a', 'phone', 'is', 'easier', '-', 'then', 'i', 'can', 'swig', 'my', 'beer', 'and', 'relaaaaxxxx', '...', '.', '.'], ['actually', 'for', 'many', 'windows', 'xp', 'users', 'it', 'is', 'easier', 'to', 'migrate', 'to', 'linux', 'mint', 'than', 'to', 'windows', '8', '.'], ['this', 'is', 'why', 'the', 'better', 'team', 'wins', 'any', 'given', 'basketball', 'game', 'with', 'far', 'greater', 'frequency', 'than', 'it', 'does', 'in', 'baseball', '\"', 'football', 'or', 'hockey', '.']]\n",
      "tag_sequences_train 523 [['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ASPOBJ', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-OTHOBJ', 'O', 'O'], ['O', 'O', 'O', 'O', 'B-ASPOBJ', 'O', 'O', 'O', 'O', 'O', 'B-OTHOBJ', 'O'], ['B-ASPOBJ', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-OTHOBJ', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'B-ASPOBJ', 'I-ASPOBJ', 'O', 'O', 'O', 'B-ASP', 'I-ASP', 'I-ASP', 'I-ASP', 'I-ASP', 'I-ASP', 'O', 'O', 'B-OTHOBJ', 'I-OTHOBJ', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ASPOBJ', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-OTHOBJ', 'O', 'O', 'O', 'O', 'O']]\n",
      "DatasetsBank: len(unique_words_list) = 7143 unique words.\n",
      "DatasetsBank: len(unique_words_list) = 7753 unique words.\n",
      "DatasetsBank: len(unique_words_list) = 7753 unique words.\n",
      "Reading embeddings file embeddings/glove.6B.100d.txt, line = 0\n",
      "Reading embeddings file embeddings/glove.6B.100d.txt, line = 25000\n",
      "Reading embeddings file embeddings/glove.6B.100d.txt, line = 50000\n",
      "Reading embeddings file embeddings/glove.6B.100d.txt, line = 75000\n",
      "Reading embeddings file embeddings/glove.6B.100d.txt, line = 100000\n",
      "Reading embeddings file embeddings/glove.6B.100d.txt, line = 125000\n",
      "Reading embeddings file embeddings/glove.6B.100d.txt, line = 150000\n",
      "Reading embeddings file embeddings/glove.6B.100d.txt, line = 175000\n",
      "Reading embeddings file embeddings/glove.6B.100d.txt, line = 200000\n",
      "Reading embeddings file embeddings/glove.6B.100d.txt, line = 225000\n",
      "Reading embeddings file embeddings/glove.6B.100d.txt, line = 250000\n",
      "Reading embeddings file embeddings/glove.6B.100d.txt, line = 275000\n",
      "Reading embeddings file embeddings/glove.6B.100d.txt, line = 300000\n",
      "Reading embeddings file embeddings/glove.6B.100d.txt, line = 325000\n",
      "Reading embeddings file embeddings/glove.6B.100d.txt, line = 350000\n",
      "Reading embeddings file embeddings/glove.6B.100d.txt, line = 375000\n",
      "Reading embeddings file embeddings/glove.6B.100d.txt, line = 0\n",
      "Reading embeddings file embeddings/glove.6B.100d.txt, line = 25000\n",
      "Reading embeddings file embeddings/glove.6B.100d.txt, line = 50000\n",
      "Reading embeddings file embeddings/glove.6B.100d.txt, line = 75000\n",
      "Reading embeddings file embeddings/glove.6B.100d.txt, line = 100000\n",
      "Reading embeddings file embeddings/glove.6B.100d.txt, line = 125000\n",
      "Reading embeddings file embeddings/glove.6B.100d.txt, line = 150000\n",
      "Reading embeddings file embeddings/glove.6B.100d.txt, line = 175000\n",
      "Reading embeddings file embeddings/glove.6B.100d.txt, line = 200000\n",
      "Reading embeddings file embeddings/glove.6B.100d.txt, line = 225000\n",
      "Reading embeddings file embeddings/glove.6B.100d.txt, line = 250000\n",
      "Reading embeddings file embeddings/glove.6B.100d.txt, line = 275000\n",
      "Reading embeddings file embeddings/glove.6B.100d.txt, line = 300000\n",
      "Reading embeddings file embeddings/glove.6B.100d.txt, line = 325000\n",
      "Reading embeddings file embeddings/glove.6B.100d.txt, line = 350000\n",
      "Reading embeddings file embeddings/glove.6B.100d.txt, line = 375000\n",
      "\n",
      "load_vocabulary_from_embeddings_file_and_unique_words_list:\n",
      "    First 50 OOV words:\n",
      "        out_of_vocabulary_words_list[0] = b'1tb'\n",
      "        out_of_vocabulary_words_list[1] = b'hgst'\n",
      "        out_of_vocabulary_words_list[2] = b''\n",
      "        out_of_vocabulary_words_list[3] = b'256kbps'\n",
      "        out_of_vocabulary_words_list[4] = b'..and'\n",
      "        out_of_vocabulary_words_list[5] = b'ofcourse'\n",
      "        out_of_vocabulary_words_list[6] = b'..go'\n",
      "        out_of_vocabulary_words_list[7] = b'..this'\n",
      "        out_of_vocabulary_words_list[8] = b'suxxxxxxxxxxxxxxxxxxxxxx'\n",
      "        out_of_vocabulary_words_list[9] = b'e2252'\n",
      "        out_of_vocabulary_words_list[10] = b'monkeypatch'\n",
      "        out_of_vocabulary_words_list[11] = b'mssql'\n",
      "        out_of_vocabulary_words_list[12] = b'eccoboard\\xe2\\x84\\xa2'\n",
      "        out_of_vocabulary_words_list[13] = b'bmw-mercedes'\n",
      "        out_of_vocabulary_words_list[14] = b'goung'\n",
      "        out_of_vocabulary_words_list[15] = b'head-'\n",
      "        out_of_vocabulary_words_list[16] = b'jumbo-shrimp'\n",
      "        out_of_vocabulary_words_list[17] = b'discount-lexus'\n",
      "        out_of_vocabulary_words_list[18] = b'quicker-than-a-cayman'\n",
      "        out_of_vocabulary_words_list[19] = b\"cat's-meow\"\n",
      "        out_of_vocabulary_words_list[20] = b'current-season'\n",
      "        out_of_vocabulary_words_list[21] = b'all-plastic'\n",
      "        out_of_vocabulary_words_list[22] = b'fan-friendly'\n",
      "        out_of_vocabulary_words_list[23] = b'siginificantly'\n",
      "        out_of_vocabulary_words_list[24] = b'games.as'\n",
      "        out_of_vocabulary_words_list[25] = b'for.so'\n",
      "        out_of_vocabulary_words_list[26] = b'power/graphics'\n",
      "        out_of_vocabulary_words_list[27] = b'nv3500'\n",
      "        out_of_vocabulary_words_list[28] = b'350c'\n",
      "        out_of_vocabulary_words_list[29] = b'gretest'\n",
      "        out_of_vocabulary_words_list[30] = b'intoduced'\n",
      "        out_of_vocabulary_words_list[31] = b'mw2'\n",
      "        out_of_vocabulary_words_list[32] = b'yet.it'\n",
      "        out_of_vocabulary_words_list[33] = b'incredebly'\n",
      "        out_of_vocabulary_words_list[34] = b'5000000000000'\n",
      "        out_of_vocabulary_words_list[35] = b'fire-power'\n",
      "        out_of_vocabulary_words_list[36] = b'o3d'\n",
      "        out_of_vocabulary_words_list[37] = b'300x'\n",
      "        out_of_vocabulary_words_list[38] = b'frats'\n",
      "        out_of_vocabulary_words_list[39] = b'jaronczyk'\n",
      "        out_of_vocabulary_words_list[40] = b'import-intenders'\n",
      "        out_of_vocabulary_words_list[41] = b'stodgy-looking'\n",
      "        out_of_vocabulary_words_list[42] = b'sloppy-handling'\n",
      "        out_of_vocabulary_words_list[43] = b'platform-sharing'\n",
      "        out_of_vocabulary_words_list[44] = b'all-weather-wood'\n",
      "        out_of_vocabulary_words_list[45] = b'awwf'\n",
      "        out_of_vocabulary_words_list[46] = b'maniculatus'\n",
      "        out_of_vocabulary_words_list[47] = b'rdbsm'\n",
      "        out_of_vocabulary_words_list[48] = b'hardibacker'\n",
      "        out_of_vocabulary_words_list[49] = b'cement/gypsum'\n",
      "        out_of_vocabulary_words_list[50] = b'atii'\n",
      " -- len(out_of_vocabulary_words_list) = 735\n",
      " -- original_words_num = 7013\n",
      " -- lowercase_words_num = 1\n",
      " -- zero_digits_replaced_num = 4\n",
      " -- zero_digits_replaced_lowercase_num = 0\n",
      "\n",
      "load_vocabulary_from_tag_sequences:\n",
      " -- class_num = 7\n",
      " -- {'<pad>': 0, 'O': 1, 'B-OTHOBJ': 2, 'B-ASPOBJ': 3, 'B-ASP': 4, 'I-ASP': 5, 'I-ASPOBJ': 6, 'I-OTHOBJ': 7}\n",
      "Empirical transition matrix from the train dataset:\n",
      "               <pad>         O  B-OTHOBJ  B-ASPOBJ     B-ASP     I-ASP  I-ASPOBJ  I-OTHOBJ     <sos>\n",
      "\n",
      "     <pad>       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0\n",
      "         O       0.0   58639.0    2722.0    2863.0     792.0     165.0      22.0      21.0    2226.0\n",
      "  B-OTHOBJ       0.0    2713.0       0.0       1.0       3.0       1.0       0.0       0.0      47.0\n",
      "  B-ASPOBJ       0.0    2560.0       8.0       0.0       4.0       3.0       0.0       0.0     341.0\n",
      "     B-ASP       0.0     919.0      14.0      28.0       0.0       0.0       0.0       0.0       5.0\n",
      "     I-ASP       0.0       0.0       0.0       2.0     167.0     307.0       0.0       0.0       0.0\n",
      "  I-ASPOBJ       0.0       0.0       0.0      22.0       0.0       0.0       0.0       0.0       0.0\n",
      "  I-OTHOBJ       0.0       0.0      21.0       0.0       0.0       0.0       0.0       0.0       0.0\n",
      "     <sos>       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0\n",
      "\n",
      "Initialized transition matrix:\n",
      "               <pad>         O  B-OTHOBJ  B-ASPOBJ     B-ASP     I-ASP  I-ASPOBJ  I-OTHOBJ     <sos>\n",
      "\n",
      "     <pad>   -9999.0   -9999.0   -9999.0   -9999.0   -9999.0   -9999.0   -9999.0   -9999.0   -9999.0\n",
      "         O   -9999.0      -1.1      -1.1      -0.9      -1.0      -1.0      -1.1      -1.0      -1.0\n",
      "  B-OTHOBJ   -9999.0      -1.0   -9999.0      -1.0      -1.2      -0.9   -9999.0   -9999.0      -0.9\n",
      "  B-ASPOBJ   -9999.0      -0.9      -1.0   -9999.0      -1.0      -0.8   -9999.0   -9999.0      -1.0\n",
      "     B-ASP   -9999.0      -1.1      -1.0      -1.0   -9999.0   -9999.0   -9999.0   -9999.0      -1.1\n",
      "     I-ASP   -9999.0   -9999.0   -9999.0      -1.1      -0.9      -1.0   -9999.0   -9999.0   -9999.0\n",
      "  I-ASPOBJ   -9999.0   -9999.0   -9999.0      -1.0   -9999.0   -9999.0   -9999.0   -9999.0   -9999.0\n",
      "  I-OTHOBJ   -9999.0   -9999.0      -1.0   -9999.0   -9999.0   -9999.0   -9999.0   -9999.0   -9999.0\n",
      "     <sos>   -9999.0   -9999.0   -9999.0   -9999.0   -9999.0   -9999.0   -9999.0   -9999.0   -9999.0\n",
      "\n",
      "Start training...\n",
      "\n",
      "\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 0/100 \"f1-alpha-match-10\" train / dev / test | 3.48 / 3.55 / 3.55.\n",
      "## [BEST epoch], 11 seconds.\n",
      "\n",
      "-- train epoch 1/100, batch 261/261 (100.00%), loss = 709.25.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 1/100 \"f1-alpha-match-10\" train / dev / test | 60.42 / 59.59 / 59.59.\n",
      "## [BEST epoch], 67 seconds.\n",
      "\n",
      "-- train epoch 2/100, batch 261/261 (100.00%), loss = 394.92.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 2/100 \"f1-alpha-match-10\" train / dev / test | 69.55 / 66.87 / 66.87.\n",
      "## [BEST epoch], 69 seconds.\n",
      "\n",
      "-- train epoch 3/100, batch 261/261 (100.00%), loss = 341.32.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 3/100 \"f1-alpha-match-10\" train / dev / test | 72.22 / 67.51 / 67.51.\n",
      "## [BEST epoch], 69 seconds.\n",
      "\n",
      "-- train epoch 4/100, batch 261/261 (100.00%), loss = 328.28.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 4/100 \"f1-alpha-match-10\" train / dev / test | 74.17 / 71.47 / 71.47.\n",
      "## [BEST epoch], 69 seconds.\n",
      "\n",
      "-- train epoch 5/100, batch 261/261 (100.00%), loss = 305.78.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 5/100 \"f1-alpha-match-10\" train / dev / test | 75.36 / 72.44 / 72.44.\n",
      "## [BEST epoch], 74 seconds.\n",
      "\n",
      "-- train epoch 6/100, batch 261/261 (100.00%), loss = 301.25.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 6/100 \"f1-alpha-match-10\" train / dev / test | 77.64 / 76.34 / 76.34.\n",
      "## [BEST epoch], 73 seconds.\n",
      "\n",
      "-- train epoch 7/100, batch 261/261 (100.00%), loss = 250.16.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 7/100 \"f1-alpha-match-10\" train / dev / test | 76.98 / 75.18 / 75.18.\n",
      "## [no improvement micro-f1 on DEV during the last 1 epochs (best_f1_dev=76.34), 71 seconds].\n",
      "\n",
      "-- train epoch 8/100, batch 261/261 (100.00%), loss = 220.17.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 8/100 \"f1-alpha-match-10\" train / dev / test | 79.53 / 77.39 / 77.39.\n",
      "## [BEST epoch], 73 seconds.\n",
      "\n",
      "-- train epoch 9/100, batch 261/261 (100.00%), loss = 221.51.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 9/100 \"f1-alpha-match-10\" train / dev / test | 80.71 / 78.51 / 78.51.\n",
      "## [BEST epoch], 74 seconds.\n",
      "\n",
      "-- train epoch 10/100, batch 261/261 (100.00%), loss = 228.58.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 10/100 \"f1-alpha-match-10\" train / dev / test | 80.51 / 78.53 / 78.53.\n",
      "## [BEST epoch], 67 seconds.\n",
      "\n",
      "-- train epoch 11/100, batch 261/261 (100.00%), loss = 198.93.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 11/100 \"f1-alpha-match-10\" train / dev / test | 81.92 / 78.74 / 78.74.\n",
      "## [BEST epoch], 71 seconds.\n",
      "\n",
      "-- train epoch 12/100, batch 261/261 (100.00%), loss = 221.02.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 12/100 \"f1-alpha-match-10\" train / dev / test | 84.32 / 80.58 / 80.58.\n",
      "## [BEST epoch], 72 seconds.\n",
      "\n",
      "-- train epoch 13/100, batch 261/261 (100.00%), loss = 199.78.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 13/100 \"f1-alpha-match-10\" train / dev / test | 84.21 / 80.75 / 80.75.\n",
      "## [BEST epoch], 72 seconds.\n",
      "\n",
      "-- train epoch 14/100, batch 261/261 (100.00%), loss = 181.75.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 14/100 \"f1-alpha-match-10\" train / dev / test | 83.41 / 80.27 / 80.27.\n",
      "## [no improvement micro-f1 on DEV during the last 1 epochs (best_f1_dev=80.75), 63 seconds].\n",
      "\n",
      "-- train epoch 15/100, batch 261/261 (100.00%), loss = 153.56.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 15/100 \"f1-alpha-match-10\" train / dev / test | 84.70 / 81.14 / 81.14.\n",
      "## [BEST epoch], 71 seconds.\n",
      "\n",
      "-- train epoch 16/100, batch 261/261 (100.00%), loss = 190.08.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 16/100 \"f1-alpha-match-10\" train / dev / test | 85.51 / 82.05 / 82.05.\n",
      "## [BEST epoch], 71 seconds.\n",
      "\n",
      "-- train epoch 17/100, batch 261/261 (100.00%), loss = 179.52.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 17/100 \"f1-alpha-match-10\" train / dev / test | 85.87 / 82.06 / 82.06.\n",
      "## [BEST epoch], 67 seconds.\n",
      "\n",
      "-- train epoch 18/100, batch 261/261 (100.00%), loss = 188.47.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 18/100 \"f1-alpha-match-10\" train / dev / test | 86.33 / 82.51 / 82.51.\n",
      "## [BEST epoch], 71 seconds.\n",
      "\n",
      "-- train epoch 19/100, batch 261/261 (100.00%), loss = 138.68.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 19/100 \"f1-alpha-match-10\" train / dev / test | 86.83 / 82.65 / 82.65.\n",
      "## [BEST epoch], 67 seconds.\n",
      "\n",
      "-- train epoch 20/100, batch 261/261 (100.00%), loss = 152.09.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 20/100 \"f1-alpha-match-10\" train / dev / test | 87.10 / 82.42 / 82.42.\n",
      "## [no improvement micro-f1 on DEV during the last 1 epochs (best_f1_dev=82.65), 72 seconds].\n",
      "\n",
      "-- train epoch 21/100, batch 261/261 (100.00%), loss = 160.41.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 21/100 \"f1-alpha-match-10\" train / dev / test | 87.50 / 82.73 / 82.73.\n",
      "## [BEST epoch], 71 seconds.\n",
      "\n",
      "-- train epoch 22/100, batch 261/261 (100.00%), loss = 171.16.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 22/100 \"f1-alpha-match-10\" train / dev / test | 87.50 / 82.93 / 82.93.\n",
      "## [BEST epoch], 68 seconds.\n",
      "\n",
      "-- train epoch 23/100, batch 261/261 (100.00%), loss = 157.21.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 23/100 \"f1-alpha-match-10\" train / dev / test | 89.01 / 82.67 / 82.67.\n",
      "## [no improvement micro-f1 on DEV during the last 1 epochs (best_f1_dev=82.93), 68 seconds].\n",
      "\n",
      "-- train epoch 24/100, batch 261/261 (100.00%), loss = 148.83.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 24/100 \"f1-alpha-match-10\" train / dev / test | 88.11 / 81.74 / 81.74.\n",
      "## [no improvement micro-f1 on DEV during the last 2 epochs (best_f1_dev=82.93), 69 seconds].\n",
      "\n",
      "-- train epoch 25/100, batch 261/261 (100.00%), loss = 142.88.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 25/100 \"f1-alpha-match-10\" train / dev / test | 89.19 / 83.50 / 83.50.\n",
      "## [BEST epoch], 71 seconds.\n",
      "\n",
      "-- train epoch 26/100, batch 261/261 (100.00%), loss = 129.07.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 26/100 \"f1-alpha-match-10\" train / dev / test | 89.51 / 83.66 / 83.66.\n",
      "## [BEST epoch], 71 seconds.\n",
      "\n",
      "-- train epoch 27/100, batch 261/261 (100.00%), loss = 123.05.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 27/100 \"f1-alpha-match-10\" train / dev / test | 88.88 / 82.74 / 82.74.\n",
      "## [no improvement micro-f1 on DEV during the last 1 epochs (best_f1_dev=83.66), 68 seconds].\n",
      "\n",
      "-- train epoch 28/100, batch 261/261 (100.00%), loss = 122.57.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 28/100 \"f1-alpha-match-10\" train / dev / test | 89.96 / 83.32 / 83.32.\n",
      "## [no improvement micro-f1 on DEV during the last 2 epochs (best_f1_dev=83.66), 62 seconds].\n",
      "\n",
      "-- train epoch 29/100, batch 261/261 (100.00%), loss = 133.82.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 29/100 \"f1-alpha-match-10\" train / dev / test | 89.92 / 83.82 / 83.82.\n",
      "## [BEST epoch], 69 seconds.\n",
      "\n",
      "-- train epoch 30/100, batch 261/261 (100.00%), loss = 123.55.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 30/100 \"f1-alpha-match-10\" train / dev / test | 90.04 / 82.99 / 82.99.\n",
      "## [no improvement micro-f1 on DEV during the last 1 epochs (best_f1_dev=83.82), 64 seconds].\n",
      "\n",
      "-- train epoch 31/100, batch 261/261 (100.00%), loss = 112.52.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 31/100 \"f1-alpha-match-10\" train / dev / test | 90.58 / 83.10 / 83.10.\n",
      "## [no improvement micro-f1 on DEV during the last 2 epochs (best_f1_dev=83.82), 64 seconds].\n",
      "\n",
      "-- train epoch 32/100, batch 261/261 (100.00%), loss = 99.79.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 32/100 \"f1-alpha-match-10\" train / dev / test | 90.62 / 83.30 / 83.30.\n",
      "## [no improvement micro-f1 on DEV during the last 3 epochs (best_f1_dev=83.82), 63 seconds].\n",
      "\n",
      "-- train epoch 33/100, batch 261/261 (100.00%), loss = 121.00.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 33/100 \"f1-alpha-match-10\" train / dev / test | 90.76 / 82.75 / 82.75.\n",
      "## [no improvement micro-f1 on DEV during the last 4 epochs (best_f1_dev=83.82), 66 seconds].\n",
      "\n",
      "-- train epoch 34/100, batch 261/261 (100.00%), loss = 102.97.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 34/100 \"f1-alpha-match-10\" train / dev / test | 91.08 / 82.92 / 82.92.\n",
      "## [no improvement micro-f1 on DEV during the last 5 epochs (best_f1_dev=83.82), 69 seconds].\n",
      "\n",
      "-- train epoch 35/100, batch 261/261 (100.00%), loss = 119.15.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 35/100 \"f1-alpha-match-10\" train / dev / test | 91.61 / 82.79 / 82.79.\n",
      "## [no improvement micro-f1 on DEV during the last 6 epochs (best_f1_dev=83.82), 70 seconds].\n",
      "\n",
      "-- train epoch 36/100, batch 261/261 (100.00%), loss = 110.34.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 36/100 \"f1-alpha-match-10\" train / dev / test | 91.58 / 83.79 / 83.79.\n",
      "## [no improvement micro-f1 on DEV during the last 7 epochs (best_f1_dev=83.82), 69 seconds].\n",
      "\n",
      "-- train epoch 37/100, batch 261/261 (100.00%), loss = 121.83.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 37/100 \"f1-alpha-match-10\" train / dev / test | 91.49 / 82.79 / 82.79.\n",
      "## [no improvement micro-f1 on DEV during the last 8 epochs (best_f1_dev=83.82), 70 seconds].\n",
      "\n",
      "-- train epoch 38/100, batch 261/261 (100.00%), loss = 114.21.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 38/100 \"f1-alpha-match-10\" train / dev / test | 92.35 / 83.30 / 83.30.\n",
      "## [no improvement micro-f1 on DEV during the last 9 epochs (best_f1_dev=83.82), 73 seconds].\n",
      "\n",
      "-- train epoch 39/100, batch 261/261 (100.00%), loss = 117.00.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 39/100 \"f1-alpha-match-10\" train / dev / test | 92.31 / 84.83 / 84.83.\n",
      "## [BEST epoch], 73 seconds.\n",
      "\n",
      "-- train epoch 40/100, batch 261/261 (100.00%), loss = 97.10.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 40/100 \"f1-alpha-match-10\" train / dev / test | 92.69 / 83.19 / 83.19.\n",
      "## [no improvement micro-f1 on DEV during the last 1 epochs (best_f1_dev=84.83), 72 seconds].\n",
      "\n",
      "-- train epoch 41/100, batch 261/261 (100.00%), loss = 94.96.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 41/100 \"f1-alpha-match-10\" train / dev / test | 92.77 / 84.27 / 84.27.\n",
      "## [no improvement micro-f1 on DEV during the last 2 epochs (best_f1_dev=84.83), 67 seconds].\n",
      "\n",
      "-- train epoch 42/100, batch 261/261 (100.00%), loss = 113.93.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 42/100 \"f1-alpha-match-10\" train / dev / test | 92.10 / 82.92 / 82.92.\n",
      "## [no improvement micro-f1 on DEV during the last 3 epochs (best_f1_dev=84.83), 70 seconds].\n",
      "\n",
      "-- train epoch 43/100, batch 261/261 (100.00%), loss = 95.82.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 43/100 \"f1-alpha-match-10\" train / dev / test | 92.90 / 83.62 / 83.62.\n",
      "## [no improvement micro-f1 on DEV during the last 4 epochs (best_f1_dev=84.83), 69 seconds].\n",
      "\n",
      "-- train epoch 44/100, batch 261/261 (100.00%), loss = 96.58.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 44/100 \"f1-alpha-match-10\" train / dev / test | 93.39 / 84.31 / 84.31.\n",
      "## [no improvement micro-f1 on DEV during the last 5 epochs (best_f1_dev=84.83), 68 seconds].\n",
      "\n",
      "-- train epoch 45/100, batch 261/261 (100.00%), loss = 96.02.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 45/100 \"f1-alpha-match-10\" train / dev / test | 93.06 / 83.96 / 83.96.\n",
      "## [no improvement micro-f1 on DEV during the last 6 epochs (best_f1_dev=84.83), 72 seconds].\n",
      "\n",
      "-- train epoch 46/100, batch 261/261 (100.00%), loss = 108.50.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 46/100 \"f1-alpha-match-10\" train / dev / test | 93.27 / 84.47 / 84.47.\n",
      "## [no improvement micro-f1 on DEV during the last 7 epochs (best_f1_dev=84.83), 72 seconds].\n",
      "\n",
      "-- train epoch 47/100, batch 261/261 (100.00%), loss = 106.51.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 47/100 \"f1-alpha-match-10\" train / dev / test | 93.14 / 84.07 / 84.07.\n",
      "## [no improvement micro-f1 on DEV during the last 8 epochs (best_f1_dev=84.83), 72 seconds].\n",
      "\n",
      "-- train epoch 48/100, batch 261/261 (100.00%), loss = 102.06.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 48/100 \"f1-alpha-match-10\" train / dev / test | 93.64 / 84.04 / 84.04.\n",
      "## [no improvement micro-f1 on DEV during the last 9 epochs (best_f1_dev=84.83), 69 seconds].\n",
      "\n",
      "-- train epoch 49/100, batch 261/261 (100.00%), loss = 97.60.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 49/100 \"f1-alpha-match-10\" train / dev / test | 94.11 / 84.24 / 84.24.\n",
      "## [no improvement micro-f1 on DEV during the last 10 epochs (best_f1_dev=84.83), 69 seconds].\n",
      "\n",
      "-- train epoch 50/100, batch 261/261 (100.00%), loss = 92.51.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 50/100 \"f1-alpha-match-10\" train / dev / test | 93.97 / 84.08 / 84.08.\n",
      "## [no improvement micro-f1 on DEV during the last 11 epochs (best_f1_dev=84.83), 67 seconds].\n",
      "\n",
      "-- train epoch 51/100, batch 261/261 (100.00%), loss = 91.43.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 51/100 \"f1-alpha-match-10\" train / dev / test | 94.32 / 85.04 / 85.04.\n",
      "## [BEST epoch], 70 seconds.\n",
      "\n",
      "-- train epoch 52/100, batch 261/261 (100.00%), loss = 88.85.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 52/100 \"f1-alpha-match-10\" train / dev / test | 94.63 / 85.07 / 85.07.\n",
      "## [BEST epoch], 67 seconds.\n",
      "\n",
      "-- train epoch 53/100, batch 261/261 (100.00%), loss = 73.28.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 53/100 \"f1-alpha-match-10\" train / dev / test | 93.51 / 84.07 / 84.07.\n",
      "## [no improvement micro-f1 on DEV during the last 1 epochs (best_f1_dev=85.07), 70 seconds].\n",
      "\n",
      "-- train epoch 54/100, batch 261/261 (100.00%), loss = 94.17.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 54/100 \"f1-alpha-match-10\" train / dev / test | 94.92 / 84.98 / 84.98.\n",
      "## [no improvement micro-f1 on DEV during the last 2 epochs (best_f1_dev=85.07), 73 seconds].\n",
      "\n",
      "-- train epoch 55/100, batch 261/261 (100.00%), loss = 90.98.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 55/100 \"f1-alpha-match-10\" train / dev / test | 94.52 / 84.40 / 84.40.\n",
      "## [no improvement micro-f1 on DEV during the last 3 epochs (best_f1_dev=85.07), 72 seconds].\n",
      "\n",
      "-- train epoch 56/100, batch 261/261 (100.00%), loss = 69.39.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 56/100 \"f1-alpha-match-10\" train / dev / test | 94.66 / 84.66 / 84.66.\n",
      "## [no improvement micro-f1 on DEV during the last 4 epochs (best_f1_dev=85.07), 70 seconds].\n",
      "\n",
      "-- train epoch 57/100, batch 261/261 (100.00%), loss = 81.71.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 57/100 \"f1-alpha-match-10\" train / dev / test | 94.62 / 85.06 / 85.06.\n",
      "## [no improvement micro-f1 on DEV during the last 5 epochs (best_f1_dev=85.07), 70 seconds].\n",
      "\n",
      "-- train epoch 58/100, batch 261/261 (100.00%), loss = 78.84.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 58/100 \"f1-alpha-match-10\" train / dev / test | 94.92 / 83.56 / 83.56.\n",
      "## [no improvement micro-f1 on DEV during the last 6 epochs (best_f1_dev=85.07), 69 seconds].\n",
      "\n",
      "-- train epoch 59/100, batch 261/261 (100.00%), loss = 83.32.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 59/100 \"f1-alpha-match-10\" train / dev / test | 94.87 / 84.47 / 84.47.\n",
      "## [no improvement micro-f1 on DEV during the last 7 epochs (best_f1_dev=85.07), 69 seconds].\n",
      "\n",
      "-- train epoch 60/100, batch 261/261 (100.00%), loss = 82.65.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 60/100 \"f1-alpha-match-10\" train / dev / test | 95.05 / 84.93 / 84.93.\n",
      "## [no improvement micro-f1 on DEV during the last 8 epochs (best_f1_dev=85.07), 70 seconds].\n",
      "\n",
      "-- train epoch 61/100, batch 261/261 (100.00%), loss = 64.26.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 61/100 \"f1-alpha-match-10\" train / dev / test | 95.02 / 84.74 / 84.74.\n",
      "## [no improvement micro-f1 on DEV during the last 9 epochs (best_f1_dev=85.07), 68 seconds].\n",
      "\n",
      "-- train epoch 62/100, batch 261/261 (100.00%), loss = 90.81.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 62/100 \"f1-alpha-match-10\" train / dev / test | 94.59 / 84.58 / 84.58.\n",
      "## [no improvement micro-f1 on DEV during the last 10 epochs (best_f1_dev=85.07), 68 seconds].\n",
      "\n",
      "-- train epoch 63/100, batch 261/261 (100.00%), loss = 77.32.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 63/100 \"f1-alpha-match-10\" train / dev / test | 95.50 / 84.52 / 84.52.\n",
      "## [no improvement micro-f1 on DEV during the last 11 epochs (best_f1_dev=85.07), 69 seconds].\n",
      "\n",
      "-- train epoch 64/100, batch 261/261 (100.00%), loss = 86.82.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 64/100 \"f1-alpha-match-10\" train / dev / test | 95.48 / 84.06 / 84.06.\n",
      "## [no improvement micro-f1 on DEV during the last 12 epochs (best_f1_dev=85.07), 69 seconds].\n",
      "\n",
      "-- train epoch 65/100, batch 261/261 (100.00%), loss = 65.77.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 65/100 \"f1-alpha-match-10\" train / dev / test | 95.64 / 84.29 / 84.29.\n",
      "## [no improvement micro-f1 on DEV during the last 13 epochs (best_f1_dev=85.07), 64 seconds].\n",
      "\n",
      "-- train epoch 66/100, batch 261/261 (100.00%), loss = 82.67.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 66/100 \"f1-alpha-match-10\" train / dev / test | 95.49 / 84.61 / 84.61.\n",
      "## [no improvement micro-f1 on DEV during the last 14 epochs (best_f1_dev=85.07), 68 seconds].\n",
      "\n",
      "-- train epoch 67/100, batch 261/261 (100.00%), loss = 78.83.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 67/100 \"f1-alpha-match-10\" train / dev / test | 95.16 / 84.77 / 84.77.\n",
      "## [no improvement micro-f1 on DEV during the last 15 epochs (best_f1_dev=85.07), 69 seconds].\n",
      "\n",
      "-- train epoch 68/100, batch 261/261 (100.00%), loss = 80.57.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 68/100 \"f1-alpha-match-10\" train / dev / test | 95.31 / 84.56 / 84.56.\n",
      "## [no improvement micro-f1 on DEV during the last 16 epochs (best_f1_dev=85.07), 64 seconds].\n",
      "\n",
      "-- train epoch 69/100, batch 261/261 (100.00%), loss = 72.96.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 69/100 \"f1-alpha-match-10\" train / dev / test | 95.56 / 84.60 / 84.60.\n",
      "## [no improvement micro-f1 on DEV during the last 17 epochs (best_f1_dev=85.07), 66 seconds].\n",
      "\n",
      "-- train epoch 70/100, batch 261/261 (100.00%), loss = 79.23.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 70/100 \"f1-alpha-match-10\" train / dev / test | 95.49 / 84.55 / 84.55.\n",
      "## [no improvement micro-f1 on DEV during the last 18 epochs (best_f1_dev=85.07), 69 seconds].\n",
      "\n",
      "-- train epoch 71/100, batch 261/261 (100.00%), loss = 71.18.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 71/100 \"f1-alpha-match-10\" train / dev / test | 95.55 / 84.34 / 84.34.\n",
      "## [no improvement micro-f1 on DEV during the last 19 epochs (best_f1_dev=85.07), 71 seconds].\n",
      "\n",
      "-- train epoch 72/100, batch 261/261 (100.00%), loss = 68.98.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 72/100 \"f1-alpha-match-10\" train / dev / test | 95.71 / 84.57 / 84.57.\n",
      "## [no improvement micro-f1 on DEV during the last 20 epochs (best_f1_dev=85.07), 65 seconds].\n",
      "\n",
      "-- train epoch 73/100, batch 261/261 (100.00%), loss = 63.54.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 73/100 \"f1-alpha-match-10\" train / dev / test | 95.56 / 84.14 / 84.14.\n",
      "## [no improvement micro-f1 on DEV during the last 21 epochs (best_f1_dev=85.07), 69 seconds].\n",
      "\n",
      "Evaluation\n",
      "\n",
      "batch_size=10\n",
      "char_cnn_filter_num=30\n",
      "char_embeddings_dim=25\n",
      "char_window_size=3\n",
      "check_for_lowercase=True\n",
      "clip_grad=5\n",
      "cross_fold_id=-1\n",
      "cross_folds_num=-1\n",
      "data_io='connl-ner-2003'\n",
      "dataset_sort=False\n",
      "dev='/home/vika/NER_RNN/targer/data/NER/Asqua_CAM_aspects/CAM_test.csv'\n",
      "dropout_ratio=0.5\n",
      "emb_delimiter=' '\n",
      "emb_dim=100\n",
      "emb_fn='embeddings/glove.6B.100d.txt'\n",
      "emb_load_all=False\n",
      "epoch_num=100\n",
      "evaluator='f1-alpha-match-10'\n",
      "freeze_char_embeddings=False\n",
      "freeze_word_embeddings=False\n",
      "gpu=1\n",
      "load=None\n",
      "lr=0.001\n",
      "lr_decay=0.05\n",
      "min_epoch_num=50\n",
      "model='BiRNNCRF'\n",
      "momentum=0.9\n",
      "opt='adam'\n",
      "patience=20\n",
      "report_fn='2019_08_22_12-07_55_report.txt'\n",
      "rnn_hidden_dim=200\n",
      "rnn_type='LSTM'\n",
      "save='BiRNNCFR.hdf5'\n",
      "save_best=True\n",
      "seed_num=42\n",
      "test='/home/vika/NER_RNN/targer/data/NER/Asqua_CAM_aspects/CAM_test.csv'\n",
      "train='/home/vika/NER_RNN/targer/data/NER/Asqua_CAM_aspects/CAM_train.csv'\n",
      "verbose=True\n",
      "word_len=20\n",
      "word_seq_indexer=None\n",
      "\n",
      "         epoch  |     train loss | f1-alpha-match-10-train | f1-alpha-match-10-dev | f1-alpha-match-10-test \n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "              0 |           0.00 |           3.48 |           3.55 |           3.55 \n",
      "              1 |         709.25 |          60.42 |          59.59 |          59.59 \n",
      "              2 |         394.92 |          69.55 |          66.87 |          66.87 \n",
      "              3 |         341.32 |          72.22 |          67.51 |          67.51 \n",
      "              4 |         328.28 |          74.17 |          71.47 |          71.47 \n",
      "              5 |         305.78 |          75.36 |          72.44 |          72.44 \n",
      "              6 |         301.25 |          77.64 |          76.34 |          76.34 \n",
      "              7 |         250.16 |          76.98 |          75.18 |          75.18 \n",
      "              8 |         220.17 |          79.53 |          77.39 |          77.39 \n",
      "              9 |         221.51 |          80.71 |          78.51 |          78.51 \n",
      "             10 |         228.58 |          80.51 |          78.53 |          78.53 \n",
      "             11 |         198.93 |          81.92 |          78.74 |          78.74 \n",
      "             12 |         221.02 |          84.32 |          80.58 |          80.58 \n",
      "             13 |         199.78 |          84.21 |          80.75 |          80.75 \n",
      "             14 |         181.75 |          83.41 |          80.27 |          80.27 \n",
      "             15 |         153.56 |          84.70 |          81.14 |          81.14 \n",
      "             16 |         190.08 |          85.51 |          82.05 |          82.05 \n",
      "             17 |         179.52 |          85.87 |          82.06 |          82.06 \n",
      "             18 |         188.47 |          86.33 |          82.51 |          82.51 \n",
      "             19 |         138.68 |          86.83 |          82.65 |          82.65 \n",
      "             20 |         152.09 |          87.10 |          82.42 |          82.42 \n",
      "             21 |         160.41 |          87.50 |          82.73 |          82.73 \n",
      "             22 |         171.16 |          87.50 |          82.93 |          82.93 \n",
      "             23 |         157.21 |          89.01 |          82.67 |          82.67 \n",
      "             24 |         148.83 |          88.11 |          81.74 |          81.74 \n",
      "             25 |         142.88 |          89.19 |          83.50 |          83.50 \n",
      "             26 |         129.07 |          89.51 |          83.66 |          83.66 \n",
      "             27 |         123.05 |          88.88 |          82.74 |          82.74 \n",
      "             28 |         122.57 |          89.96 |          83.32 |          83.32 \n",
      "             29 |         133.82 |          89.92 |          83.82 |          83.82 \n",
      "             30 |         123.55 |          90.04 |          82.99 |          82.99 \n",
      "             31 |         112.52 |          90.58 |          83.10 |          83.10 \n",
      "             32 |          99.79 |          90.62 |          83.30 |          83.30 \n",
      "             33 |         121.00 |          90.76 |          82.75 |          82.75 \n",
      "             34 |         102.97 |          91.08 |          82.92 |          82.92 \n",
      "             35 |         119.15 |          91.61 |          82.79 |          82.79 \n",
      "             36 |         110.34 |          91.58 |          83.79 |          83.79 \n",
      "             37 |         121.83 |          91.49 |          82.79 |          82.79 \n",
      "             38 |         114.21 |          92.35 |          83.30 |          83.30 \n",
      "             39 |         117.00 |          92.31 |          84.83 |          84.83 \n",
      "             40 |          97.10 |          92.69 |          83.19 |          83.19 \n",
      "             41 |          94.96 |          92.77 |          84.27 |          84.27 \n",
      "             42 |         113.93 |          92.10 |          82.92 |          82.92 \n",
      "             43 |          95.82 |          92.90 |          83.62 |          83.62 \n",
      "             44 |          96.58 |          93.39 |          84.31 |          84.31 \n",
      "             45 |          96.02 |          93.06 |          83.96 |          83.96 \n",
      "             46 |         108.50 |          93.27 |          84.47 |          84.47 \n",
      "             47 |         106.51 |          93.14 |          84.07 |          84.07 \n",
      "             48 |         102.06 |          93.64 |          84.04 |          84.04 \n",
      "             49 |          97.60 |          94.11 |          84.24 |          84.24 \n",
      "             50 |          92.51 |          93.97 |          84.08 |          84.08 \n",
      "             51 |          91.43 |          94.32 |          85.04 |          85.04 \n",
      "             52 |          88.85 |          94.63 |          85.07 |          85.07 \n",
      "             53 |          73.28 |          93.51 |          84.07 |          84.07 \n",
      "             54 |          94.17 |          94.92 |          84.98 |          84.98 \n",
      "             55 |          90.98 |          94.52 |          84.40 |          84.40 \n",
      "             56 |          69.39 |          94.66 |          84.66 |          84.66 \n",
      "             57 |          81.71 |          94.62 |          85.06 |          85.06 \n",
      "             58 |          78.84 |          94.92 |          83.56 |          83.56 \n",
      "             59 |          83.32 |          94.87 |          84.47 |          84.47 \n",
      "             60 |          82.65 |          95.05 |          84.93 |          84.93 \n",
      "             61 |          64.26 |          95.02 |          84.74 |          84.74 \n",
      "             62 |          90.81 |          94.59 |          84.58 |          84.58 \n",
      "             63 |          77.32 |          95.50 |          84.52 |          84.52 \n",
      "             64 |          86.82 |          95.48 |          84.06 |          84.06 \n",
      "             65 |          65.77 |          95.64 |          84.29 |          84.29 \n",
      "             66 |          82.67 |          95.49 |          84.61 |          84.61 \n",
      "             67 |          78.83 |          95.16 |          84.77 |          84.77 \n",
      "             68 |          80.57 |          95.31 |          84.56 |          84.56 \n",
      "             69 |          72.96 |          95.56 |          84.60 |          84.60 \n",
      "             70 |          79.23 |          95.49 |          84.55 |          84.55 \n",
      "             71 |          71.18 |          95.55 |          84.34 |          84.34 \n",
      "             72 |          68.98 |          95.71 |          84.57 |          84.57 \n",
      "             73 |          63.54 |          95.56 |          84.14 |          84.14 \n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Final eval on test, \"save best\", best epoch on dev 52, f1-alpha-match-10, test = 85.07)\n",
      "--------------------------------------------------------------------------------------------------------------*** f1 alpha match, alpha = 1.0\n",
      "*** f1 = 85.07, precision = 87.37, recall = 82.89\n",
      "*** TP = 1114, FP = 161, FN = 230\n",
      "Input arguments:\n",
      "python3 main.py --train /home/vika/NER_RNN/targer/data/NER/Asqua_CAM_aspects/CAM_train.csv --dev /home/vika/NER_RNN/targer/data/NER/Asqua_CAM_aspects/CAM_test.csv --test /home/vika/NER_RNN/targer/data/NER/Asqua_CAM_aspects/CAM_dev.csv --data-io connl-ner-2003 --evaluator f1-alpha-match-10 --model BiRNNCRF --opt adam --lr 0.001 --save-best yes --patience 20 --rnn-hidden-dim 200 --gpu 1 --test /home/vika/NER_RNN/targer/data/NER/Asqua_CAM_aspects/CAM_test.csv --save BiRNNCFR.hdf5\n",
      "\n",
      "85.0706\n"
     ]
    }
   ],
   "source": [
    "! python3 main.py --train \"/home/vika/NER_RNN/targer/data/NER/Asqua_CAM_aspects/CAM_train.csv\" --dev \"/home/vika/NER_RNN/targer/data/NER/Asqua_CAM_aspects/CAM_test.csv\" --test \"/home/vika/NER_RNN/targer/data/NER/Asqua_CAM_aspects/CAM_dev.csv\" --data-io connl-ner-2003 --evaluator f1-alpha-match-10 --model BiRNNCRF --opt adam --lr 0.001 --save-best yes --patience 20 --rnn-hidden-dim 200 --gpu 1 --test \"/home/vika/NER_RNN/targer/data/NER/Asqua_CAM_aspects/CAM_test.csv\" --save \"BiRNNCFR.hdf5\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "args train = /home/vika/NER_RNN/targer/data/NER/Asqua_CAM_aspects/CAM_train.csv\n",
      "Loading from /home/vika/NER_RNN/targer/data/NER/Asqua_CAM_aspects/CAM_train.csv: 2619 samples, 74616 words.\n",
      "Loading from /home/vika/NER_RNN/targer/data/NER/Asqua_CAM_aspects/CAM_dev.csv: 350 samples, 10092 words.\n",
      "Loading from /home/vika/NER_RNN/targer/data/NER/Asqua_CAM_aspects/CAM_test.csv: 523 samples, 14728 words.\n",
      "DatasetsBank: len(unique_words_list) = 7143 unique words.\n",
      "DatasetsBank: len(unique_words_list) = 7600 unique words.\n",
      "DatasetsBank: len(unique_words_list) = 8167 unique words.\n",
      "\n",
      "load_vocabulary_from_tag_sequences:\n",
      " -- class_num = 7\n",
      " -- {'<pad>': 0, 'O': 1, 'B-OTHOBJ': 2, 'B-ASPOBJ': 3, 'B-ASP': 4, 'I-ASP': 5, 'I-ASPOBJ': 6, 'I-OTHOBJ': 7}\n",
      "in main\n",
      "True\n",
      "/home/vika/NER_RNN/targer/embeddings/elmo_2x4096_512_2048cnn_2xhighway_5.5B_weights.hdf5\n",
      "/home/vika/NER_RNN/targer/embeddings/elmo_2x4096_512_2048cnn_2xhighway_5.5B_options.json\n",
      "init targer base\n",
      "True\n",
      "/home/vika/NER_RNN/targer/embeddings/elmo_2x4096_512_2048cnn_2xhighway_5.5B_weights.hdf5\n",
      "/home/vika/NER_RNN/targer/embeddings/elmo_2x4096_512_2048cnn_2xhighway_5.5B_options.json\n",
      "elmo is initiated\n",
      "init targer BiRNNCNNCRF\n",
      "True\n",
      "/home/vika/NER_RNN/targer/embeddings/elmo_2x4096_512_2048cnn_2xhighway_5.5B_weights.hdf5\n",
      "/home/vika/NER_RNN/targer/embeddings/elmo_2x4096_512_2048cnn_2xhighway_5.5B_options.json\n",
      "Empirical transition matrix from the train dataset:\n",
      "               <pad>         O  B-OTHOBJ  B-ASPOBJ     B-ASP     I-ASP  I-ASPOBJ  I-OTHOBJ     <sos>\n",
      "\n",
      "     <pad>       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0\n",
      "         O       0.0   58639.0    2722.0    2863.0     792.0     165.0      22.0      21.0    2226.0\n",
      "  B-OTHOBJ       0.0    2713.0       0.0       1.0       3.0       1.0       0.0       0.0      47.0\n",
      "  B-ASPOBJ       0.0    2560.0       8.0       0.0       4.0       3.0       0.0       0.0     341.0\n",
      "     B-ASP       0.0     919.0      14.0      28.0       0.0       0.0       0.0       0.0       5.0\n",
      "     I-ASP       0.0       0.0       0.0       2.0     167.0     307.0       0.0       0.0       0.0\n",
      "  I-ASPOBJ       0.0       0.0       0.0      22.0       0.0       0.0       0.0       0.0       0.0\n",
      "  I-OTHOBJ       0.0       0.0      21.0       0.0       0.0       0.0       0.0       0.0       0.0\n",
      "     <sos>       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0\n",
      "\n",
      "Initialized transition matrix:\n",
      "               <pad>         O  B-OTHOBJ  B-ASPOBJ     B-ASP     I-ASP  I-ASPOBJ  I-OTHOBJ     <sos>\n",
      "\n",
      "     <pad>   -9999.0   -9999.0   -9999.0   -9999.0   -9999.0   -9999.0   -9999.0   -9999.0   -9999.0\n",
      "         O   -9999.0      -1.0      -0.9      -1.1      -1.0      -1.1      -1.1      -1.0      -1.0\n",
      "  B-OTHOBJ   -9999.0      -1.0   -9999.0      -0.8      -1.1      -1.1   -9999.0   -9999.0      -1.1\n",
      "  B-ASPOBJ   -9999.0      -1.0      -1.0   -9999.0      -1.2      -0.9   -9999.0   -9999.0      -0.9\n",
      "     B-ASP   -9999.0      -1.0      -0.8      -1.1   -9999.0   -9999.0   -9999.0   -9999.0      -1.0\n",
      "     I-ASP   -9999.0   -9999.0   -9999.0      -1.1      -1.0      -0.9   -9999.0   -9999.0   -9999.0\n",
      "  I-ASPOBJ   -9999.0   -9999.0   -9999.0      -0.9   -9999.0   -9999.0   -9999.0   -9999.0   -9999.0\n",
      "  I-OTHOBJ   -9999.0   -9999.0      -0.9   -9999.0   -9999.0   -9999.0   -9999.0   -9999.0   -9999.0\n",
      "     <sos>   -9999.0   -9999.0   -9999.0   -9999.0   -9999.0   -9999.0   -9999.0   -9999.0   -9999.0\n",
      "\n",
      "Start training...\n",
      "\n",
      "\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 0/50 \"f1-alpha-match-10\" train / dev / test | 4.90 / 4.60 / 4.71.\n",
      "## [BEST epoch], 18 seconds.\n",
      "\n",
      "-- train epoch 1/50, batch 261/261 (100.00%), loss = 481.85.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 1/50 \"f1-alpha-match-10\" train / dev / test | 59.00 / 55.35 / 57.57.\n",
      "## [BEST epoch], 105 seconds.\n",
      "\n",
      "-- train epoch 2/50, batch 261/261 (100.00%), loss = 295.73.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 2/50 \"f1-alpha-match-10\" train / dev / test | 62.07 / 57.16 / 60.13.\n",
      "## [BEST epoch], 102 seconds.\n",
      "\n",
      "-- train epoch 3/50, batch 261/261 (100.00%), loss = 269.04.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 3/50 \"f1-alpha-match-10\" train / dev / test | 68.30 / 64.60 / 67.97.\n",
      "## [BEST epoch], 105 seconds.\n",
      "\n",
      "-- train epoch 4/50, batch 261/261 (100.00%), loss = 251.46.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 4/50 \"f1-alpha-match-10\" train / dev / test | 68.73 / 61.71 / 64.08.\n",
      "## [no improvement micro-f1 on DEV during the last 1 epochs (best_f1_dev=64.60), 103 seconds].\n",
      "\n",
      "-- train epoch 5/50, batch 261/261 (100.00%), loss = 236.51.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 5/50 \"f1-alpha-match-10\" train / dev / test | 72.35 / 67.72 / 67.84.\n",
      "## [BEST epoch], 106 seconds.\n",
      "\n",
      "-- train epoch 6/50, batch 261/261 (100.00%), loss = 199.35.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 6/50 \"f1-alpha-match-10\" train / dev / test | 76.23 / 70.89 / 73.64.\n",
      "## [BEST epoch], 102 seconds.\n",
      "\n",
      "-- train epoch 7/50, batch 261/261 (100.00%), loss = 191.80.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 7/50 \"f1-alpha-match-10\" train / dev / test | 77.89 / 71.71 / 73.26.\n",
      "## [BEST epoch], 105 seconds.\n",
      "\n",
      "-- train epoch 8/50, batch 261/261 (100.00%), loss = 194.80.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 8/50 \"f1-alpha-match-10\" train / dev / test | 80.15 / 72.80 / 74.81.\n",
      "## [BEST epoch], 105 seconds.\n",
      "\n",
      "-- train epoch 9/50, batch 261/261 (100.00%), loss = 182.43.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 9/50 \"f1-alpha-match-10\" train / dev / test | 82.59 / 75.04 / 76.26.\n",
      "## [BEST epoch], 103 seconds.\n",
      "\n",
      "-- train epoch 10/50, batch 261/261 (100.00%), loss = 161.46.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 10/50 \"f1-alpha-match-10\" train / dev / test | 81.40 / 74.15 / 76.13.\n",
      "## [no improvement micro-f1 on DEV during the last 1 epochs (best_f1_dev=75.04), 103 seconds].\n",
      "\n",
      "-- train epoch 11/50, batch 261/261 (100.00%), loss = 155.96.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 11/50 \"f1-alpha-match-10\" train / dev / test | 83.04 / 75.93 / 75.98.\n",
      "## [BEST epoch], 102 seconds.\n",
      "\n",
      "-- train epoch 12/50, batch 261/261 (100.00%), loss = 133.37.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 12/50 \"f1-alpha-match-10\" train / dev / test | 84.07 / 76.17 / 77.15.\n",
      "## [BEST epoch], 105 seconds.\n",
      "\n",
      "-- train epoch 13/50, batch 261/261 (100.00%), loss = 132.98.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 13/50 \"f1-alpha-match-10\" train / dev / test | 85.94 / 79.86 / 78.64.\n",
      "## [BEST epoch], 101 seconds.\n",
      "\n",
      "-- train epoch 14/50, batch 261/261 (100.00%), loss = 126.90.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 14/50 \"f1-alpha-match-10\" train / dev / test | 85.51 / 79.51 / 78.18.\n",
      "## [no improvement micro-f1 on DEV during the last 1 epochs (best_f1_dev=79.86), 101 seconds].\n",
      "\n",
      "-- train epoch 15/50, batch 261/261 (100.00%), loss = 121.69.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 15/50 \"f1-alpha-match-10\" train / dev / test | 86.60 / 79.70 / 80.15.\n",
      "## [no improvement micro-f1 on DEV during the last 2 epochs (best_f1_dev=79.86), 104 seconds].\n",
      "\n",
      "-- train epoch 16/50, batch 261/261 (100.00%), loss = 138.85.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 16/50 \"f1-alpha-match-10\" train / dev / test | 88.35 / 80.11 / 80.03.\n",
      "## [BEST epoch], 105 seconds.\n",
      "\n",
      "-- train epoch 17/50, batch 261/261 (100.00%), loss = 122.46.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 17/50 \"f1-alpha-match-10\" train / dev / test | 88.16 / 79.81 / 79.69.\n",
      "## [no improvement micro-f1 on DEV during the last 1 epochs (best_f1_dev=80.11), 102 seconds].\n",
      "\n",
      "-- train epoch 18/50, batch 261/261 (100.00%), loss = 131.72.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 18/50 \"f1-alpha-match-10\" train / dev / test | 87.68 / 78.88 / 80.22.\n",
      "## [no improvement micro-f1 on DEV during the last 2 epochs (best_f1_dev=80.11), 100 seconds].\n",
      "\n",
      "-- train epoch 19/50, batch 261/261 (100.00%), loss = 88.13.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 19/50 \"f1-alpha-match-10\" train / dev / test | 89.13 / 79.72 / 81.47.\n",
      "## [no improvement micro-f1 on DEV during the last 3 epochs (best_f1_dev=80.11), 107 seconds].\n",
      "\n",
      "-- train epoch 20/50, batch 261/261 (100.00%), loss = 87.74.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 20/50 \"f1-alpha-match-10\" train / dev / test | 90.42 / 80.82 / 81.98.\n",
      "## [BEST epoch], 107 seconds.\n",
      "\n",
      "-- train epoch 21/50, batch 261/261 (100.00%), loss = 104.64.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 21/50 \"f1-alpha-match-10\" train / dev / test | 90.00 / 79.43 / 79.02.\n",
      "## [no improvement micro-f1 on DEV during the last 1 epochs (best_f1_dev=80.82), 98 seconds].\n",
      "\n",
      "-- train epoch 22/50, batch 261/261 (100.00%), loss = 122.44.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 22/50 \"f1-alpha-match-10\" train / dev / test | 90.90 / 81.32 / 82.05.\n",
      "## [BEST epoch], 103 seconds.\n",
      "\n",
      "-- train epoch 23/50, batch 261/261 (100.00%), loss = 105.05.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 23/50 \"f1-alpha-match-10\" train / dev / test | 91.73 / 81.25 / 82.00.\n",
      "## [no improvement micro-f1 on DEV during the last 1 epochs (best_f1_dev=81.32), 99 seconds].\n",
      "\n",
      "-- train epoch 24/50, batch 261/261 (100.00%), loss = 94.19.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 24/50 \"f1-alpha-match-10\" train / dev / test | 91.42 / 80.27 / 80.49.\n",
      "## [no improvement micro-f1 on DEV during the last 2 epochs (best_f1_dev=81.32), 101 seconds].\n",
      "\n",
      "-- train epoch 25/50, batch 261/261 (100.00%), loss = 90.64.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 25/50 \"f1-alpha-match-10\" train / dev / test | 92.18 / 81.37 / 81.72.\n",
      "## [BEST epoch], 104 seconds.\n",
      "\n",
      "-- train epoch 26/50, batch 261/261 (100.00%), loss = 74.65.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 26/50 \"f1-alpha-match-10\" train / dev / test | 92.34 / 81.23 / 82.26.\n",
      "## [no improvement micro-f1 on DEV during the last 1 epochs (best_f1_dev=81.37), 101 seconds].\n",
      "\n",
      "-- train epoch 27/50, batch 261/261 (100.00%), loss = 86.79.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 27/50 \"f1-alpha-match-10\" train / dev / test | 92.77 / 81.65 / 81.60.\n",
      "## [BEST epoch], 99 seconds.\n",
      "\n",
      "-- train epoch 28/50, batch 261/261 (100.00%), loss = 82.92.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 28/50 \"f1-alpha-match-10\" train / dev / test | 92.88 / 81.36 / 82.40.\n",
      "## [no improvement micro-f1 on DEV during the last 1 epochs (best_f1_dev=81.65), 100 seconds].\n",
      "\n",
      "-- train epoch 29/50, batch 261/261 (100.00%), loss = 85.63.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 29/50 \"f1-alpha-match-10\" train / dev / test | 93.46 / 82.59 / 83.06.\n",
      "## [BEST epoch], 99 seconds.\n",
      "\n",
      "-- train epoch 30/50, batch 261/261 (100.00%), loss = 77.07.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 30/50 \"f1-alpha-match-10\" train / dev / test | 93.13 / 81.27 / 81.97.\n",
      "## [no improvement micro-f1 on DEV during the last 1 epochs (best_f1_dev=82.59), 106 seconds].\n",
      "\n",
      "-- train epoch 31/50, batch 261/261 (100.00%), loss = 68.34.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 31/50 \"f1-alpha-match-10\" train / dev / test | 94.18 / 82.32 / 82.26.\n",
      "## [no improvement micro-f1 on DEV during the last 2 epochs (best_f1_dev=82.59), 104 seconds].\n",
      "\n",
      "-- train epoch 32/50, batch 261/261 (100.00%), loss = 47.47.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 32/50 \"f1-alpha-match-10\" train / dev / test | 94.13 / 82.28 / 83.40.\n",
      "## [no improvement micro-f1 on DEV during the last 3 epochs (best_f1_dev=82.59), 106 seconds].\n",
      "\n",
      "-- train epoch 33/50, batch 261/261 (100.00%), loss = 63.96.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 33/50 \"f1-alpha-match-10\" train / dev / test | 93.06 / 80.52 / 81.88.\n",
      "## [no improvement micro-f1 on DEV during the last 4 epochs (best_f1_dev=82.59), 96 seconds].\n",
      "\n",
      "-- train epoch 34/50, batch 261/261 (100.00%), loss = 52.10.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 34/50 \"f1-alpha-match-10\" train / dev / test | 94.05 / 81.69 / 82.91.\n",
      "## [no improvement micro-f1 on DEV during the last 5 epochs (best_f1_dev=82.59), 99 seconds].\n",
      "\n",
      "-- train epoch 35/50, batch 261/261 (100.00%), loss = 73.22.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 35/50 \"f1-alpha-match-10\" train / dev / test | 94.47 / 81.35 / 83.31.\n",
      "## [no improvement micro-f1 on DEV during the last 6 epochs (best_f1_dev=82.59), 101 seconds].\n",
      "\n",
      "-- train epoch 36/50, batch 261/261 (100.00%), loss = 60.37.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 36/50 \"f1-alpha-match-10\" train / dev / test | 94.69 / 82.40 / 83.21.\n",
      "## [no improvement micro-f1 on DEV during the last 7 epochs (best_f1_dev=82.59), 101 seconds].\n",
      "\n",
      "-- train epoch 37/50, batch 71/261 (27.00%), loss = 10.78."
     ]
    }
   ],
   "source": [
    "! python3 main.py --train \"/home/vika/NER_RNN/targer/data/NER/Asqua_CAM_aspects/CAM_train.csv\" --dev \"/home/vika/NER_RNN/targer/data/NER/Asqua_CAM_aspects/CAM_dev.csv\" --data-io connl-ner-2003 --evaluator f1-alpha-match-10 --opt adam --lr 0.001 --save-best yes --patience 20 --rnn-hidden-dim 200 --gpu 1 --test \"/home/vika/NER_RNN/targer/data/NER/Asqua_CAM_aspects/CAM_test.csv\" --epoch-num 50 --isElmo True --save \"regular_elmo.hdf5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "args train = /home/vika/NER_RNN/targer/data/NER/Asqua_CAM_aspects/CAM_train.csv\n",
      "Loading from /home/vika/NER_RNN/targer/data/NER/Asqua_CAM_aspects/CAM_train.csv: 2619 samples, 74616 words.\n",
      "Loading from /home/vika/NER_RNN/targer/data/NER/Asqua_CAM_aspects/CAM_dev.csv: 350 samples, 10092 words.\n",
      "Loading from /home/vika/NER_RNN/targer/data/NER/Asqua_CAM_aspects/CAM_qw_new.csv: 63 samples, 322 words.\n",
      "DatasetsBank: len(unique_words_list) = 7143 unique words.\n",
      "DatasetsBank: len(unique_words_list) = 7600 unique words.\n",
      "DatasetsBank: len(unique_words_list) = 7604 unique words.\n",
      "\n",
      "load_vocabulary_from_tag_sequences:\n",
      " -- class_num = 7\n",
      " -- {'<pad>': 0, 'O': 1, 'B-OTHOBJ': 2, 'B-ASPOBJ': 3, 'B-ASP': 4, 'I-ASP': 5, 'I-ASPOBJ': 6, 'I-OTHOBJ': 7}\n",
      "in main\n",
      "True\n",
      "/home/vika/NER_RNN/targer/embeddings/elmo_2x4096_512_2048cnn_2xhighway_5.5B_weights.hdf5\n",
      "/home/vika/NER_RNN/targer/embeddings/elmo_2x4096_512_2048cnn_2xhighway_5.5B_options.json\n",
      "init targer base\n",
      "True\n",
      "/home/vika/NER_RNN/targer/embeddings/elmo_2x4096_512_2048cnn_2xhighway_5.5B_weights.hdf5\n",
      "/home/vika/NER_RNN/targer/embeddings/elmo_2x4096_512_2048cnn_2xhighway_5.5B_options.json\n",
      "elmo is initiated\n",
      "init targer BiRNNCNNCRF\n",
      "True\n",
      "/home/vika/NER_RNN/targer/embeddings/elmo_2x4096_512_2048cnn_2xhighway_5.5B_weights.hdf5\n",
      "/home/vika/NER_RNN/targer/embeddings/elmo_2x4096_512_2048cnn_2xhighway_5.5B_options.json\n",
      "Empirical transition matrix from the train dataset:\n",
      "               <pad>         O  B-OTHOBJ  B-ASPOBJ     B-ASP     I-ASP  I-ASPOBJ  I-OTHOBJ     <sos>\n",
      "\n",
      "     <pad>       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0\n",
      "         O       0.0   58639.0    2722.0    2863.0     792.0     165.0      22.0      21.0    2226.0\n",
      "  B-OTHOBJ       0.0    2713.0       0.0       1.0       3.0       1.0       0.0       0.0      47.0\n",
      "  B-ASPOBJ       0.0    2560.0       8.0       0.0       4.0       3.0       0.0       0.0     341.0\n",
      "     B-ASP       0.0     919.0      14.0      28.0       0.0       0.0       0.0       0.0       5.0\n",
      "     I-ASP       0.0       0.0       0.0       2.0     167.0     307.0       0.0       0.0       0.0\n",
      "  I-ASPOBJ       0.0       0.0       0.0      22.0       0.0       0.0       0.0       0.0       0.0\n",
      "  I-OTHOBJ       0.0       0.0      21.0       0.0       0.0       0.0       0.0       0.0       0.0\n",
      "     <sos>       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0\n",
      "\n",
      "Initialized transition matrix:\n",
      "               <pad>         O  B-OTHOBJ  B-ASPOBJ     B-ASP     I-ASP  I-ASPOBJ  I-OTHOBJ     <sos>\n",
      "\n",
      "     <pad>   -9999.0   -9999.0   -9999.0   -9999.0   -9999.0   -9999.0   -9999.0   -9999.0   -9999.0\n",
      "         O   -9999.0      -1.0      -0.9      -1.1      -1.0      -1.1      -1.1      -1.0      -1.0\n",
      "  B-OTHOBJ   -9999.0      -1.0   -9999.0      -0.8      -1.1      -1.1   -9999.0   -9999.0      -1.1\n",
      "  B-ASPOBJ   -9999.0      -1.0      -1.0   -9999.0      -1.2      -0.9   -9999.0   -9999.0      -0.9\n",
      "     B-ASP   -9999.0      -1.0      -0.8      -1.1   -9999.0   -9999.0   -9999.0   -9999.0      -1.0\n",
      "     I-ASP   -9999.0   -9999.0   -9999.0      -1.1      -1.0      -0.9   -9999.0   -9999.0   -9999.0\n",
      "  I-ASPOBJ   -9999.0   -9999.0   -9999.0      -0.9   -9999.0   -9999.0   -9999.0   -9999.0   -9999.0\n",
      "  I-OTHOBJ   -9999.0   -9999.0      -0.9   -9999.0   -9999.0   -9999.0   -9999.0   -9999.0   -9999.0\n",
      "     <sos>   -9999.0   -9999.0   -9999.0   -9999.0   -9999.0   -9999.0   -9999.0   -9999.0   -9999.0\n",
      "\n",
      "Start training...\n",
      "\n",
      "\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 1/1 (0.00%).\n",
      "== eval epoch 0/50 \"f1-alpha-match-10\" train / dev / test | 4.90 / 4.60 / 12.08.\n",
      "## [BEST epoch], 16 seconds.\n",
      "\n",
      "-- train epoch 1/50, batch 261/261 (100.00%), loss = 482.29.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 1/1 (0.00%).\n",
      "== eval epoch 1/50 \"f1-alpha-match-10\" train / dev / test | 60.09 / 58.18 / 44.87.\n",
      "## [BEST epoch], 104 seconds.\n",
      "\n",
      "-- train epoch 2/50, batch 261/261 (100.00%), loss = 297.58.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 1/1 (0.00%).\n",
      "== eval epoch 2/50 \"f1-alpha-match-10\" train / dev / test | 63.26 / 58.87 / 42.55.\n",
      "## [BEST epoch], 103 seconds.\n",
      "\n",
      "-- train epoch 3/50, batch 261/261 (100.00%), loss = 271.47.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 1/1 (0.00%).\n",
      "== eval epoch 3/50 \"f1-alpha-match-10\" train / dev / test | 68.38 / 64.74 / 46.43.\n",
      "## [BEST epoch], 102 seconds.\n",
      "\n",
      "-- train epoch 4/50, batch 261/261 (100.00%), loss = 246.57.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 1/1 (0.00%).\n",
      "== eval epoch 4/50 \"f1-alpha-match-10\" train / dev / test | 70.43 / 64.01 / 34.43.\n",
      "## [no improvement micro-f1 on DEV during the last 1 epochs (best_f1_dev=64.74), 106 seconds].\n",
      "\n",
      "-- train epoch 5/50, batch 261/261 (100.00%), loss = 234.79.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 1/1 (0.00%).\n",
      "== eval epoch 5/50 \"f1-alpha-match-10\" train / dev / test | 72.03 / 67.81 / 45.49.\n",
      "## [BEST epoch], 101 seconds.\n",
      "\n",
      "-- train epoch 6/50, batch 261/261 (100.00%), loss = 197.57.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 1/1 (0.00%).\n",
      "== eval epoch 6/50 \"f1-alpha-match-10\" train / dev / test | 76.58 / 70.35 / 45.19.\n",
      "## [BEST epoch], 96 seconds.\n",
      "\n",
      "-- train epoch 7/50, batch 261/261 (100.00%), loss = 182.52.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 1/1 (0.00%).\n",
      "== eval epoch 7/50 \"f1-alpha-match-10\" train / dev / test | 77.64 / 70.21 / 45.28.\n",
      "## [no improvement micro-f1 on DEV during the last 1 epochs (best_f1_dev=70.35), 99 seconds].\n",
      "\n",
      "-- train epoch 8/50, batch 261/261 (100.00%), loss = 193.16.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 1/1 (0.00%).\n",
      "== eval epoch 8/50 \"f1-alpha-match-10\" train / dev / test | 79.05 / 72.93 / 50.97.\n",
      "## [BEST epoch], 98 seconds.\n",
      "\n",
      "-- train epoch 9/50, batch 261/261 (100.00%), loss = 183.23.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 1/1 (0.00%).\n",
      "== eval epoch 9/50 \"f1-alpha-match-10\" train / dev / test | 82.60 / 76.35 / 42.69.\n",
      "## [BEST epoch], 102 seconds.\n",
      "\n",
      "-- train epoch 10/50, batch 261/261 (100.00%), loss = 157.00.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 1/1 (0.00%).\n",
      "== eval epoch 10/50 \"f1-alpha-match-10\" train / dev / test | 81.92 / 74.99 / 42.97.\n",
      "## [no improvement micro-f1 on DEV during the last 1 epochs (best_f1_dev=76.35), 101 seconds].\n",
      "\n",
      "-- train epoch 11/50, batch 261/261 (100.00%), loss = 152.17.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 1/1 (0.00%).\n",
      "== eval epoch 11/50 \"f1-alpha-match-10\" train / dev / test | 83.08 / 76.47 / 42.80.\n",
      "## [BEST epoch], 106 seconds.\n",
      "\n",
      "-- train epoch 12/50, batch 261/261 (100.00%), loss = 127.66.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 1/1 (0.00%).\n",
      "== eval epoch 12/50 \"f1-alpha-match-10\" train / dev / test | 83.59 / 76.03 / 40.34.\n",
      "## [no improvement micro-f1 on DEV during the last 1 epochs (best_f1_dev=76.47), 102 seconds].\n",
      "\n",
      "-- train epoch 13/50, batch 261/261 (100.00%), loss = 132.64.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 1/1 (0.00%).\n",
      "== eval epoch 13/50 \"f1-alpha-match-10\" train / dev / test | 85.58 / 77.95 / 34.75.\n",
      "## [BEST epoch], 99 seconds.\n",
      "\n",
      "-- train epoch 14/50, batch 261/261 (100.00%), loss = 130.28.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 1/1 (0.00%).\n",
      "== eval epoch 14/50 \"f1-alpha-match-10\" train / dev / test | 84.71 / 78.42 / 48.19.\n",
      "## [BEST epoch], 104 seconds.\n",
      "\n",
      "-- train epoch 15/50, batch 261/261 (100.00%), loss = 120.01.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 1/1 (0.00%).\n",
      "== eval epoch 15/50 \"f1-alpha-match-10\" train / dev / test | 85.55 / 78.11 / 49.41.\n",
      "## [no improvement micro-f1 on DEV during the last 1 epochs (best_f1_dev=78.42), 103 seconds].\n",
      "\n",
      "-- train epoch 16/50, batch 261/261 (100.00%), loss = 134.78.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 1/1 (0.00%).\n",
      "== eval epoch 16/50 \"f1-alpha-match-10\" train / dev / test | 88.04 / 79.42 / 46.04.\n",
      "## [BEST epoch], 105 seconds.\n",
      "\n",
      "-- train epoch 17/50, batch 261/261 (100.00%), loss = 124.69.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 1/1 (0.00%).\n",
      "== eval epoch 17/50 \"f1-alpha-match-10\" train / dev / test | 88.31 / 78.95 / 42.19.\n",
      "## [no improvement micro-f1 on DEV during the last 1 epochs (best_f1_dev=79.42), 97 seconds].\n",
      "\n",
      "-- train epoch 18/50, batch 261/261 (100.00%), loss = 137.27.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 1/1 (0.00%).\n",
      "== eval epoch 18/50 \"f1-alpha-match-10\" train / dev / test | 88.30 / 80.43 / 38.06.\n",
      "## [BEST epoch], 101 seconds.\n",
      "\n",
      "-- train epoch 19/50, batch 261/261 (100.00%), loss = 88.62.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 1/1 (0.00%).\n",
      "== eval epoch 19/50 \"f1-alpha-match-10\" train / dev / test | 89.44 / 79.88 / 32.92.\n",
      "## [no improvement micro-f1 on DEV during the last 1 epochs (best_f1_dev=80.43), 102 seconds].\n",
      "\n",
      "-- train epoch 20/50, batch 261/261 (100.00%), loss = 85.18.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 1/1 (0.00%).\n",
      "== eval epoch 20/50 \"f1-alpha-match-10\" train / dev / test | 90.75 / 81.99 / 41.27.\n",
      "## [BEST epoch], 99 seconds.\n",
      "\n",
      "-- train epoch 21/50, batch 261/261 (100.00%), loss = 107.26.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 1/1 (0.00%).\n",
      "== eval epoch 21/50 \"f1-alpha-match-10\" train / dev / test | 89.94 / 80.27 / 34.78.\n",
      "## [no improvement micro-f1 on DEV during the last 1 epochs (best_f1_dev=81.99), 103 seconds].\n",
      "\n",
      "-- train epoch 22/50, batch 261/261 (100.00%), loss = 120.54.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 1/1 (0.00%).\n",
      "== eval epoch 22/50 \"f1-alpha-match-10\" train / dev / test | 90.99 / 81.55 / 39.66.\n",
      "## [no improvement micro-f1 on DEV during the last 2 epochs (best_f1_dev=81.99), 96 seconds].\n",
      "\n",
      "-- train epoch 23/50, batch 261/261 (100.00%), loss = 103.42.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 1/1 (0.00%).\n",
      "== eval epoch 23/50 \"f1-alpha-match-10\" train / dev / test | 91.88 / 81.28 / 44.36.\n",
      "## [no improvement micro-f1 on DEV during the last 3 epochs (best_f1_dev=81.99), 97 seconds].\n",
      "\n",
      "-- train epoch 24/50, batch 261/261 (100.00%), loss = 94.27.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 1/1 (0.00%).\n",
      "== eval epoch 24/50 \"f1-alpha-match-10\" train / dev / test | 91.23 / 79.44 / 44.26.\n",
      "## [no improvement micro-f1 on DEV during the last 4 epochs (best_f1_dev=81.99), 96 seconds].\n",
      "\n",
      "-- train epoch 25/50, batch 261/261 (100.00%), loss = 90.57.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 1/1 (0.00%).\n",
      "== eval epoch 25/50 \"f1-alpha-match-10\" train / dev / test | 92.44 / 80.88 / 48.00.\n",
      "## [no improvement micro-f1 on DEV during the last 5 epochs (best_f1_dev=81.99), 98 seconds].\n",
      "\n",
      "-- train epoch 26/50, batch 261/261 (100.00%), loss = 74.31.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 1/1 (0.00%).\n",
      "== eval epoch 26/50 \"f1-alpha-match-10\" train / dev / test | 92.19 / 80.35 / 45.45.\n",
      "## [no improvement micro-f1 on DEV during the last 6 epochs (best_f1_dev=81.99), 97 seconds].\n",
      "\n",
      "-- train epoch 27/50, batch 261/261 (100.00%), loss = 79.91.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 1/1 (0.00%).\n",
      "== eval epoch 27/50 \"f1-alpha-match-10\" train / dev / test | 92.21 / 80.28 / 47.88.\n",
      "## [no improvement micro-f1 on DEV during the last 7 epochs (best_f1_dev=81.99), 96 seconds].\n",
      "\n",
      "-- train epoch 28/50, batch 261/261 (100.00%), loss = 86.83.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 1/1 (0.00%).\n",
      "== eval epoch 28/50 \"f1-alpha-match-10\" train / dev / test | 93.30 / 81.06 / 43.33.\n",
      "## [no improvement micro-f1 on DEV during the last 8 epochs (best_f1_dev=81.99), 97 seconds].\n",
      "\n",
      "-- train epoch 29/50, batch 261/261 (100.00%), loss = 79.07.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 1/1 (0.00%).\n",
      "== eval epoch 29/50 \"f1-alpha-match-10\" train / dev / test | 92.80 / 81.12 / 43.65.\n",
      "## [no improvement micro-f1 on DEV during the last 9 epochs (best_f1_dev=81.99), 102 seconds].\n",
      "\n",
      "-- train epoch 30/50, batch 261/261 (100.00%), loss = 79.53.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 1/1 (0.00%).\n",
      "== eval epoch 30/50 \"f1-alpha-match-10\" train / dev / test | 93.12 / 81.97 / 45.45.\n",
      "## [no improvement micro-f1 on DEV during the last 10 epochs (best_f1_dev=81.99), 99 seconds].\n",
      "\n",
      "-- train epoch 31/50, batch 261/261 (100.00%), loss = 71.21.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 1/1 (0.00%).\n",
      "== eval epoch 31/50 \"f1-alpha-match-10\" train / dev / test | 94.07 / 82.15 / 41.43.\n",
      "## [BEST epoch], 101 seconds.\n",
      "\n",
      "-- train epoch 32/50, batch 261/261 (100.00%), loss = 47.15.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 1/1 (0.00%).\n",
      "== eval epoch 32/50 \"f1-alpha-match-10\" train / dev / test | 94.20 / 82.37 / 38.89.\n",
      "## [BEST epoch], 104 seconds.\n",
      "\n",
      "-- train epoch 33/50, batch 261/261 (100.00%), loss = 64.21.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 1/1 (0.00%).\n",
      "== eval epoch 33/50 \"f1-alpha-match-10\" train / dev / test | 93.03 / 81.21 / 47.19.\n",
      "## [no improvement micro-f1 on DEV during the last 1 epochs (best_f1_dev=82.37), 102 seconds].\n",
      "\n",
      "-- train epoch 34/50, batch 261/261 (100.00%), loss = 51.36.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 1/1 (0.00%).\n",
      "== eval epoch 34/50 \"f1-alpha-match-10\" train / dev / test | 94.08 / 81.68 / 44.44.\n",
      "## [no improvement micro-f1 on DEV during the last 2 epochs (best_f1_dev=82.37), 97 seconds].\n",
      "\n",
      "-- train epoch 35/50, batch 261/261 (100.00%), loss = 69.79.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 1/1 (0.00%).\n",
      "== eval epoch 35/50 \"f1-alpha-match-10\" train / dev / test | 94.54 / 81.42 / 46.64.\n",
      "## [no improvement micro-f1 on DEV during the last 3 epochs (best_f1_dev=82.37), 97 seconds].\n",
      "\n",
      "-- train epoch 36/50, batch 261/261 (100.00%), loss = 61.80.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 1/1 (0.00%).\n",
      "== eval epoch 36/50 \"f1-alpha-match-10\" train / dev / test | 94.93 / 80.98 / 46.88.\n",
      "## [no improvement micro-f1 on DEV during the last 4 epochs (best_f1_dev=82.37), 95 seconds].\n",
      "\n",
      "-- train epoch 37/50, batch 261/261 (100.00%), loss = 77.17.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 1/1 (0.00%).\n",
      "== eval epoch 37/50 \"f1-alpha-match-10\" train / dev / test | 94.04 / 81.85 / 43.92.\n",
      "## [no improvement micro-f1 on DEV during the last 5 epochs (best_f1_dev=82.37), 97 seconds].\n",
      "\n",
      "-- train epoch 38/50, batch 261/261 (100.00%), loss = 67.29.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 1/1 (0.00%).\n",
      "== eval epoch 38/50 \"f1-alpha-match-10\" train / dev / test | 94.54 / 81.54 / 41.60.\n",
      "## [no improvement micro-f1 on DEV during the last 6 epochs (best_f1_dev=82.37), 99 seconds].\n",
      "\n",
      "-- train epoch 39/50, batch 261/261 (100.00%), loss = 70.26.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 1/1 (0.00%).\n",
      "== eval epoch 39/50 \"f1-alpha-match-10\" train / dev / test | 94.75 / 82.11 / 41.32.\n",
      "## [no improvement micro-f1 on DEV during the last 7 epochs (best_f1_dev=82.37), 98 seconds].\n",
      "\n",
      "-- train epoch 40/50, batch 261/261 (100.00%), loss = 61.19.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 1/1 (0.00%).\n",
      "== eval epoch 40/50 \"f1-alpha-match-10\" train / dev / test | 95.10 / 81.77 / 38.40.\n",
      "## [no improvement micro-f1 on DEV during the last 8 epochs (best_f1_dev=82.37), 101 seconds].\n",
      "\n",
      "-- train epoch 41/50, batch 261/261 (100.00%), loss = 48.37.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 1/1 (0.00%).\n",
      "== eval epoch 41/50 \"f1-alpha-match-10\" train / dev / test | 95.26 / 81.73 / 38.91.\n",
      "## [no improvement micro-f1 on DEV during the last 9 epochs (best_f1_dev=82.37), 96 seconds].\n",
      "\n",
      "-- train epoch 42/50, batch 261/261 (100.00%), loss = 65.36.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 1/1 (0.00%).\n",
      "== eval epoch 42/50 \"f1-alpha-match-10\" train / dev / test | 95.11 / 81.01 / 37.45.\n",
      "## [no improvement micro-f1 on DEV during the last 10 epochs (best_f1_dev=82.37), 99 seconds].\n",
      "\n",
      "-- train epoch 43/50, batch 261/261 (100.00%), loss = 44.95.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 1/1 (0.00%).\n",
      "== eval epoch 43/50 \"f1-alpha-match-10\" train / dev / test | 95.16 / 82.13 / 37.94.\n",
      "## [no improvement micro-f1 on DEV during the last 11 epochs (best_f1_dev=82.37), 97 seconds].\n",
      "\n",
      "-- train epoch 44/50, batch 261/261 (100.00%), loss = 49.34.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 1/1 (0.00%).\n",
      "== eval epoch 44/50 \"f1-alpha-match-10\" train / dev / test | 95.17 / 81.70 / 42.47.\n",
      "## [no improvement micro-f1 on DEV during the last 12 epochs (best_f1_dev=82.37), 98 seconds].\n",
      "\n",
      "-- train epoch 45/50, batch 261/261 (100.00%), loss = 52.43.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 1/1 (0.00%).\n",
      "== eval epoch 45/50 \"f1-alpha-match-10\" train / dev / test | 95.61 / 81.37 / 34.24.\n",
      "## [no improvement micro-f1 on DEV during the last 13 epochs (best_f1_dev=82.37), 98 seconds].\n",
      "\n",
      "-- train epoch 46/50, batch 261/261 (100.00%), loss = 70.05.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 1/1 (0.00%).\n",
      "== eval epoch 46/50 \"f1-alpha-match-10\" train / dev / test | 95.10 / 81.02 / 41.86.\n",
      "## [no improvement micro-f1 on DEV during the last 14 epochs (best_f1_dev=82.37), 99 seconds].\n",
      "\n",
      "-- train epoch 47/50, batch 261/261 (100.00%), loss = 68.13.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 1/1 (0.00%).\n",
      "== eval epoch 47/50 \"f1-alpha-match-10\" train / dev / test | 95.54 / 82.24 / 36.58.\n",
      "## [no improvement micro-f1 on DEV during the last 15 epochs (best_f1_dev=82.37), 100 seconds].\n",
      "\n",
      "-- train epoch 48/50, batch 261/261 (100.00%), loss = 51.89.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 1/1 (0.00%).\n",
      "== eval epoch 48/50 \"f1-alpha-match-10\" train / dev / test | 95.67 / 80.87 / 41.70.\n",
      "## [no improvement micro-f1 on DEV during the last 16 epochs (best_f1_dev=82.37), 102 seconds].\n",
      "\n",
      "-- train epoch 49/50, batch 261/261 (100.00%), loss = 51.79.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 1/1 (0.00%).\n",
      "== eval epoch 49/50 \"f1-alpha-match-10\" train / dev / test | 95.84 / 81.43 / 41.30.\n",
      "## [no improvement micro-f1 on DEV during the last 17 epochs (best_f1_dev=82.37), 102 seconds].\n",
      "\n",
      "-- train epoch 50/50, batch 261/261 (100.00%), loss = 66.44.\n",
      "\n",
      "++ predicting, batch 26/26 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 1/1 (0.00%).\n",
      "== eval epoch 50/50 \"f1-alpha-match-10\" train / dev / test | 95.83 / 81.55 / 46.74.\n",
      "## [no improvement micro-f1 on DEV during the last 18 epochs (best_f1_dev=82.37), 95 seconds].\n",
      "\n",
      "Evaluation\n",
      "\n",
      "batch_size=10\n",
      "char_cnn_filter_num=30\n",
      "char_embeddings_dim=25\n",
      "char_window_size=3\n",
      "check_for_lowercase=True\n",
      "clip_grad=5\n",
      "cross_fold_id=-1\n",
      "cross_folds_num=-1\n",
      "data_io='connl-ner-2003'\n",
      "dataset_sort=False\n",
      "dev='/home/vika/NER_RNN/targer/data/NER/Asqua_CAM_aspects/CAM_dev.csv'\n",
      "dropout_ratio=0.5\n",
      "elmo_options='/home/vika/NER_RNN/targer/embeddings/elmo_2x4096_512_2048cnn_2xhighway_5.5B_options.json'\n",
      "elmo_weights='/home/vika/NER_RNN/targer/embeddings/elmo_2x4096_512_2048cnn_2xhighway_5.5B_weights.hdf5'\n",
      "emb_delimiter=' '\n",
      "emb_dim=100\n",
      "emb_fn='embeddings/glove.6B.100d.txt'\n",
      "emb_load_all=False\n",
      "epoch_num=50\n",
      "evaluator='f1-alpha-match-10'\n",
      "freeze_char_embeddings=False\n",
      "freeze_word_embeddings=False\n",
      "gpu=1\n",
      "isElmo=True\n",
      "load=None\n",
      "lr=0.001\n",
      "lr_decay=0.05\n",
      "min_epoch_num=50\n",
      "model='BiRNNCNNCRF'\n",
      "momentum=0.9\n",
      "opt='adam'\n",
      "patience=20\n",
      "report_fn='2019_09_06_05-34_58_report.txt'\n",
      "rnn_hidden_dim=200\n",
      "rnn_type='LSTM'\n",
      "save='new_tagger1.hdf5'\n",
      "save_best=True\n",
      "seed_num=42\n",
      "test='/home/vika/NER_RNN/targer/data/NER/Asqua_CAM_aspects/CAM_qw_new.csv'\n",
      "train='/home/vika/NER_RNN/targer/data/NER/Asqua_CAM_aspects/CAM_train.csv'\n",
      "verbose=True\n",
      "word_len=20\n",
      "word_seq_indexer=None\n",
      "\n",
      "         epoch  |     train loss | f1-alpha-match-10-train | f1-alpha-match-10-dev | f1-alpha-match-10-test \n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "              0 |           0.00 |           4.90 |           4.60 |          12.08 \n",
      "              1 |         482.29 |          60.09 |          58.18 |          44.87 \n",
      "              2 |         297.58 |          63.26 |          58.87 |          42.55 \n",
      "              3 |         271.47 |          68.38 |          64.74 |          46.43 \n",
      "              4 |         246.57 |          70.43 |          64.01 |          34.43 \n",
      "              5 |         234.79 |          72.03 |          67.81 |          45.49 \n",
      "              6 |         197.57 |          76.58 |          70.35 |          45.19 \n",
      "              7 |         182.52 |          77.64 |          70.21 |          45.28 \n",
      "              8 |         193.16 |          79.05 |          72.93 |          50.97 \n",
      "              9 |         183.23 |          82.60 |          76.35 |          42.69 \n",
      "             10 |         157.00 |          81.92 |          74.99 |          42.97 \n",
      "             11 |         152.17 |          83.08 |          76.47 |          42.80 \n",
      "             12 |         127.66 |          83.59 |          76.03 |          40.34 \n",
      "             13 |         132.64 |          85.58 |          77.95 |          34.75 \n",
      "             14 |         130.28 |          84.71 |          78.42 |          48.19 \n",
      "             15 |         120.01 |          85.55 |          78.11 |          49.41 \n",
      "             16 |         134.78 |          88.04 |          79.42 |          46.04 \n",
      "             17 |         124.69 |          88.31 |          78.95 |          42.19 \n",
      "             18 |         137.27 |          88.30 |          80.43 |          38.06 \n",
      "             19 |          88.62 |          89.44 |          79.88 |          32.92 \n",
      "             20 |          85.18 |          90.75 |          81.99 |          41.27 \n",
      "             21 |         107.26 |          89.94 |          80.27 |          34.78 \n",
      "             22 |         120.54 |          90.99 |          81.55 |          39.66 \n",
      "             23 |         103.42 |          91.88 |          81.28 |          44.36 \n",
      "             24 |          94.27 |          91.23 |          79.44 |          44.26 \n",
      "             25 |          90.57 |          92.44 |          80.88 |          48.00 \n",
      "             26 |          74.31 |          92.19 |          80.35 |          45.45 \n",
      "             27 |          79.91 |          92.21 |          80.28 |          47.88 \n",
      "             28 |          86.83 |          93.30 |          81.06 |          43.33 \n",
      "             29 |          79.07 |          92.80 |          81.12 |          43.65 \n",
      "             30 |          79.53 |          93.12 |          81.97 |          45.45 \n",
      "             31 |          71.21 |          94.07 |          82.15 |          41.43 \n",
      "             32 |          47.15 |          94.20 |          82.37 |          38.89 \n",
      "             33 |          64.21 |          93.03 |          81.21 |          47.19 \n",
      "             34 |          51.36 |          94.08 |          81.68 |          44.44 \n",
      "             35 |          69.79 |          94.54 |          81.42 |          46.64 \n",
      "             36 |          61.80 |          94.93 |          80.98 |          46.88 \n",
      "             37 |          77.17 |          94.04 |          81.85 |          43.92 \n",
      "             38 |          67.29 |          94.54 |          81.54 |          41.60 \n",
      "             39 |          70.26 |          94.75 |          82.11 |          41.32 \n",
      "             40 |          61.19 |          95.10 |          81.77 |          38.40 \n",
      "             41 |          48.37 |          95.26 |          81.73 |          38.91 \n",
      "             42 |          65.36 |          95.11 |          81.01 |          37.45 \n",
      "             43 |          44.95 |          95.16 |          82.13 |          37.94 \n",
      "             44 |          49.34 |          95.17 |          81.70 |          42.47 \n",
      "             45 |          52.43 |          95.61 |          81.37 |          34.24 \n",
      "             46 |          70.05 |          95.10 |          81.02 |          41.86 \n",
      "             47 |          68.13 |          95.54 |          82.24 |          36.58 \n",
      "             48 |          51.89 |          95.67 |          80.87 |          41.70 \n",
      "             49 |          51.79 |          95.84 |          81.43 |          41.30 \n",
      "             50 |          66.44 |          95.83 |          81.55 |          46.74 \n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Final eval on test, \"save best\", best epoch on dev 32, f1-alpha-match-10, test = 38.89)\n",
      "--------------------------------------------------------------------------------------------------------------*** f1 alpha match, alpha = 1.0\n",
      "*** f1 = 38.89, precision = 51.58, recall = 31.21\n",
      "*** TP = 49, FP = 46, FN = 108\n",
      "Input arguments:\n",
      "python3 main.py --train /home/vika/NER_RNN/targer/data/NER/Asqua_CAM_aspects/CAM_train.csv --dev /home/vika/NER_RNN/targer/data/NER/Asqua_CAM_aspects/CAM_dev.csv --data-io connl-ner-2003 --evaluator f1-alpha-match-10 --opt adam --lr 0.001 --save-best yes --patience 20 --rnn-hidden-dim 200 --gpu 1 --test /home/vika/NER_RNN/targer/data/NER/Asqua_CAM_aspects/CAM_qw_new.csv --epoch-num 50 --isElmo True --save new_tagger1.hdf5\n",
      "\n",
      "38.8889\n"
     ]
    }
   ],
   "source": [
    "! python3 main.py --train \"/home/vika/NER_RNN/targer/data/NER/Asqua_CAM_aspects/CAM_train.csv\" --dev \"/home/vika/NER_RNN/targer/data/NER/Asqua_CAM_aspects/CAM_dev.csv\" --data-io connl-ner-2003 --evaluator f1-alpha-match-10 --opt adam --lr 0.001 --save-best yes --patience 20 --rnn-hidden-dim 200 --gpu 1 --test \"/home/vika/NER_RNN/targer/data/NER/Asqua_CAM_aspects/CAM_qw_new.csv\" --epoch-num 50 --isElmo True --save \"new_tagger1.hdf5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.modules.elmo import Elmo, batch_to_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/vika/NER_RNN/targer')\n",
    "from src.factories.factory_tagger import TaggerFactory\n",
    "\n",
    "model = TaggerFactory.load(PATH_TO_PRETRAINED + MODEL_NAME)\n",
    "model.cuda(device=1)\n",
    "model.gpu = 1\n",
    "tags = model.predict_tags_from_words(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "args train = /home/vika/NER_RNN/targer/data/NER/Asqua_CAM_aspects/CAM_train_new.csv\n",
      "Loading from /home/vika/NER_RNN/targer/data/NER/Asqua_CAM_aspects/CAM_train_new.csv: 2719 samples, 75117 words.\n",
      "Loading from /home/vika/NER_RNN/targer/data/NER/Asqua_CAM_aspects/CAM_dev.csv: 350 samples, 10092 words.\n",
      "Loading from /home/vika/NER_RNN/targer/data/NER/Asqua_CAM_aspects/CAM_test_new.csv: 584 samples, 15032 words.\n",
      "DatasetsBank: len(unique_words_list) = 7144 unique words.\n",
      "DatasetsBank: len(unique_words_list) = 7601 unique words.\n",
      "DatasetsBank: len(unique_words_list) = 8171 unique words.\n",
      "\n",
      "load_vocabulary_from_tag_sequences:\n",
      " -- class_num = 8\n",
      " -- {'<pad>': 0, 'tag': 1, 'O': 2, 'B-OTHOBJ': 3, 'B-ASPOBJ': 4, 'B-ASP': 5, 'I-ASP': 6, 'I-ASPOBJ': 7, 'I-OTHOBJ': 8}\n",
      "in main\n",
      "True\n",
      "/home/vika/NER_RNN/targer/embeddings/elmo_2x4096_512_2048cnn_2xhighway_5.5B_weights.hdf5\n",
      "/home/vika/NER_RNN/targer/embeddings/elmo_2x4096_512_2048cnn_2xhighway_5.5B_options.json\n",
      "init targer base\n",
      "True\n",
      "/home/vika/NER_RNN/targer/embeddings/elmo_2x4096_512_2048cnn_2xhighway_5.5B_weights.hdf5\n",
      "/home/vika/NER_RNN/targer/embeddings/elmo_2x4096_512_2048cnn_2xhighway_5.5B_options.json\n",
      "elmo is initiated\n",
      "init targer BiRNNCNNCRF\n",
      "True\n",
      "/home/vika/NER_RNN/targer/embeddings/elmo_2x4096_512_2048cnn_2xhighway_5.5B_weights.hdf5\n",
      "/home/vika/NER_RNN/targer/embeddings/elmo_2x4096_512_2048cnn_2xhighway_5.5B_options.json\n",
      "Empirical transition matrix from the train dataset:\n",
      "               <pad>       tag         O  B-OTHOBJ  B-ASPOBJ     B-ASP     I-ASP  I-ASPOBJ  I-OTHOBJ     <sos>\n",
      "\n",
      "     <pad>       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0\n",
      "       tag       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       1.0\n",
      "         O       0.0       1.0   58723.0    2722.0    2947.0     808.0     165.0      22.0      21.0    2307.0\n",
      "  B-OTHOBJ       0.0       0.0    2813.0       0.0       1.0       3.0       1.0       0.0       0.0      47.0\n",
      "  B-ASPOBJ       0.0       0.0    2624.0       8.0       0.0      22.0       3.0       0.0       0.0     359.0\n",
      "     B-ASP       0.0       0.0     937.0      14.0      44.0       0.0       0.0       0.0       0.0       5.0\n",
      "     I-ASP       0.0       0.0       0.0       0.0       2.0     167.0     307.0       0.0       0.0       0.0\n",
      "  I-ASPOBJ       0.0       0.0       0.0       0.0      22.0       0.0       0.0       0.0       0.0       0.0\n",
      "  I-OTHOBJ       0.0       0.0       0.0      21.0       0.0       0.0       0.0       0.0       0.0       0.0\n",
      "     <sos>       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0\n",
      "\n",
      "Initialized transition matrix:\n",
      "               <pad>       tag         O  B-OTHOBJ  B-ASPOBJ     B-ASP     I-ASP  I-ASPOBJ  I-OTHOBJ     <sos>\n",
      "\n",
      "     <pad>   -9999.0   -9999.0   -9999.0   -9999.0   -9999.0   -9999.0   -9999.0   -9999.0   -9999.0   -9999.0\n",
      "       tag   -9999.0   -9999.0   -9999.0   -9999.0   -9999.0   -9999.0   -9999.0   -9999.0   -9999.0      -0.9\n",
      "         O   -9999.0      -1.1      -1.1      -0.9      -0.9      -0.8      -0.9      -0.9      -1.0      -0.8\n",
      "  B-OTHOBJ   -9999.0   -9999.0      -1.0   -9999.0      -1.1      -1.1      -0.9   -9999.0   -9999.0      -1.2\n",
      "  B-ASPOBJ   -9999.0   -9999.0      -1.0      -0.9   -9999.0      -1.0      -0.9   -9999.0   -9999.0      -1.2\n",
      "     B-ASP   -9999.0   -9999.0      -0.9      -0.9      -1.0   -9999.0   -9999.0   -9999.0   -9999.0      -1.1\n",
      "     I-ASP   -9999.0   -9999.0   -9999.0   -9999.0      -1.2      -1.0      -1.1   -9999.0   -9999.0   -9999.0\n",
      "  I-ASPOBJ   -9999.0   -9999.0   -9999.0   -9999.0      -1.0   -9999.0   -9999.0   -9999.0   -9999.0   -9999.0\n",
      "  I-OTHOBJ   -9999.0   -9999.0   -9999.0      -0.9   -9999.0   -9999.0   -9999.0   -9999.0   -9999.0   -9999.0\n",
      "     <sos>   -9999.0   -9999.0   -9999.0   -9999.0   -9999.0   -9999.0   -9999.0   -9999.0   -9999.0   -9999.0\n",
      "\n",
      "Start training...\n",
      "\n",
      "\n",
      "\n",
      "++ predicting, batch 27/27 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 0/50 \"f1-alpha-match-10\" train / dev / test | 4.39 / 4.55 / 4.49.\n",
      "## [BEST epoch], 19 seconds.\n",
      "\n",
      "-- train epoch 1/50, batch 271/271 (100.00%), loss = 471.46.\n",
      "\n",
      "++ predicting, batch 27/27 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 1/50 \"f1-alpha-match-10\" train / dev / test | 59.43 / 56.88 / 60.68.\n",
      "## [BEST epoch], 111 seconds.\n",
      "\n",
      "-- train epoch 2/50, batch 271/271 (100.00%), loss = 311.63.\n",
      "\n",
      "++ predicting, batch 27/27 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 2/50 \"f1-alpha-match-10\" train / dev / test | 65.54 / 61.71 / 64.25.\n",
      "## [BEST epoch], 107 seconds.\n",
      "\n",
      "-- train epoch 3/50, batch 271/271 (100.00%), loss = 263.07.\n",
      "\n",
      "++ predicting, batch 27/27 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 3/50 \"f1-alpha-match-10\" train / dev / test | 65.42 / 60.86 / 63.47.\n",
      "## [no improvement micro-f1 on DEV during the last 1 epochs (best_f1_dev=61.71), 111 seconds].\n",
      "\n",
      "-- train epoch 4/50, batch 271/271 (100.00%), loss = 241.10.\n",
      "\n",
      "++ predicting, batch 27/27 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 4/50 \"f1-alpha-match-10\" train / dev / test | 72.40 / 64.60 / 68.90.\n",
      "## [BEST epoch], 112 seconds.\n",
      "\n",
      "-- train epoch 5/50, batch 271/271 (100.00%), loss = 226.24.\n",
      "\n",
      "++ predicting, batch 27/27 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 5/50 \"f1-alpha-match-10\" train / dev / test | 76.05 / 69.95 / 74.28.\n",
      "## [BEST epoch], 104 seconds.\n",
      "\n",
      "-- train epoch 6/50, batch 271/271 (100.00%), loss = 213.57.\n",
      "\n",
      "++ predicting, batch 27/27 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 6/50 \"f1-alpha-match-10\" train / dev / test | 77.31 / 70.24 / 74.36.\n",
      "## [BEST epoch], 111 seconds.\n",
      "\n",
      "-- train epoch 7/50, batch 271/271 (100.00%), loss = 169.04.\n",
      "\n",
      "++ predicting, batch 27/27 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 7/50 \"f1-alpha-match-10\" train / dev / test | 79.29 / 72.59 / 77.10.\n",
      "## [BEST epoch], 127 seconds.\n",
      "\n",
      "-- train epoch 8/50, batch 271/271 (100.00%), loss = 170.33.\n",
      "\n",
      "++ predicting, batch 27/27 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 8/50 \"f1-alpha-match-10\" train / dev / test | 77.94 / 71.17 / 73.04.\n",
      "## [no improvement micro-f1 on DEV during the last 1 epochs (best_f1_dev=72.59), 155 seconds].\n",
      "\n",
      "-- train epoch 9/50, batch 271/271 (100.00%), loss = 134.57.\n",
      "\n",
      "++ predicting, batch 27/27 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 9/50 \"f1-alpha-match-10\" train / dev / test | 82.77 / 74.56 / 76.31.\n",
      "## [BEST epoch], 153 seconds.\n",
      "\n",
      "-- train epoch 10/50, batch 271/271 (100.00%), loss = 179.24.\n",
      "\n",
      "++ predicting, batch 27/27 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 10/50 \"f1-alpha-match-10\" train / dev / test | 83.55 / 76.47 / 78.44.\n",
      "## [BEST epoch], 155 seconds.\n",
      "\n",
      "-- train epoch 11/50, batch 271/271 (100.00%), loss = 140.36.\n",
      "\n",
      "++ predicting, batch 27/27 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 11/50 \"f1-alpha-match-10\" train / dev / test | 86.31 / 77.43 / 80.45.\n",
      "## [BEST epoch], 160 seconds.\n",
      "\n",
      "-- train epoch 12/50, batch 271/271 (100.00%), loss = 147.24.\n",
      "\n",
      "++ predicting, batch 27/27 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 12/50 \"f1-alpha-match-10\" train / dev / test | 86.70 / 79.10 / 80.21.\n",
      "## [BEST epoch], 153 seconds.\n",
      "\n",
      "-- train epoch 13/50, batch 271/271 (100.00%), loss = 134.88.\n",
      "\n",
      "++ predicting, batch 27/27 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 13/50 \"f1-alpha-match-10\" train / dev / test | 86.60 / 77.57 / 79.79.\n",
      "## [no improvement micro-f1 on DEV during the last 1 epochs (best_f1_dev=79.10), 155 seconds].\n",
      "\n",
      "-- train epoch 14/50, batch 271/271 (100.00%), loss = 125.42.\n",
      "\n",
      "++ predicting, batch 27/27 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 14/50 \"f1-alpha-match-10\" train / dev / test | 87.39 / 76.45 / 81.97.\n",
      "## [no improvement micro-f1 on DEV during the last 2 epochs (best_f1_dev=79.10), 154 seconds].\n",
      "\n",
      "-- train epoch 15/50, batch 271/271 (100.00%), loss = 110.34.\n",
      "\n",
      "++ predicting, batch 27/27 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 15/50 \"f1-alpha-match-10\" train / dev / test | 87.51 / 76.90 / 80.79.\n",
      "## [no improvement micro-f1 on DEV during the last 3 epochs (best_f1_dev=79.10), 155 seconds].\n",
      "\n",
      "-- train epoch 16/50, batch 271/271 (100.00%), loss = 114.84.\n",
      "\n",
      "++ predicting, batch 27/27 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 16/50 \"f1-alpha-match-10\" train / dev / test | 88.69 / 78.76 / 81.54.\n",
      "## [no improvement micro-f1 on DEV during the last 4 epochs (best_f1_dev=79.10), 153 seconds].\n",
      "\n",
      "-- train epoch 17/50, batch 271/271 (100.00%), loss = 123.46.\n",
      "\n",
      "++ predicting, batch 27/27 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 17/50 \"f1-alpha-match-10\" train / dev / test | 89.21 / 77.54 / 81.22.\n",
      "## [no improvement micro-f1 on DEV during the last 5 epochs (best_f1_dev=79.10), 155 seconds].\n",
      "\n",
      "-- train epoch 18/50, batch 271/271 (100.00%), loss = 105.34.\n",
      "\n",
      "++ predicting, batch 27/27 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 18/50 \"f1-alpha-match-10\" train / dev / test | 90.26 / 78.83 / 82.23.\n",
      "## [no improvement micro-f1 on DEV during the last 6 epochs (best_f1_dev=79.10), 152 seconds].\n",
      "\n",
      "-- train epoch 19/50, batch 271/271 (100.00%), loss = 94.80.\n",
      "\n",
      "++ predicting, batch 27/27 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 19/50 \"f1-alpha-match-10\" train / dev / test | 90.78 / 79.01 / 82.49.\n",
      "## [no improvement micro-f1 on DEV during the last 7 epochs (best_f1_dev=79.10), 154 seconds].\n",
      "\n",
      "-- train epoch 20/50, batch 271/271 (100.00%), loss = 100.66.\n",
      "\n",
      "++ predicting, batch 27/27 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 20/50 \"f1-alpha-match-10\" train / dev / test | 91.18 / 79.14 / 82.98.\n",
      "## [BEST epoch], 159 seconds.\n",
      "\n",
      "-- train epoch 21/50, batch 271/271 (100.00%), loss = 103.85.\n",
      "\n",
      "++ predicting, batch 27/27 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 21/50 \"f1-alpha-match-10\" train / dev / test | 91.58 / 79.06 / 82.96.\n",
      "## [no improvement micro-f1 on DEV during the last 1 epochs (best_f1_dev=79.14), 159 seconds].\n",
      "\n",
      "-- train epoch 22/50, batch 271/271 (100.00%), loss = 94.58.\n",
      "\n",
      "++ predicting, batch 27/27 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 22/50 \"f1-alpha-match-10\" train / dev / test | 92.08 / 80.40 / 82.67.\n",
      "## [BEST epoch], 159 seconds.\n",
      "\n",
      "-- train epoch 23/50, batch 271/271 (100.00%), loss = 79.92.\n",
      "\n",
      "++ predicting, batch 27/27 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 23/50 \"f1-alpha-match-10\" train / dev / test | 92.18 / 80.04 / 82.04.\n",
      "## [no improvement micro-f1 on DEV during the last 1 epochs (best_f1_dev=80.40), 156 seconds].\n",
      "\n",
      "-- train epoch 24/50, batch 271/271 (100.00%), loss = 86.33.\n",
      "\n",
      "++ predicting, batch 27/27 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 24/50 \"f1-alpha-match-10\" train / dev / test | 93.03 / 80.11 / 82.47.\n",
      "## [no improvement micro-f1 on DEV during the last 2 epochs (best_f1_dev=80.40), 151 seconds].\n",
      "\n",
      "-- train epoch 25/50, batch 271/271 (100.00%), loss = 74.16.\n",
      "\n",
      "++ predicting, batch 27/27 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 25/50 \"f1-alpha-match-10\" train / dev / test | 92.42 / 79.60 / 82.39.\n",
      "## [no improvement micro-f1 on DEV during the last 3 epochs (best_f1_dev=80.40), 153 seconds].\n",
      "\n",
      "-- train epoch 26/50, batch 271/271 (100.00%), loss = 71.81.\n",
      "\n",
      "++ predicting, batch 27/27 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 26/50 \"f1-alpha-match-10\" train / dev / test | 92.62 / 80.86 / 82.83.\n",
      "## [BEST epoch], 156 seconds.\n",
      "\n",
      "-- train epoch 27/50, batch 271/271 (100.00%), loss = 59.15.\n",
      "\n",
      "++ predicting, batch 27/27 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 27/50 \"f1-alpha-match-10\" train / dev / test | 93.20 / 81.53 / 83.38.\n",
      "## [BEST epoch], 125 seconds.\n",
      "\n",
      "-- train epoch 28/50, batch 271/271 (100.00%), loss = 68.64.\n",
      "\n",
      "++ predicting, batch 27/27 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 28/50 \"f1-alpha-match-10\" train / dev / test | 93.54 / 80.92 / 83.33.\n",
      "## [no improvement micro-f1 on DEV during the last 1 epochs (best_f1_dev=81.53), 113 seconds].\n",
      "\n",
      "-- train epoch 29/50, batch 271/271 (100.00%), loss = 75.70.\n",
      "\n",
      "++ predicting, batch 27/27 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 29/50 \"f1-alpha-match-10\" train / dev / test | 94.03 / 79.51 / 83.45.\n",
      "## [no improvement micro-f1 on DEV during the last 2 epochs (best_f1_dev=81.53), 145 seconds].\n",
      "\n",
      "-- train epoch 30/50, batch 271/271 (100.00%), loss = 74.57.\n",
      "\n",
      "++ predicting, batch 27/27 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 30/50 \"f1-alpha-match-10\" train / dev / test | 93.99 / 80.29 / 83.44.\n",
      "## [no improvement micro-f1 on DEV during the last 3 epochs (best_f1_dev=81.53), 154 seconds].\n",
      "\n",
      "-- train epoch 31/50, batch 271/271 (100.00%), loss = 69.73.\n",
      "\n",
      "++ predicting, batch 27/27 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 31/50 \"f1-alpha-match-10\" train / dev / test | 94.17 / 79.95 / 83.26.\n",
      "## [no improvement micro-f1 on DEV during the last 4 epochs (best_f1_dev=81.53), 155 seconds].\n",
      "\n",
      "-- train epoch 32/50, batch 271/271 (100.00%), loss = 49.83.\n",
      "\n",
      "++ predicting, batch 27/27 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 32/50 \"f1-alpha-match-10\" train / dev / test | 94.59 / 79.64 / 83.37.\n",
      "## [no improvement micro-f1 on DEV during the last 5 epochs (best_f1_dev=81.53), 155 seconds].\n",
      "\n",
      "-- train epoch 33/50, batch 271/271 (100.00%), loss = 62.78.\n",
      "\n",
      "++ predicting, batch 27/27 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 33/50 \"f1-alpha-match-10\" train / dev / test | 94.39 / 81.25 / 83.60.\n",
      "## [no improvement micro-f1 on DEV during the last 6 epochs (best_f1_dev=81.53), 154 seconds].\n",
      "\n",
      "-- train epoch 34/50, batch 271/271 (100.00%), loss = 66.57.\n",
      "\n",
      "++ predicting, batch 27/27 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 34/50 \"f1-alpha-match-10\" train / dev / test | 94.53 / 80.56 / 84.14.\n",
      "## [no improvement micro-f1 on DEV during the last 7 epochs (best_f1_dev=81.53), 154 seconds].\n",
      "\n",
      "-- train epoch 35/50, batch 271/271 (100.00%), loss = 66.50.\n",
      "\n",
      "++ predicting, batch 27/27 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 35/50 \"f1-alpha-match-10\" train / dev / test | 95.07 / 81.85 / 84.08.\n",
      "## [BEST epoch], 154 seconds.\n",
      "\n",
      "-- train epoch 36/50, batch 271/271 (100.00%), loss = 58.16.\n",
      "\n",
      "++ predicting, batch 27/27 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 36/50 \"f1-alpha-match-10\" train / dev / test | 95.00 / 82.00 / 83.44.\n",
      "## [BEST epoch], 155 seconds.\n",
      "\n",
      "-- train epoch 37/50, batch 271/271 (100.00%), loss = 61.50.\n",
      "\n",
      "++ predicting, batch 27/27 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 37/50 \"f1-alpha-match-10\" train / dev / test | 95.23 / 81.94 / 83.28.\n",
      "## [no improvement micro-f1 on DEV during the last 1 epochs (best_f1_dev=82.00), 154 seconds].\n",
      "\n",
      "-- train epoch 38/50, batch 271/271 (100.00%), loss = 52.78.\n",
      "\n",
      "++ predicting, batch 27/27 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 38/50 \"f1-alpha-match-10\" train / dev / test | 95.31 / 81.44 / 84.11.\n",
      "## [no improvement micro-f1 on DEV during the last 2 epochs (best_f1_dev=82.00), 150 seconds].\n",
      "\n",
      "-- train epoch 39/50, batch 271/271 (100.00%), loss = 60.88.\n",
      "\n",
      "++ predicting, batch 27/27 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 39/50 \"f1-alpha-match-10\" train / dev / test | 95.74 / 81.48 / 83.83.\n",
      "## [no improvement micro-f1 on DEV during the last 3 epochs (best_f1_dev=82.00), 153 seconds].\n",
      "\n",
      "-- train epoch 40/50, batch 271/271 (100.00%), loss = 62.90.\n",
      "\n",
      "++ predicting, batch 27/27 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 40/50 \"f1-alpha-match-10\" train / dev / test | 95.62 / 81.39 / 83.59.\n",
      "## [no improvement micro-f1 on DEV during the last 4 epochs (best_f1_dev=82.00), 153 seconds].\n",
      "\n",
      "-- train epoch 41/50, batch 271/271 (100.00%), loss = 60.31.\n",
      "\n",
      "++ predicting, batch 27/27 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 41/50 \"f1-alpha-match-10\" train / dev / test | 95.92 / 80.92 / 83.96.\n",
      "## [no improvement micro-f1 on DEV during the last 5 epochs (best_f1_dev=82.00), 153 seconds].\n",
      "\n",
      "-- train epoch 42/50, batch 271/271 (100.00%), loss = 38.39.\n",
      "\n",
      "++ predicting, batch 27/27 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 42/50 \"f1-alpha-match-10\" train / dev / test | 95.92 / 82.22 / 84.12.\n",
      "## [BEST epoch], 154 seconds.\n",
      "\n",
      "-- train epoch 43/50, batch 271/271 (100.00%), loss = 50.95.\n",
      "\n",
      "++ predicting, batch 27/27 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 43/50 \"f1-alpha-match-10\" train / dev / test | 95.46 / 80.78 / 83.16.\n",
      "## [no improvement micro-f1 on DEV during the last 1 epochs (best_f1_dev=82.22), 152 seconds].\n",
      "\n",
      "-- train epoch 44/50, batch 271/271 (100.00%), loss = 52.35.\n",
      "\n",
      "++ predicting, batch 27/27 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 44/50 \"f1-alpha-match-10\" train / dev / test | 95.78 / 81.72 / 83.66.\n",
      "## [no improvement micro-f1 on DEV during the last 2 epochs (best_f1_dev=82.22), 158 seconds].\n",
      "\n",
      "-- train epoch 45/50, batch 271/271 (100.00%), loss = 71.44.\n",
      "\n",
      "++ predicting, batch 27/27 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 45/50 \"f1-alpha-match-10\" train / dev / test | 95.57 / 81.92 / 83.27.\n",
      "## [no improvement micro-f1 on DEV during the last 3 epochs (best_f1_dev=82.22), 152 seconds].\n",
      "\n",
      "-- train epoch 46/50, batch 271/271 (100.00%), loss = 39.98.\n",
      "\n",
      "++ predicting, batch 27/27 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 46/50 \"f1-alpha-match-10\" train / dev / test | 95.96 / 81.67 / 83.85.\n",
      "## [no improvement micro-f1 on DEV during the last 4 epochs (best_f1_dev=82.22), 153 seconds].\n",
      "\n",
      "-- train epoch 47/50, batch 271/271 (100.00%), loss = 45.98.\n",
      "\n",
      "++ predicting, batch 27/27 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 47/50 \"f1-alpha-match-10\" train / dev / test | 96.16 / 81.73 / 83.41.\n",
      "## [no improvement micro-f1 on DEV during the last 5 epochs (best_f1_dev=82.22), 153 seconds].\n",
      "\n",
      "-- train epoch 48/50, batch 271/271 (100.00%), loss = 53.98.\n",
      "\n",
      "++ predicting, batch 27/27 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 48/50 \"f1-alpha-match-10\" train / dev / test | 96.17 / 81.69 / 83.71.\n",
      "## [no improvement micro-f1 on DEV during the last 6 epochs (best_f1_dev=82.22), 152 seconds].\n",
      "\n",
      "-- train epoch 49/50, batch 271/271 (100.00%), loss = 51.61.\n",
      "\n",
      "++ predicting, batch 27/27 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 49/50 \"f1-alpha-match-10\" train / dev / test | 95.92 / 80.31 / 83.83.\n",
      "## [no improvement micro-f1 on DEV during the last 7 epochs (best_f1_dev=82.22), 155 seconds].\n",
      "\n",
      "-- train epoch 50/50, batch 271/271 (100.00%), loss = 43.00.\n",
      "\n",
      "++ predicting, batch 27/27 (97.00%).\n",
      "\n",
      "++ predicting, batch 3/3 (67.00%).\n",
      "\n",
      "++ predicting, batch 5/5 (80.00%).\n",
      "== eval epoch 50/50 \"f1-alpha-match-10\" train / dev / test | 96.08 / 81.34 / 84.22.\n",
      "## [no improvement micro-f1 on DEV during the last 8 epochs (best_f1_dev=82.22), 147 seconds].\n",
      "\n",
      "Evaluation\n",
      "\n",
      "batch_size=10\n",
      "char_cnn_filter_num=30\n",
      "char_embeddings_dim=25\n",
      "char_window_size=3\n",
      "check_for_lowercase=True\n",
      "clip_grad=5\n",
      "cross_fold_id=-1\n",
      "cross_folds_num=-1\n",
      "data_io='connl-ner-2003'\n",
      "dataset_sort=False\n",
      "dev='/home/vika/NER_RNN/targer/data/NER/Asqua_CAM_aspects/CAM_dev.csv'\n",
      "dropout_ratio=0.5\n",
      "elmo_options='/home/vika/NER_RNN/targer/embeddings/elmo_2x4096_512_2048cnn_2xhighway_5.5B_options.json'\n",
      "elmo_weights='/home/vika/NER_RNN/targer/embeddings/elmo_2x4096_512_2048cnn_2xhighway_5.5B_weights.hdf5'\n",
      "emb_delimiter=' '\n",
      "emb_dim=100\n",
      "emb_fn='embeddings/glove.6B.100d.txt'\n",
      "emb_load_all=False\n",
      "epoch_num=50\n",
      "evaluator='f1-alpha-match-10'\n",
      "freeze_char_embeddings=False\n",
      "freeze_word_embeddings=False\n",
      "gpu=1\n",
      "isElmo=True\n",
      "load=None\n",
      "lr=0.001\n",
      "lr_decay=0.05\n",
      "min_epoch_num=50\n",
      "model='BiRNNCNNCRF'\n",
      "momentum=0.9\n",
      "opt='adam'\n",
      "patience=20\n",
      "report_fn='2019_09_16_17-51_34_report.txt'\n",
      "rnn_hidden_dim=200\n",
      "rnn_type='LSTM'\n",
      "save='new_tagger1.hdf5'\n",
      "save_best=True\n",
      "seed_num=42\n",
      "test='/home/vika/NER_RNN/targer/data/NER/Asqua_CAM_aspects/CAM_test_new.csv'\n",
      "train='/home/vika/NER_RNN/targer/data/NER/Asqua_CAM_aspects/CAM_train_new.csv'\n",
      "verbose=True\n",
      "word_len=20\n",
      "word_seq_indexer=None\n",
      "\n",
      "         epoch  |     train loss | f1-alpha-match-10-train | f1-alpha-match-10-dev | f1-alpha-match-10-test \n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "              0 |           0.00 |           4.39 |           4.55 |           4.49 \n",
      "              1 |         471.46 |          59.43 |          56.88 |          60.68 \n",
      "              2 |         311.63 |          65.54 |          61.71 |          64.25 \n",
      "              3 |         263.07 |          65.42 |          60.86 |          63.47 \n",
      "              4 |         241.10 |          72.40 |          64.60 |          68.90 \n",
      "              5 |         226.24 |          76.05 |          69.95 |          74.28 \n",
      "              6 |         213.57 |          77.31 |          70.24 |          74.36 \n",
      "              7 |         169.04 |          79.29 |          72.59 |          77.10 \n",
      "              8 |         170.33 |          77.94 |          71.17 |          73.04 \n",
      "              9 |         134.57 |          82.77 |          74.56 |          76.31 \n",
      "             10 |         179.24 |          83.55 |          76.47 |          78.44 \n",
      "             11 |         140.36 |          86.31 |          77.43 |          80.45 \n",
      "             12 |         147.24 |          86.70 |          79.10 |          80.21 \n",
      "             13 |         134.88 |          86.60 |          77.57 |          79.79 \n",
      "             14 |         125.42 |          87.39 |          76.45 |          81.97 \n",
      "             15 |         110.34 |          87.51 |          76.90 |          80.79 \n",
      "             16 |         114.84 |          88.69 |          78.76 |          81.54 \n",
      "             17 |         123.46 |          89.21 |          77.54 |          81.22 \n",
      "             18 |         105.34 |          90.26 |          78.83 |          82.23 \n",
      "             19 |          94.80 |          90.78 |          79.01 |          82.49 \n",
      "             20 |         100.66 |          91.18 |          79.14 |          82.98 \n",
      "             21 |         103.85 |          91.58 |          79.06 |          82.96 \n",
      "             22 |          94.58 |          92.08 |          80.40 |          82.67 \n",
      "             23 |          79.92 |          92.18 |          80.04 |          82.04 \n",
      "             24 |          86.33 |          93.03 |          80.11 |          82.47 \n",
      "             25 |          74.16 |          92.42 |          79.60 |          82.39 \n",
      "             26 |          71.81 |          92.62 |          80.86 |          82.83 \n",
      "             27 |          59.15 |          93.20 |          81.53 |          83.38 \n",
      "             28 |          68.64 |          93.54 |          80.92 |          83.33 \n",
      "             29 |          75.70 |          94.03 |          79.51 |          83.45 \n",
      "             30 |          74.57 |          93.99 |          80.29 |          83.44 \n",
      "             31 |          69.73 |          94.17 |          79.95 |          83.26 \n",
      "             32 |          49.83 |          94.59 |          79.64 |          83.37 \n",
      "             33 |          62.78 |          94.39 |          81.25 |          83.60 \n",
      "             34 |          66.57 |          94.53 |          80.56 |          84.14 \n",
      "             35 |          66.50 |          95.07 |          81.85 |          84.08 \n",
      "             36 |          58.16 |          95.00 |          82.00 |          83.44 \n",
      "             37 |          61.50 |          95.23 |          81.94 |          83.28 \n",
      "             38 |          52.78 |          95.31 |          81.44 |          84.11 \n",
      "             39 |          60.88 |          95.74 |          81.48 |          83.83 \n",
      "             40 |          62.90 |          95.62 |          81.39 |          83.59 \n",
      "             41 |          60.31 |          95.92 |          80.92 |          83.96 \n",
      "             42 |          38.39 |          95.92 |          82.22 |          84.12 \n",
      "             43 |          50.95 |          95.46 |          80.78 |          83.16 \n",
      "             44 |          52.35 |          95.78 |          81.72 |          83.66 \n",
      "             45 |          71.44 |          95.57 |          81.92 |          83.27 \n",
      "             46 |          39.98 |          95.96 |          81.67 |          83.85 \n",
      "             47 |          45.98 |          96.16 |          81.73 |          83.41 \n",
      "             48 |          53.98 |          96.17 |          81.69 |          83.71 \n",
      "             49 |          51.61 |          95.92 |          80.31 |          83.83 \n",
      "             50 |          43.00 |          96.08 |          81.34 |          84.22 \n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Final eval on test, \"save best\", best epoch on dev 42, f1-alpha-match-10, test = 84.12)\n",
      "--------------------------------------------------------------------------------------------------------------*** f1 alpha match, alpha = 1.0\n",
      "*** f1 = 84.12, precision = 86.43, recall = 81.94\n",
      "*** TP = 1216, FP = 191, FN = 268\n",
      "Input arguments:\n",
      "python3 main.py --train /home/vika/NER_RNN/targer/data/NER/Asqua_CAM_aspects/CAM_train_new.csv --dev /home/vika/NER_RNN/targer/data/NER/Asqua_CAM_aspects/CAM_dev.csv --data-io connl-ner-2003 --evaluator f1-alpha-match-10 --opt adam --lr 0.001 --save-best yes --patience 20 --rnn-hidden-dim 200 --gpu 1 --test /home/vika/NER_RNN/targer/data/NER/Asqua_CAM_aspects/CAM_test_new.csv --epoch-num 50 --isElmo True --save new_tagger1.hdf5\n",
      "\n",
      "84.1231\n"
     ]
    }
   ],
   "source": [
    "! python3 main.py --train \"/home/vika/NER_RNN/targer/data/NER/Asqua_CAM_aspects/CAM_train_new.csv\" --dev \"/home/vika/NER_RNN/targer/data/NER/Asqua_CAM_aspects/CAM_dev.csv\" --data-io connl-ner-2003 --evaluator f1-alpha-match-10 --opt adam --lr 0.001 --save-best yes --patience 20 --rnn-hidden-dim 200 --gpu 1 --test \"/home/vika/NER_RNN/targer/data/NER/Asqua_CAM_aspects/CAM_test_new.csv\" --epoch-num 50 --isElmo True --save \"new_tagger1.hdf5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/vika/NER_RNN/targer\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
